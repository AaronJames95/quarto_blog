{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fd0921d-1fdc-4232-8408-cecf45817657",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Predicting Disaster Tweets with NLP\"\n",
    "date: 2025-04-21\n",
    "slug: nlp-disaster-tweets\n",
    "author: \"Aaron James\"\n",
    "categories: [machine learning, tutorial, fastai, NLP, kaggle]\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    code-fold: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f70b79-a056-442d-a73a-14842a05d24e",
   "metadata": {},
   "source": [
    "# NLP, Kaggle, and Disaster Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b41237-05f3-4f72-84f4-a41dfc6f5ddd",
   "metadata": {},
   "source": [
    "![nlp_tweets.png](nlp_tweets.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2411633f-8975-4099-86d6-dca95781cfe9",
   "metadata": {},
   "source": [
    "This lesson focused on applying NLP using hugging face's library. In the book we used the fastai library. I decided to apply what I learned to the **kaggle [NLP disaster tweets competition](https://www.kaggle.com/competitions/nlp-getting-started/overview)**. I'll be referencing the two course notebooks for this lesson [Chapter 10](https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb) and [getting-started-with-NLP](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29c6ebf-3f91-4ec5-ad29-0946e1815038",
   "metadata": {},
   "source": [
    "The goal of this competition is to take a set of tweets and determine based on their metadata whether or not they refer to real disasters. We use Natural Language Processing (NLP) to fit a model to the tweets to make our predictions. \n",
    "\n",
    "The first thing I did was make sure I could download the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7410207c-13a2-4942-aac8-dda1a315c5b1",
   "metadata": {},
   "source": [
    "# Downloading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5cf161-0d3d-42e6-ac6d-983bb165c27a",
   "metadata": {},
   "source": [
    "I wrote a short script to download the dataset. Actually, I just used the code from the getting-started notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b01baa81-8c90-4edc-b0c9-c90e0d8ed5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp-getting-started.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n",
    "competition = 'nlp-getting-started'\n",
    "if iskaggle:\n",
    "    !pip install -Uqq fastai\n",
    "else:\n",
    "    import zipfile,kaggle\n",
    "    path = Path(competition)\n",
    "    kaggle.api.competition_download_cli(str(path))\n",
    "    zipfile.ZipFile(f'{path}.zip').extractall(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fde641-b0af-4bcf-93a1-7cffb0d4516f",
   "metadata": {},
   "source": [
    "# Loading the data and EDA\n",
    "Now that the dataset is downloaded, we want to load it into memory to start playing around with various features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "115e4429-a642-425f-b131-a6af4d31e355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7552</td>\n",
       "      <td>5080</td>\n",
       "      <td>7613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>221</td>\n",
       "      <td>3341</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>fatalities</td>\n",
       "      <td>USA</td>\n",
       "      <td>11-Year-Old Boy Charged With Manslaughter of Toddler: Report: An 11-year-old boy has been charged with manslaughter over the fatal sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>45</td>\n",
       "      <td>104</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           keyword location  \\\n",
       "count         7552     5080   \n",
       "unique         221     3341   \n",
       "top     fatalities      USA   \n",
       "freq            45      104   \n",
       "\n",
       "                                                                                                                                            text  \n",
       "count                                                                                                                                       7613  \n",
       "unique                                                                                                                                      7503  \n",
       "top     11-Year-Old Boy Charged With Manslaughter of Toddler: Report: An 11-year-old boy has been charged with manslaughter over the fatal sh...  \n",
       "freq                                                                                                                                          10  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import relevant frameworks\n",
    "from fastai.imports import *\n",
    "if iskaggle: path = Path('../input/' + competition)\n",
    "df = pd.read_csv(path/'train.csv')\n",
    "df_test = pd.read_csv(path/'test.csv')\n",
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be42a002-8acc-4092-a0ba-12b832ec7f40",
   "metadata": {},
   "source": [
    "## Analyzing Data Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48885144-556f-4a16-891c-0a6238b535b9",
   "metadata": {},
   "source": [
    "Since this is my first time looking at the dataset, I want to see what category breakdowns seem to be significant. Basically, I want to get a sense of what variables it might be helpful to look at more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f96dff7-3c25-4d37-8e8a-26b19ce32cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keyword\n",
       "fatalities               45\n",
       "deluge                   42\n",
       "armageddon               42\n",
       "sinking                  41\n",
       "damage                   41\n",
       "                         ..\n",
       "forest%20fire            19\n",
       "epicentre                12\n",
       "threat                   11\n",
       "inundation               10\n",
       "radiation%20emergency     9\n",
       "Name: count, Length: 221, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keyword.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8d80940-2bb7-4fb1-9143-ae7dd38f34d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location\n",
       "USA                    104\n",
       "New York                71\n",
       "United States           50\n",
       "London                  45\n",
       "Canada                  29\n",
       "                      ... \n",
       "MontrÌ©al, QuÌ©bec       1\n",
       "Montreal                 1\n",
       "ÌÏT: 6.4682,3.18287      1\n",
       "Live4Heed??              1\n",
       "Lincoln                  1\n",
       "Name: count, Length: 3341, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.location.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd014c4-51fb-44ab-8b35-70a925042e7c",
   "metadata": {},
   "source": [
    "## Create another baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea331ac1-7dc5-4e95-a021-6ff9b396c894",
   "metadata": {},
   "source": [
    "I spent a good amount of time looking into novel ways to break the data down to give the model an optimal input. I considered:\n",
    "\n",
    "* removing the `%20` and replacing with a space for the keywords\n",
    "* removing the location to see what impact that has\n",
    "* collapsing duplicate locations into one (theoretically, USA == United States)\n",
    "\n",
    "Then I realized, before I can determine how those edits would affect my results... I need results. I was reminded that for our last lesson we started by creating a baseline. I decided to do this in the simplest way I could think of and iterate from there. To me, that was squishing all the default features of the data into a single string, and training the model on that input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2f6160-7bea-4278-ba2c-5bf6d3f9479d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>inputs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "      <td>LOC: ; KW: ; TEXT: Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>LOC: ; KW: ; TEXT: Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "      <td>LOC: ; KW: ; TEXT: All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "      <td>LOC: ; KW: ; TEXT: 13,000 people receive #wildfires evacuation orders in California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "      <td>LOC: ; KW: ; TEXT: Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0                                                                  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                                                                 Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3                                                                      13,000 people receive #wildfires evacuation orders in California    \n",
       "4                                               Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "\n",
       "   target  \\\n",
       "0       1   \n",
       "1       1   \n",
       "2       1   \n",
       "3       1   \n",
       "4       1   \n",
       "\n",
       "                                                                                                                                                     inputs  \n",
       "0                                                                  LOC: ; KW: ; TEXT: Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all  \n",
       "1                                                                                                 LOC: ; KW: ; TEXT: Forest fire near La Ronge Sask. Canada  \n",
       "2  LOC: ; KW: ; TEXT: All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected  \n",
       "3                                                                      LOC: ; KW: ; TEXT: 13,000 people receive #wildfires evacuation orders in California   \n",
       "4                                               LOC: ; KW: ; TEXT: Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school   "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_col = 'inputs'\n",
    "na_fill = ''\n",
    "df[input_col] = 'LOC: ' + df.location.fillna(na_fill) + '; KW: ' + df.keyword.fillna(na_fill) + '; TEXT: ' + df.text.fillna(na_fill)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7190b13c-4ce3-4549-99b4-db8a9dc895c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings to make the output look cleaner\n",
    "from transformers.utils import logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7af9583c-e6d2-497f-9481-cafc3e7d3354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09bbc009752548a0a7fa34b039f70ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset,DatasetDict\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "\n",
    "# convert dataframe into a huggingface dataset\n",
    "ds = Dataset.from_pandas(df)\n",
    "\n",
    "model_nm = 'microsoft/deberta-v3-small'\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm)\n",
    "def tok_func(x): return tokz(x[input_col])\n",
    "\n",
    "# tokenize the input\n",
    "tok_ds = ds.map(tok_func, batched=True)\n",
    "tok_ds = tok_ds.rename_columns({'target':'labels'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c3449-b5f3-4464-9650-c8d057f72756",
   "metadata": {},
   "source": [
    "I briefly considered trying to engineer a perfect validation dataset before remembering that the goal was to create a simple baseline first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae9da523-e9bb-4dc9-823e-b567fd4d2aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'keyword', 'location', 'text', 'labels', 'inputs', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5709\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'keyword', 'location', 'text', 'labels', 'inputs', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1904\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting up the validation set now\n",
    "dds = tok_ds.train_test_split(0.25, seed=1337)\n",
    "dds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d383f1a7-461b-4230-90d4-61e7c02ac93f",
   "metadata": {},
   "source": [
    "## Defining the F1 Metric\n",
    "\n",
    "The kaggle competition uses the [F1 metric](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) between the predicted values and expected values. So we have to define it in a way that huggingface understands to use it in our training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a94203df-40d2-4dd8-9056-95fa3ce26dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'keyword', 'location', 'text', 'labels', 'inputs', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 1904\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"f1\": f1_score(labels, preds),\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds),\n",
    "        \"recall\": recall_score(labels, preds)\n",
    "    }\n",
    "df_eval = dds['test']\n",
    "df_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7220d6-1705-47db-a37d-a411133b1c8a",
   "metadata": {},
   "source": [
    "Now we can run our training loop. I'm not deeply concerned with every hyper parameter here, because I'm not there yet. I just want to focus on epochs, batch size `bs` and learning rate `lr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5048e55-4142-4329-9ab5-b56a24de2ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 00:27, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.524064</td>\n",
       "      <td>0.628931</td>\n",
       "      <td>0.752101</td>\n",
       "      <td>0.950119</td>\n",
       "      <td>0.470035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.456287</td>\n",
       "      <td>0.751911</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.920068</td>\n",
       "      <td>0.635723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.462908</td>\n",
       "      <td>0.783172</td>\n",
       "      <td>0.824055</td>\n",
       "      <td>0.871758</td>\n",
       "      <td>0.710928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.476750</td>\n",
       "      <td>0.787008</td>\n",
       "      <td>0.820903</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.740306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments,Trainer\n",
    "\n",
    "\n",
    "epochs = 4\n",
    "bs = 128\n",
    "lr = 8e-5\n",
    "\n",
    "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=0.01, report_to='none')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=2)\n",
    "trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
    "                  tokenizer=tokz, compute_metrics=compute_metrics)\n",
    "\n",
    "trainer.train();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645643d-7210-4c88-a951-44c95baa3550",
   "metadata": {},
   "source": [
    "# Iterating from the Baseline Results\n",
    "Alright ok thank God. It took a while, but we got a baseline. I tried a few randoms seeds for the test split. I got F1 values of .7949, .7948, and .8049. This is good because it tells me that based on a few random splits the metric doesn't change too much. Now we want to see what are the hardest tweets to classify. We started with the baseline to just give us a sense of what data gives the model trouble. We didn't curate a validation set, because we were stumped on which aspects of the data to sort into a valdiation set. Analyzing the tweets that are hardest to classify hopefully gives us a sense of what data features lead to trouble for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c37c11-ee50-4362-aa48-63d0a534cb59",
   "metadata": {},
   "source": [
    "## Analyzing the Model's \"Trouble\" Tweets (or Error Analysis)\n",
    "\n",
    "First I'll store the main outputs of the training for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a126688-a0ce-463b-95e6-426b965968cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from fastai.vision.all import *\n",
    "\n",
    "# save our validation set under a new name\n",
    "df_eval = dds['test']\n",
    "\n",
    "# Get predictions and probabilities\n",
    "outputs = trainer.predict(df_eval)\n",
    "logits = outputs.predictions\n",
    "labels = outputs.label_ids\n",
    "probs = F.softmax(tensor(logits), dim=1).numpy()\n",
    "preds = probs.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd1be2c-f51f-4891-9797-7edae84375bb",
   "metadata": {},
   "source": [
    "Now I want to create a dataframe that has additional relevant features. We already have the training output, so these new features will be based on the training. Once I have this dataframe I can start visualizing it in different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a7fbd97-0138-46d6-a88e-eb129ecde95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-example loss\n",
    "losses = [F.cross_entropy(tensor(logit), tensor(preds)) for logit, preds in zip(logits, preds)]\n",
    "\n",
    "def count_cap(text):\n",
    "    words = text.split()\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word and word[0].isupper():\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Set up up new dataframe with the results of the first pass\n",
    "df_valid_results = pd.DataFrame({\n",
    "    \"text\": [df_eval[i][\"text\"] for i in range(len(df_eval))],\n",
    "    \"keyword\": [df_eval[i].get(\"keyword\") or \"n/a\" for i in range(len(labels))],\n",
    "    \"location\": [df_eval[i].get(\"location\") or \"n/a\" for i in range(len(labels))],\n",
    "    \"label\": labels,\n",
    "    \"cap_ratio\": [count_cap(twt)/float(len(twt)) for twt in df_eval['text']],\n",
    "    \"pred\": preds,\n",
    "    \"prob_1\": probs[:, 1],\n",
    "    \"contains_link\": [twt.count(\"http\") > 0 for twt in df_eval['text']],\n",
    "    \"tweet_len\": [len(twt) for twt in df_eval['text']],\n",
    "    \"hashtags\": [twt.count(\"#\") for twt in df_eval['text']],\n",
    "    \"is_location\": [bool(loc) for loc in df_eval['location']],\n",
    "    \"loss\": losses\n",
    "})\n",
    "\n",
    "# Tag confusion type\n",
    "def label_type(row):\n",
    "    if row.label == 1 and row.pred == 1: return \"TP\"\n",
    "    elif row.label == 0 and row.pred == 0: return \"TN\"\n",
    "    elif row.label == 1 and row.pred == 0: return \"FN\"\n",
    "    else: return \"FP\"\n",
    "\n",
    "df_valid_results[\"type\"] = df_valid_results.apply(label_type, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b083d3a-a095-4991-8768-92286706a2de",
   "metadata": {},
   "source": [
    "## EDA on Baseline Results\n",
    "\n",
    "You can see from the creation of this dataset some of my ideas on relevant features. Frankly I spent too much time going down the feature engineering rabbit hole. The whole point of our baseline is that we can see what are the tweets that the model find hardest to classify. Theoretically this will give us some insight on how to structure a more optimal input string.\n",
    "\n",
    "The first method I tried was to sort each tweet by the cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "030826f0-6992-425f-a2da-67799e626d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>label</th>\n",
       "      <th>cap_ratio</th>\n",
       "      <th>pred</th>\n",
       "      <th>prob_1</th>\n",
       "      <th>contains_link</th>\n",
       "      <th>tweet_len</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>is_location</th>\n",
       "      <th>loss</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>'Since1970the 2 biggest depreciations in CAD:USD in yr b4federal election coincide w/landslide win for opposition' http://t.co/wgqKXmby3B</td>\n",
       "      <td>landslide</td>\n",
       "      <td>n/a</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500412</td>\n",
       "      <td>True</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>tensor(0.6923)</td>\n",
       "      <td>FP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>Firepower in the lab [electronic resource] : automation in the fight against infectious diseases and bioterrorism /Û_ http://t.co/KvpbybglSR</td>\n",
       "      <td>bioterrorism</td>\n",
       "      <td>n/a</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007092</td>\n",
       "      <td>1</td>\n",
       "      <td>0.501099</td>\n",
       "      <td>True</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>tensor(0.6910)</td>\n",
       "      <td>FP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>@JakeGint the mass murder got her hot and bothered but at heart she was always a traditionalist.</td>\n",
       "      <td>mass%20murder</td>\n",
       "      <td>n/a</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.498703</td>\n",
       "      <td>False</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>tensor(0.6906)</td>\n",
       "      <td>FN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>Of what use exactly is the national Assembly? Honestly they are worthless. We are derailed.</td>\n",
       "      <td>derailed</td>\n",
       "      <td>Kwara, Nigeria</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043956</td>\n",
       "      <td>0</td>\n",
       "      <td>0.496643</td>\n",
       "      <td>False</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>tensor(0.6865)</td>\n",
       "      <td>TN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>Hat #russian soviet army kgb  military #cossack #ushanka  LINK:\\nhttp://t.co/bla42Rdt1O http://t.co/EInSQS8tFq</td>\n",
       "      <td>military</td>\n",
       "      <td>n/a</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018349</td>\n",
       "      <td>1</td>\n",
       "      <td>0.503387</td>\n",
       "      <td>True</td>\n",
       "      <td>109</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>tensor(0.6864)</td>\n",
       "      <td>FP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                               text  \\\n",
       "264       'Since1970the 2 biggest depreciations in CAD:USD in yr b4federal election coincide w/landslide win for opposition' http://t.co/wgqKXmby3B   \n",
       "1831  Firepower in the lab [electronic resource] : automation in the fight against infectious diseases and bioterrorism /Û_ http://t.co/KvpbybglSR   \n",
       "654                                                @JakeGint the mass murder got her hot and bothered but at heart she was always a traditionalist.   \n",
       "399                                                     Of what use exactly is the national Assembly? Honestly they are worthless. We are derailed.   \n",
       "960                                  Hat #russian soviet army kgb  military #cossack #ushanka  LINK:\\nhttp://t.co/bla42Rdt1O http://t.co/EInSQS8tFq   \n",
       "\n",
       "            keyword        location  label  cap_ratio  pred    prob_1  \\\n",
       "264       landslide             n/a      0   0.007299     1  0.500412   \n",
       "1831   bioterrorism             n/a      0   0.007092     1  0.501099   \n",
       "654   mass%20murder             n/a      1   0.000000     0  0.498703   \n",
       "399        derailed  Kwara, Nigeria      0   0.043956     0  0.496643   \n",
       "960        military             n/a      0   0.018349     1  0.503387   \n",
       "\n",
       "      contains_link  tweet_len  hashtags  is_location            loss type  \n",
       "264            True        137         0        False  tensor(0.6923)   FP  \n",
       "1831           True        141         0        False  tensor(0.6910)   FP  \n",
       "654           False         96         0        False  tensor(0.6906)   FN  \n",
       "399           False         91         0         True  tensor(0.6865)   TN  \n",
       "960            True        109         3        False  tensor(0.6864)   FP  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_results.sort_values(by=\"loss\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a97af6-546f-4ac0-a26c-8024c3c3e9ac",
   "metadata": {},
   "source": [
    "I can immediately see a problem with this. We don't only get incorrect predictions! Indeed the highest loss value across our entire dataset was a correct prediction. This means, we have to sort our data by different values to determine which tweets caused the most trouble. My strategy was to sort by a new \"confidence\" feature, and only look at the data that was predicted incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9aae8452-a620-4065-8239-2813925ba060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>label</th>\n",
       "      <th>cap_ratio</th>\n",
       "      <th>pred</th>\n",
       "      <th>prob_1</th>\n",
       "      <th>contains_link</th>\n",
       "      <th>tweet_len</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>is_location</th>\n",
       "      <th>loss</th>\n",
       "      <th>type</th>\n",
       "      <th>prob_0</th>\n",
       "      <th>is_wrong</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>Over half of poll respondents worry nuclear disaster fading from public consciousness http://t.co/YtnnnD631z ##fukushima</td>\n",
       "      <td>nuclear%20disaster</td>\n",
       "      <td>Fukushima city Fukushima.pref</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998309</td>\n",
       "      <td>True</td>\n",
       "      <td>120</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>tensor(0.0017)</td>\n",
       "      <td>FP</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>True</td>\n",
       "      <td>0.998309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>Angry Woman Openly Accuses NEMA Of Stealing Relief Materials Meant For IDPs: An angry Internally Displaced wom... http://t.co/6ySbCSSzYS</td>\n",
       "      <td>displaced</td>\n",
       "      <td>Nigeria</td>\n",
       "      <td>0</td>\n",
       "      <td>0.110294</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998276</td>\n",
       "      <td>True</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>tensor(0.0017)</td>\n",
       "      <td>FP</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>True</td>\n",
       "      <td>0.998276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>#hot  C-130 specially modified to land in a stadium and rescue hostages in Iran in 1980 http://t.co/zY3hpdJNwg #prebreak #best</td>\n",
       "      <td>hostages</td>\n",
       "      <td>china</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998269</td>\n",
       "      <td>True</td>\n",
       "      <td>126</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>tensor(0.0017)</td>\n",
       "      <td>FP</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>True</td>\n",
       "      <td>0.998269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Satellite Spies Super Typhoon Soudelor from Space (Photo) http://t.co/VBhu2t8wgB</td>\n",
       "      <td>typhoon</td>\n",
       "      <td>Evergreen Colorado</td>\n",
       "      <td>0</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998245</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>tensor(0.0018)</td>\n",
       "      <td>FP</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>True</td>\n",
       "      <td>0.998245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>Angry Woman Openly Accuses NEMA Of Stealing Relief Materials Meant For IDPs: An angry Internally Displaced wom... http://t.co/Khd99oZ7u3</td>\n",
       "      <td>displaced</td>\n",
       "      <td>Ojodu,Lagos</td>\n",
       "      <td>0</td>\n",
       "      <td>0.110294</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998168</td>\n",
       "      <td>True</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>tensor(0.0018)</td>\n",
       "      <td>FP</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>True</td>\n",
       "      <td>0.998168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                          text  \\\n",
       "770                   Over half of poll respondents worry nuclear disaster fading from public consciousness http://t.co/YtnnnD631z ##fukushima   \n",
       "1877  Angry Woman Openly Accuses NEMA Of Stealing Relief Materials Meant For IDPs: An angry Internally Displaced wom... http://t.co/6ySbCSSzYS   \n",
       "305             #hot  C-130 specially modified to land in a stadium and rescue hostages in Iran in 1980 http://t.co/zY3hpdJNwg #prebreak #best   \n",
       "89                                                            Satellite Spies Super Typhoon Soudelor from Space (Photo) http://t.co/VBhu2t8wgB   \n",
       "378   Angry Woman Openly Accuses NEMA Of Stealing Relief Materials Meant For IDPs: An angry Internally Displaced wom... http://t.co/Khd99oZ7u3   \n",
       "\n",
       "                 keyword                       location  label  cap_ratio  \\\n",
       "770   nuclear%20disaster  Fukushima city Fukushima.pref      0   0.008333   \n",
       "1877           displaced                        Nigeria      0   0.110294   \n",
       "305             hostages                          china      0   0.015873   \n",
       "89               typhoon             Evergreen Colorado      0   0.075000   \n",
       "378            displaced                    Ojodu,Lagos      0   0.110294   \n",
       "\n",
       "      pred    prob_1  contains_link  tweet_len  hashtags  is_location  \\\n",
       "770      1  0.998309           True        120         2         True   \n",
       "1877     1  0.998276           True        136         0         True   \n",
       "305      1  0.998269           True        126         3         True   \n",
       "89       1  0.998245           True         80         0         True   \n",
       "378      1  0.998168           True        136         0         True   \n",
       "\n",
       "                loss type    prob_0  is_wrong  confidence  \n",
       "770   tensor(0.0017)   FP  0.001691      True    0.998309  \n",
       "1877  tensor(0.0017)   FP  0.001724      True    0.998276  \n",
       "305   tensor(0.0017)   FP  0.001731      True    0.998269  \n",
       "89    tensor(0.0018)   FP  0.001755      True    0.998245  \n",
       "378   tensor(0.0018)   FP  0.001832      True    0.998168  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_results[\"prob_0\"] = 1 - df_valid_results[\"prob_1\"]\n",
    "df_valid_results[\"is_wrong\"] = (df_valid_results[\"label\"] != df_valid_results[\"pred\"])\n",
    "df_valid_results[\"confidence\"] = df_valid_results[[\"prob_1\", \"prob_0\"]].max(axis=1)\n",
    "\n",
    "conf_sorted = df_valid_results.sort_values(by=\"confidence\", ascending=False)\n",
    "filtered = conf_sorted[conf_sorted[\"is_wrong\"] == True]\n",
    "\n",
    "\n",
    "filtered.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec54a2e8-e889-48a2-b4e7-38f432f8dd4f",
   "metadata": {},
   "source": [
    "These are the tweets that the model was most confident about that it got wrong. Unfortunately, I never really was able to extract more meaningful data from the dataset. When I compare features like keyword, location, has_link, or hashtags, there just isn't much difference between the wrong predictions and the full dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f54a2ae-1c69-4f34-99fe-d65d180ecee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyword 0.007878151260504201 0.017595307917888565\n",
      "location 0.3382352941176471 0.31671554252199413\n",
      "contains_link 0.5341386554621849 0.5043988269794721\n",
      "hashtags 0.7657563025210085 0.7771260997067448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "keyword\n",
       "bioterror              6\n",
       "hellfire               5\n",
       "pandemonium            5\n",
       "burning%20buildings    4\n",
       "fire                   4\n",
       "                      ..\n",
       "sirens                 1\n",
       "wild%20fires           1\n",
       "bombed                 1\n",
       "injured                1\n",
       "landslide              1\n",
       "Name: count, Length: 168, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_preds(feature):\n",
    "    full_set_data, wrongs_only_data = conf_sorted, filtered\n",
    "    max_total_perc = full_set_data[feature].value_counts().max()/len(conf_sorted)\n",
    "    max_wrong_perc = wrongs_only_data[feature].value_counts().max()/len(filtered)\n",
    "    print(feature, max_total_perc, max_wrong_perc)\n",
    "\n",
    "\n",
    "compare_preds('keyword')\n",
    "compare_preds('location')\n",
    "compare_preds('contains_link')\n",
    "compare_preds('hashtags')\n",
    "filtered['keyword'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876412a2-4654-47f1-8a4e-f9b2f643912e",
   "metadata": {},
   "source": [
    "We see that the keywords values is significantly different in the incorrect portion of the dataset. But again, the only meaningful way to split this would be to break up each of these problem keywords into a train/valid proportion. We would hope that the model would generalize for the first few keywords and be able to predict the others two but that doesn't seem likely. Unfortunately, it seems like the bins for each keyword are too small for the model to make meaningful generalizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b7e87a-872f-4dcf-80a3-6eceb9e4a2ea",
   "metadata": {},
   "source": [
    "# Model Tweaking\n",
    "\n",
    "I think I just want to try some small tweaks now to see if we can optimize the results. I thought about using another model to get sentiment analysis for the tweets, but I'm not sure if I want to do that right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "670c216d-ee9c-44ed-a19b-82f4a05b3beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions that expedite turning new inputs into training data\n",
    "epochs = 4\n",
    "bs = 128\n",
    "lr = 8e-5\n",
    "wd = 0.01\n",
    "\n",
    "def get_dds(df):\n",
    "    inps = \"location\", \"keyword\", \"text\"\n",
    "    ds = Dataset.from_pandas(df).rename_column('target', 'label')\n",
    "    tok_ds = ds.map(tok_func, batched=True, remove_columns=inps)\n",
    "    dds = tok_ds.train_test_split(0.25, seed=52)\n",
    "    return dds\n",
    "\n",
    "def get_model(): return AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=2)\n",
    "\n",
    "def get_trainer(dds, model=None):\n",
    "    if model is None: model = get_model()\n",
    "    args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
    "        evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "        num_train_epochs=epochs, weight_decay=wd, report_to='none')\n",
    "    return Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
    "                   tokenizer=tokz, compute_metrics=compute_metrics)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f00d0c-45b2-4cfd-8694-7592553bf168",
   "metadata": {},
   "source": [
    "Now I can just run some crazy experiemnts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cbabafc-abb3-4e1c-94e6-2fa430d04a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                                    LOC: none; KW: none; TEXT: Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "1                                                                                                   LOC: none; KW: none; TEXT: Forest fire near La Ronge Sask. Canada\n",
      "2    LOC: none; KW: none; TEXT: All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n",
      "3                                                                        LOC: none; KW: none; TEXT: 13,000 people receive #wildfires evacuation orders in California \n",
      "4                                                 LOC: none; KW: none; TEXT: Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school \n",
      "Name: inputs, dtype: object\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd42ace8cf9476eb7e62b511ad69c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 00:28, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.435222</td>\n",
       "      <td>0.743494</td>\n",
       "      <td>0.818803</td>\n",
       "      <td>0.899281</td>\n",
       "      <td>0.633714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.412216</td>\n",
       "      <td>0.797673</td>\n",
       "      <td>0.835609</td>\n",
       "      <td>0.813984</td>\n",
       "      <td>0.782003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.490506</td>\n",
       "      <td>0.782878</td>\n",
       "      <td>0.816176</td>\n",
       "      <td>0.766707</td>\n",
       "      <td>0.799747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.489541</td>\n",
       "      <td>0.789371</td>\n",
       "      <td>0.829307</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.771863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=180, training_loss=0.35910987854003906, metrics={'train_runtime': 28.2531, 'train_samples_per_second': 808.264, 'train_steps_per_second': 6.371, 'total_flos': 464396789823708.0, 'train_loss': 0.35910987854003906, 'epoch': 4.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the effect of switching the fill string\n",
    "na_fill = 'none'\n",
    "df[input_col] = 'LOC: ' + df.location.fillna(na_fill) + '; KW: ' + df.keyword.fillna(na_fill) + '; TEXT: ' + df.text.fillna(na_fill)\n",
    "\n",
    "print(df.inputs.head())\n",
    "dds = get_dds(df)\n",
    "get_trainer(dds).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfb215d-bcd0-44d4-8347-a57e73f182e1",
   "metadata": {},
   "source": [
    "Doesn't seem to be a large effect. Does lowercasing the strings help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13941592-70b6-4569-8663-098b129471da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5948d4d57fdc4d058b3b91c457b2a893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 00:27, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.440292</td>\n",
       "      <td>0.733683</td>\n",
       "      <td>0.813550</td>\n",
       "      <td>0.898897</td>\n",
       "      <td>0.619772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.412970</td>\n",
       "      <td>0.790576</td>\n",
       "      <td>0.831933</td>\n",
       "      <td>0.817321</td>\n",
       "      <td>0.765526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.454561</td>\n",
       "      <td>0.795322</td>\n",
       "      <td>0.834559</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.775665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.493377</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.829832</td>\n",
       "      <td>0.799228</td>\n",
       "      <td>0.787072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=180, training_loss=0.3674246470133464, metrics={'train_runtime': 27.6703, 'train_samples_per_second': 825.29, 'train_steps_per_second': 6.505, 'total_flos': 449771816332608.0, 'train_loss': 0.3674246470133464, 'epoch': 4.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_fill = ''\n",
    "df[input_col] = 'LOC: ' + df.location.fillna(na_fill) + '; KW: ' + df.keyword.fillna(na_fill) + '; TEXT: ' + df.text.fillna(na_fill)\n",
    "df[input_col] = df.inputs.str.lower()\n",
    "\n",
    "\n",
    "dds = get_dds(df)\n",
    "get_trainer(dds).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514e4bc9-f7a2-4bb7-a825-ef96db557f6d",
   "metadata": {},
   "source": [
    "What about removing the \"%20\"s from the keywords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecbdc1f7-6cd6-4553-87a5-cb5aada29f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82c1ffa24404edbaeece11506046e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 00:28, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.443140</td>\n",
       "      <td>0.737075</td>\n",
       "      <td>0.813025</td>\n",
       "      <td>0.883186</td>\n",
       "      <td>0.632446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.403884</td>\n",
       "      <td>0.785333</td>\n",
       "      <td>0.830882</td>\n",
       "      <td>0.828411</td>\n",
       "      <td>0.746515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.468701</td>\n",
       "      <td>0.784762</td>\n",
       "      <td>0.821954</td>\n",
       "      <td>0.786260</td>\n",
       "      <td>0.783270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.497474</td>\n",
       "      <td>0.789809</td>\n",
       "      <td>0.826681</td>\n",
       "      <td>0.793854</td>\n",
       "      <td>0.785805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=180, training_loss=0.35947723388671876, metrics={'train_runtime': 28.3494, 'train_samples_per_second': 805.518, 'train_steps_per_second': 6.349, 'total_flos': 461217459009756.0, 'train_loss': 0.35947723388671876, 'epoch': 4.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_fill = ''\n",
    "df[input_col] = 'LOC: ' + df.location.fillna(na_fill) + '; KW: ' + df.keyword.fillna(na_fill).str.replace(\"%20\", ' ') + '; TEXT: ' + df.text.fillna(na_fill)\n",
    "\n",
    "dds = get_dds(df)\n",
    "get_trainer(dds).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea7305d-bc63-474d-9871-76d9b9231e8b",
   "metadata": {},
   "source": [
    "None of these transformations seem to be helping us become more accurate. There are several more experiments that we could run, but I had another idea that I wanted to try first..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971b6073-638e-4ff8-b237-6b01dbb6b554",
   "metadata": {},
   "source": [
    "## Special Tokens?\n",
    "\n",
    "I noticed that the high-confidence incorrect predictions above all had links. I wonder what ahppens if I make some subset of links, tags, and hashtags into special tokens. I think it makes more sense to do it with tags and links because the text of each of those doesn't have much to do with the meaning of the data. The hashtag, however, does contain useful information. But I'll run some experiments and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3652aee-aeef-4232-981e-8b9da9433af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae2bff470d6437ba06ad168d07757a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 00:24, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.357041</td>\n",
       "      <td>0.801756</td>\n",
       "      <td>0.857668</td>\n",
       "      <td>0.948097</td>\n",
       "      <td>0.694550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.344914</td>\n",
       "      <td>0.837116</td>\n",
       "      <td>0.871849</td>\n",
       "      <td>0.884344</td>\n",
       "      <td>0.794677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.431433</td>\n",
       "      <td>0.828608</td>\n",
       "      <td>0.860294</td>\n",
       "      <td>0.842726</td>\n",
       "      <td>0.814956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.467705</td>\n",
       "      <td>0.826142</td>\n",
       "      <td>0.856092</td>\n",
       "      <td>0.827192</td>\n",
       "      <td>0.825095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=180, training_loss=0.22589988708496095, metrics={'train_runtime': 24.4876, 'train_samples_per_second': 932.552, 'train_steps_per_second': 7.351, 'total_flos': 374670733971756.0, 'train_loss': 0.22589988708496095, 'epoch': 4.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new special tokens\n",
    "new_toks = [\"[L]\", \"[A]\", \"[X]\"]\n",
    "tokz.add_special_tokens({'additional_special_tokens': new_toks})\n",
    "\n",
    "# remove links and replace with [L]\n",
    "df.loc[:, 'mod_text'] = (\n",
    "    df['text']\n",
    "    .astype(str) \n",
    "    .str.replace(r'https?://\\S+', '[L]', regex=True))   # Match http or https links\n",
    "\n",
    "\n",
    "na_fill = ''\n",
    "df[input_col] = 'LOC: ' + df.location.fillna(na_fill) + '; KW: ' + df.keyword.fillna(na_fill) + '; TEXT: ' + df.mod_text.fillna(na_fill)\n",
    "\n",
    "dds = get_dds(df)\n",
    "get_trainer(dds, model).train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb6dd5-fead-452e-8435-dff8993aaabb",
   "metadata": {},
   "source": [
    "Awesome, huge jump! This makes sense because there's no great way to tokenize links and get real meaning from them; the text of a shortened link doesn't really tell much about whats in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24b2280a-1753-4e6d-8bc4-b8394f405daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2b6f30c36847ca9fe454d70bcb753b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 00:23, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.532192</td>\n",
       "      <td>0.827950</td>\n",
       "      <td>0.862920</td>\n",
       "      <td>0.862637</td>\n",
       "      <td>0.795944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.574797</td>\n",
       "      <td>0.800705</td>\n",
       "      <td>0.821954</td>\n",
       "      <td>0.746711</td>\n",
       "      <td>0.863118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.538084</td>\n",
       "      <td>0.822023</td>\n",
       "      <td>0.853992</td>\n",
       "      <td>0.830530</td>\n",
       "      <td>0.813688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.640214</td>\n",
       "      <td>0.824639</td>\n",
       "      <td>0.853466</td>\n",
       "      <td>0.817955</td>\n",
       "      <td>0.831432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=180, training_loss=0.12367356618245443, metrics={'train_runtime': 23.7139, 'train_samples_per_second': 962.979, 'train_steps_per_second': 7.59, 'total_flos': 362423531491416.0, 'train_loss': 0.12367356618245443, 'epoch': 4.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep previous formatting and remove mentions replace with [A]\n",
    "df.loc[:, 'mod_text'] = (\n",
    "    df['text']\n",
    "    .astype(str)\n",
    "    .str.replace(r'https?://\\S+', '[L]', regex=True)\n",
    "    .str.replace(r'@\\w+', '[A]', regex=True))\n",
    "\n",
    "na_fill = ''\n",
    "df[input_col] = 'LOC: ' + df.location.fillna(na_fill) + '; KW: ' + df.keyword.fillna(na_fill) + '; TEXT: ' + df.mod_text.fillna(na_fill)\n",
    "\n",
    "dds = get_dds(df)\n",
    "get_trainer(dds, model).train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2af71b7-2c66-4bc7-8fae-418d820ec5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4900361347454fa9b331f4f457b1cc95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 00:23, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.682632</td>\n",
       "      <td>0.799759</td>\n",
       "      <td>0.825630</td>\n",
       "      <td>0.762946</td>\n",
       "      <td>0.840304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.551201</td>\n",
       "      <td>0.802696</td>\n",
       "      <td>0.830882</td>\n",
       "      <td>0.776987</td>\n",
       "      <td>0.830165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.612302</td>\n",
       "      <td>0.800490</td>\n",
       "      <td>0.828782</td>\n",
       "      <td>0.773964</td>\n",
       "      <td>0.828897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.698242</td>\n",
       "      <td>0.804938</td>\n",
       "      <td>0.834034</td>\n",
       "      <td>0.784597</td>\n",
       "      <td>0.826362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=180, training_loss=0.08939082887437609, metrics={'train_runtime': 23.3512, 'train_samples_per_second': 977.936, 'train_steps_per_second': 7.708, 'total_flos': 352560827121540.0, 'train_loss': 0.08939082887437609, 'epoch': 4.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep previous formatting and remove hashtags replace with [X]\n",
    "df.loc[:, 'mod_text'] = (\n",
    "    df['text']\n",
    "    .astype(str)\n",
    "    .str.replace(r'https?://\\S+', '[L]', regex=True)\n",
    "    .str.replace(r'@\\w+', '[A]', regex=True)\n",
    "    .str.replace(r'#\\w+', '[X]', regex=True)\n",
    ")\n",
    "\n",
    "na_fill = ''\n",
    "df[input_col] = 'LOC: ' + df.location.fillna(na_fill) + '; KW: ' + df.keyword.fillna(na_fill) + '; TEXT: ' + df.mod_text.fillna(na_fill)\n",
    "\n",
    "dds = get_dds(df)\n",
    "get_trainer(dds, model).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff13ae92-f9fa-472f-9991-32ce951c2fb3",
   "metadata": {},
   "source": [
    "Ok, so as I predicted, the best combination is masking the links and mentions with a single new special token each. This makes sense because hashtags are often real words, whereas mentions are only rarely, and links never are. It's all about tokenization. If there are pieces of data aren't easy to tokenize, then the model spends resources attempting to do so. Or worse, it sees correlations where there aren't any."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46047aa4-b8df-4cbe-a3cb-726a03f64f63",
   "metadata": {},
   "source": [
    "# Submitting Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78772dd3-bd36-445d-b86a-bfc8ca308ace",
   "metadata": {},
   "source": [
    "I'm deciding to submit pretty early. I'm using what I know how to do so far and I want to see how far that takes me. I leave some examples of further analysis that I'd considr in order to make this a more competitive result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe8424da-d841-4b7c-aa4b-d46437b041c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1013d99b593442cb6042e61251784bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 00:23, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.948050</td>\n",
       "      <td>0.793727</td>\n",
       "      <td>0.820378</td>\n",
       "      <td>0.757192</td>\n",
       "      <td>0.833967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.806260</td>\n",
       "      <td>0.805412</td>\n",
       "      <td>0.841387</td>\n",
       "      <td>0.819135</td>\n",
       "      <td>0.792142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.134959</td>\n",
       "      <td>0.780175</td>\n",
       "      <td>0.801996</td>\n",
       "      <td>0.722462</td>\n",
       "      <td>0.847909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.833781</td>\n",
       "      <td>0.795699</td>\n",
       "      <td>0.830357</td>\n",
       "      <td>0.794192</td>\n",
       "      <td>0.797212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=180, training_loss=0.035915311177571616, metrics={'train_runtime': 23.6941, 'train_samples_per_second': 963.784, 'train_steps_per_second': 7.597, 'total_flos': 362423531491416.0, 'train_loss': 0.035915311177571616, 'epoch': 4.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rewrote dataframe transformation as a function\n",
    "def transform_df(df_x):\n",
    "    df_x.loc[:, 'mod_text'] = (\n",
    "        df_x['text']\n",
    "        .astype(str)\n",
    "        .str.replace(r'https?://\\S+', '[L]', regex=True)\n",
    "        .str.replace(r'@\\w+', '[A]', regex=True))\n",
    "    na_fill = ''\n",
    "    df_x[input_col] = 'LOC: ' + df_x.location.fillna(na_fill) + '; KW: ' + df_x.keyword.fillna(na_fill) + '; TEXT: ' + df_x.mod_text.fillna(na_fill)\n",
    "    return df_x\n",
    "    \n",
    "df_trans = transform_df(df)\n",
    "dds = get_dds(df_trans)\n",
    "trainer = get_trainer(dds, model)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71347dc9-29dc-4937-bf13-a5d23ccf77b9",
   "metadata": {},
   "source": [
    "I rewrote the transformation as a function so we can be sure we do exactly the same thing to our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc8400c8-a26e-4069-a04a-cedc3980fb48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, stay safe everyone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   0     NaN      NaN   \n",
       "1   2     NaN      NaN   \n",
       "2   3     NaN      NaN   \n",
       "3   9     NaN      NaN   \n",
       "4  11     NaN      NaN   \n",
       "\n",
       "                                                                                               text  \n",
       "0                                                                Just happened a terrible car crash  \n",
       "1                                  Heard about #earthquake is different cities, stay safe everyone.  \n",
       "2  there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all  \n",
       "3                                                          Apocalypse lighting. #Spokane #wildfires  \n",
       "4                                                     Typhoon Soudelor kills 28 in China and Taiwan  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a422a80-7ec6-40b1-b868-f18fe64bf7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a983005b3f774385b33bd96670e2035f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3263 7613\n"
     ]
    }
   ],
   "source": [
    "# Transform test dataset based on work we just did\n",
    "\n",
    "df_test_trans = transform_df(df_test)\n",
    "test_ds = Dataset.from_pandas(df_test).map(tok_func, batched=True)\n",
    "outputs = trainer.predict(test_ds)\n",
    "print(len(test_ds), len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5908b1e3-29b5-4f3b-a100-d1a726da24fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0           0\n",
       "1           2\n",
       "2           3\n",
       "3           9\n",
       "4          11\n",
       "        ...  \n",
       "3258    10861\n",
       "3259    10865\n",
       "3260    10868\n",
       "3261    10874\n",
       "3262    10875\n",
       "Name: id, Length: 3263, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = trainer.predict(test_ds)\n",
    "logits = outputs.predictions\n",
    "probs = F.softmax(tensor(logits), dim=1).numpy()\n",
    "preds = probs.argmax(axis=1)\n",
    "df_test_results = pd.DataFrame({\n",
    "    \"pred\": preds,\n",
    "})\n",
    "df_test.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb607ec-95b1-4757-bbfb-9543df3755fa",
   "metadata": {},
   "source": [
    "Now we can finally submit to kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d31381d-9aab-4b37-8413-4aae62ce6768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3263 3263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59227e87126a4ffabf32b1ad420e3c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "22746"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "submission = datasets.Dataset.from_dict({\n",
    "    'id': df_test['id'],\n",
    "    'target': df_test_results[\"pred\"]\n",
    "})\n",
    "print(len(df_test.id), len(df_test_results.pred))\n",
    "submission.to_csv('nlp_tweets_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e91e8d-224e-495d-a280-257841ce4115",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "\n",
    "So, I performed right in the middle of the pack (rank 367/784). I was able to classify the tweets at a rate of **.79895**. This makes sense. I didn't really do a mega-deep dive into the data. I mostly just wanted to get some predictions and submit them. If I have time I'll go deeper into it, and see if I can get a higher score. Here's what I would try:\n",
    "\n",
    "* I would probably try to use a different model. We used deberta for prototyping, but if we used a stronger deberta or another model we'd likely do better. Maybe something specifically trained on tweets. \n",
    "* I would add sentiment analysis to see if I can use that feature to make a more robust validation set. We never quite found a feature of the data that was harder to classify, so we couldn't make a meaningful validation set. I would want to do further research into this and see if a better validation set could be created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf939ec-6527-46b6-9c1b-c681791f837e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
