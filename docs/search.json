[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Posts",
    "section": "",
    "text": "Welcome to my blog posts!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying MNIST Digits (part 1)\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2025\n\n\nAaron James\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying MNIST Digits (part 2)\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2025\n\n\nAaron James\n\n\n\n\n\n\n\n\n\n\n\n\nTraining an Emotional Face Classifier\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning Learning",
    "section": "",
    "text": "Check out my recent posts:\nüëâ Classifying MNIST Digits (1)\nüëâ Classifying MNIST Digits (2)"
  },
  {
    "objectID": "index.html#latest-project",
    "href": "index.html#latest-project",
    "title": "Learning Learning",
    "section": "",
    "text": "Check out my recent posts:\nüëâ Classifying MNIST Digits (1)\nüëâ Classifying MNIST Digits (2)"
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p1/index.html",
    "href": "posts/2025_04_14_MNIST_p1/index.html",
    "title": "Classifying MNIST Digits (part 1)",
    "section": "",
    "text": "image\n\n\nAs I work through the fastai course, I‚Äôm committed to posting something concerning each lesson I complete. This lesson was regarding Stochastic Gradient Descent and the mathematics behind neural nets. The course instructor worked through binary classification of the mnist dataset; basically, writing software to determine if a handwritten digit was a 3 or not a 3.\nThe ‚Äúchallenge‚Äù problem this week was to use the same principles to do multi-class classification; given an image of a handwritten number, which digit is it most likely to be? This problem took me a while to do and as such I‚Äôve split the work into two posts. You can find Part 2 right here\nHere is a demo of my final model.\nI appreciate the layout of the fastai course for many reasons. One of the best parts of it is its emphasis on application. So I don‚Äôt just learn the theory, I‚Äôm immediately applying it to code that can do things I‚Äôve never done before. This week focused on what is going on behind the fastai library abstractions (or ‚Äúunder the hood‚Äù). If I was being asked to do this for a commercial application or personal project, I wouldn‚Äôt rewrite tiny components of fastai‚Äôs library that already exist. I did so here to follow the course, understand how model optimization works, and to learn more about what the library is actually doing.\n\n\n\nI learned that programming ML/DL systems and models is actually much harder than other types of software development. Typically, you know what your code is supposed to be doing and can write tests to prevent it from being obnoxiously buggy. If I have a simple function I can write a brief test that makes sure I‚Äôm getting the right output:\n\n\nCode\n# Simple code output test\ndef addition(x,y): return x+y\n\nassert(addition(3,7) == 10)\nprint(\"Test passes\")\n\nassert(addition(2,2) == 5)\nprint(\"This statement never displays because the above assertion will fail, 2+2 is not 5\")\n\n\nTest passes\n\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[1], line 7\n      4 assert(addition(3,7) == 10)\n      5 print(\"Test passes\")\n----&gt; 7 assert(addition(2,2) == 5)\n      8 print(\"This statement never displays because the above assertion will fail, 2+2 is not 5\")\n\nAssertionError: \n\n\n\nWe can‚Äôt do that so easily when writing ML/DL code. There are places where this type of test will work, but the main output doesn‚Äôt have a concrete measurable performance that can be tested in this way. Certainly we can use loss functions and metrics to determine how well our model is doing, but even these are highly subjective in the majority of applications.\nOne way we can get a sense of our model‚Äôs performance is to develop a baseline. So that‚Äôs what we‚Äôll be doing here.\nI found that baselining the multi-class (10-digit) model was really helpful in enabling me to understand how to apply the concepts differently. Working through it made it possible for me to even have a clue on how to tackle the actual model.\n\n\n\nThis is just the standard notebook initialization that loads the relevant fastai libraries\n\n\nCode\n#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n\nfrom fastai.vision.all import *\nfrom fastbook import *\n\nmatplotlib.rc('image', cmap='Greys')\n\n\n\n\n\nOk so now we need to load the full dataset that has all the digits inside of it. Took me a bit to figure out how to get the full dataset, but its just URLS.MNIST.\n\n\nCode\npath = untar_data(URLs.MNIST)\n\n\nThe dataset has been downloaded and is in memory. I can use the path variable to save a reference to its top-level directory (the folder that has the downloaded data). This helps me access the data later on because I‚Äôve saved a reference to where it exists. Now I need to prove to myself that I can navigate the file structure of the dataset, which I do in the next two codeblocks:\n\n\nCode\npath.ls()\n\n\n(#2) [Path('/home/aharon/.fastai/data/mnist_png/training'),Path('/home/aharon/.fastai/data/mnist_png/testing')]\n\n\n\n\nCode\n(path/\"training\").ls()\n\n\n(#10) [Path('/home/aharon/.fastai/data/mnist_png/training/0'),Path('/home/aharon/.fastai/data/mnist_png/training/8'),Path('/home/aharon/.fastai/data/mnist_png/training/1'),Path('/home/aharon/.fastai/data/mnist_png/training/5'),Path('/home/aharon/.fastai/data/mnist_png/training/6'),Path('/home/aharon/.fastai/data/mnist_png/training/4'),Path('/home/aharon/.fastai/data/mnist_png/training/2'),Path('/home/aharon/.fastai/data/mnist_png/training/3'),Path('/home/aharon/.fastai/data/mnist_png/training/9'),Path('/home/aharon/.fastai/data/mnist_png/training/7')]\n\n\nThis just helps me see how the data is structured. There‚Äôs two main directories (again, just folders): training and testing. And then each of these directories have 10 sub-directories; one for each numerical digit. Within each of those directories there are a large number of images that contain the handwritten pictures of those digits. The training set has 60000 images, and the testing (or validation) set has 10000.\nTo prove to myself that I understand the file structure, I‚Äôm going to write some code that grabs and displays one of the ‚Äú8‚Äùs from the training set.\n\n\nCode\n# Show some images, see if I can display one of each image\n\nmnist_training_digit_paths = [p for p in (path/\"training\").ls().sorted()]\nsample_images = []\nfor p in mnist_training_digit_paths:\n    training_digit_im_paths = [im_path for im_path in p.ls().sorted()]\n    sample_images.append(Image.open(training_digit_im_paths[42]))\nsample_images[8]\n\n\n\n\n\n\n\n\n\nWhat‚Äôs helpful here is that I‚Äôve created this variable sample_images so that I can display any number I want to (at least, in index 42 of the training set). This is great because i can more easily access other numbers; I don‚Äôt have to rewrite the whole thing just to get a different result:\n\n\nCode\nsample_images[7]\n\n\n\n\n\n\n\n\n\nAwesome! I was just trying to see if I understood the file structure and I proved to myself that I did. Now I want a single variable that contains the entire dataset so I can just type something like x['training'][6] and get all of the ‚Äú6‚Äùs in the training set. There‚Äôs two more things I have to do. I turn each image into a pytorch tensor, and I normalize the pixel intensity matrix of each image‚Ä¶ yeah, that‚Äôs certainly some high-level jargon. Basically I‚Äôm converting the data that is contained in the .png (or w/e it is) into a format that the library can use more easily.\nHere‚Äôs how I made that data structure:\n\n\nCode\ndef create_dataset(top_dir):\n    # takes a top level directory and makes the aforementioned list of 3d tensors for each digit\n    \n    # grab each of the paths for each digit directory in this folder\n    mnist_datatype_digit_paths = [p for p in top_dir.ls().sorted()] \n\n    sample_images = []\n    for p in mnist_datatype_digit_paths:\n        # grab the path to each image included for the current digit\n        training_digit_im_paths = [im_path for im_path in p.ls().sorted()] \n        # open the image as tensors for each included image\n        training_image_tensors = [tensor(Image.open(im)) for im in training_digit_im_paths] \n        # turn the list of several pixel intensity matrices into a single 3D tensor\n        stacked_digit_tensors = torch.stack(training_image_tensors).float() / 255 \n        sample_images.append(stacked_digit_tensors)\n    return sample_images\n        \nmnist_dataset = {}\nmnist_dataset[\"training\"] = create_dataset((path/\"training\"))\nmnist_dataset[\"testing\"] = create_dataset((path/\"testing\"))\nshow_image(mnist_dataset[\"training\"][7][100])\n\n\n\n\n\n\n\n\n\nThat‚Äôs one of the longer running cells, but it makes sense because its loading 70000 images into memory as tensors. But we were successful! The mnist_dataset variable now contains all of our data in a really simple way for us to access using the python programming lanuguage.\n\n\n\nRemember, we are trying to create a ML model that can determine which digit a given image is a drawing of. We need a baseline to determine if our model performs better than a non-ML method. How can we come up with a way of doing this without using an ML model?\nWell, here‚Äôs what we did in the lesson. We were only looking at a small subset of this dataset. We looked at a small percentage of the 3s and the 7s. Then, we created an ‚Äúaverage‚Äù 3. Each of these images is a 28x28 matrix of pixel intensities, ranging from 0 to 255. If we take the average 28x28 matrix of pixel intensities across the training set, we can ‚Äúsee‚Äù what the average digit looks like:\n\n\nCode\nshow_image(mnist_dataset['testing'][3].mean(0))\n\n\n\n\n\n\n\n\n\nPerfect! It‚Äôs blurry because it‚Äôs the combination of the pixels densities of a few thousand threes. And if I want the average of another number (like a 7) all I have to do is change the index.\n\n\nCode\nshow_image(mnist_dataset['testing'][7].mean(0))\n\n\n\n\n\n\n\n\n\nWith our average digit, we then realized we could compare each image to it. We could get the distance between the average three and the image we want to classify mathematically. If we were only considering 3s and 7s this is simple. If the distance between the image I‚Äôm looking at and the average 3 is less than its distance to the average 7, it‚Äôs probably a 3! This is exactly what we did for binary classification in the lesson.\nBut how do we extend this to multiclass classification? I don‚Äôt just want to know if an image is a 3 or not, I want to know exactly which digit it is. Honestly, it was pretty hard for me to imagine a way to do this at first. Here‚Äôs my initial reasoning:\n\n‚ÄúOk I‚Äôm not totally sure how to do this I think I need to make something that calculates the metrics as my goal. But I think I need to define both loss functions first. Also need something that turns the loss of a single image against each golden digit. Then, we compare our test number to each of the golden digits and find which it is closest to. What If I create a 2D tensor of vectors, and then each vector is the loss function of the input compared to each of the golden digits, shape of 1,10.‚Äù\n\nThis is basically what worked! I gave each image a 1x10 vector that had its distance to each of the 10 digits. Then classification was simply determining which of the 10 distances was smallest (more simply, which digit was it closest to).\n\n\nCode\n# Loss Functions\ndef loss_L1(x, y):\n    return (x-y).abs().mean((-1,-2))\n\ndef loss_L2(x, y):\n    # Alternatively, this is called RMSE\n    return torch.sqrt(((x - y) ** 2).mean((-1, -2)))\n\n\nWe use these calculations for distance because we can‚Äôt just subtract one matrix from the other. Some numbers in it would be positive and the others would be negative. If we took the average, it wouldn‚Äôt really indicate the distance very accurately. These two functions are the most common ones that data scientists use for comparison in this context. We‚Äôll look later at why you might use one over the other.\nSo then I continue my plan. We create the comparison matrix which is several 1x10 vectors (or a single severalx10 matrix) that has the distances to each ‚Äúaverage digit‚Äù for every image in the dataset. Then the classify_digit function turns each of those 1x10 vectors into a prediction by finding which digit corresponds to the smallest distance for the given image. Remember, we used the training data to create the average digits, and we used the testing data to see how well this method actually worked for prediction.\n\n\nCode\ndef create_comparison_matrix(d, loss):\n    mnist_distances = []\n    d_testing_images = mnist_dataset[\"testing\"][d]\n    for digit in range(len(mnist_dataset[\"testing\"])):\n        golden_digit = mnist_dataset[\"training\"][digit].mean(0)\n        mnist_distances.append(loss(d_testing_images, golden_digit))\n    # the dim parameter allows me to modify the final shape. I want output[0] to give me the first classification vector length 10\n    return torch.stack(mnist_distances, dim=1)\n\n\ndef classify_digit(d, loss):\n    comp_matrix = create_comparison_matrix(d, loss)\n    min_vals, min_indices = comp_matrix.min(dim=1)\n    ans = (min_indices == d).sum() / len(comp_matrix)\n    return ans\n\n\nWe‚Äôre basically done! Out of my own curiosity, I wanted to see which loss function would perform better:\n\n\nCode\nprint(\"Loss Function Correct Prediction Comparison: \\n\")\nprint(\"tensor([[digit , L1Acc%, L2Acc%], \\n\")\nmetrics = []\nfor d in range(10):\n    metrics.append([d, float(classify_digit(d, loss_L1)),float(classify_digit(d, loss_L2))])\nprint(tensor(metrics))\nprint(\"Average\")\nprint(tensor(metrics).mean(0))\n\n\nLoss Function Correct Prediction Comparison: \n\ntensor([[digit , L1Acc%, L2Acc%], \n\ntensor([[0.0000, 0.8153, 0.8959],\n        [1.0000, 0.9982, 0.9621],\n        [2.0000, 0.4234, 0.7568],\n        [3.0000, 0.6089, 0.8059],\n        [4.0000, 0.6680, 0.8259],\n        [5.0000, 0.3262, 0.6861],\n        [6.0000, 0.7871, 0.8633],\n        [7.0000, 0.7646, 0.8327],\n        [8.0000, 0.4425, 0.7372],\n        [9.0000, 0.7760, 0.8067]])\nAverage\ntensor([4.5000, 0.6610, 0.8173])\n\n\nOk I guess this is pretty cool. We see that the RMSE loss function (L2 Norm) performs better. Well what does better even mean? Its worst accuracy score is a 69% on the fives, whereas the L1 norm performs at a 33% for the same number. The average accuracy of L1 is 66%, whereas RMSE performs at 82%.\nWhy is RMSE so much better? The L1 norm simply takes the absolute value of the difference between the prediction and the target. The RMSE squares the difference. Very basically, this means that bigger mistakes are penalized more heavily than for the L1 norm.\nDoes this make sense for our use case? Absolutely. We are comparing the difference in intensity of each pixel in a 28x28 grid to determine image similarity. Let‚Äôs say we were comparing something to a zero. The image we were looking at had a very dark pixel in the exact center of the screen. A zero is basically a circle, so there shouldn‚Äôt be anything in the center of the image. If we could point out that a big mistake like that is much further away from a zero, then we‚Äôd be in a really good spot. Since we are comparing pixel densities, it makes a lot of sense that for images that have some pixels that are way off of the ideal image we would get more accurate scores with the RMSE vs the L2 norm.\nHere‚Äôs another way to visualize it.\n\n\n\nRMSE_L2_compare.png\n\n\nThe RMSE is way higher on the parts of the graph where the prediction is furthest away from the true value. Bigger mistakes are penalized much more intensely than other mistakes in the same environment. L1 says ‚ÄúOk, yeah that‚Äôs bad‚Äù, L2 says ‚ÄúThis is insane, how are you this wrong?!‚Äù\n\n\n\nThis isn‚Äôt really a ML post, but it does pave the way to understand the next part in this series. We need a baseline to assess if our ML model performs better than the alternatives. In this case we saw that using the RMSE loss we could get a classification accuracy of our validation set of 81.73%. We will aim to beat that performance using Stochastic Gradient Descent in the next post.\n\n\n\nThe hardest part of this was making the conceptual shift from realizing each image needed 10 values associated with it. Once I made that leap this became way easier.\nA big part of this was just working through some of the code and fiddly bits. I had to remember and re-teach myself why sometimes the tensor.mean(x) changes based on what you want. I didn‚Äôt include it in the post but it was part of the process.\nComparing the two loss functions was a huge lightbulb moment for me, and the side-by-side graph was really helpful. Hopefully you learn something too!"
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p1/index.html#or-classifying-mnist-digits-part-1",
    "href": "posts/2025_04_14_MNIST_p1/index.html#or-classifying-mnist-digits-part-1",
    "title": "Classifying MNIST Digits (part 1)",
    "section": "",
    "text": "image\n\n\nAs I work through the fastai course, I‚Äôm committed to posting something concerning each lesson I complete. This lesson was regarding Stochastic Gradient Descent and the mathematics behind neural nets. The course instructor worked through binary classification of the mnist dataset; basically, writing software to determine if a handwritten digit was a 3 or not a 3.\nThe ‚Äúchallenge‚Äù problem this week was to use the same principles to do multi-class classification; given an image of a handwritten number, which digit is it most likely to be? This problem took me a while to do and as such I‚Äôve split the work into two posts. You can find Part 2 right here\nHere is a demo of my final model.\nI appreciate the layout of the fastai course for many reasons. One of the best parts of it is its emphasis on application. So I don‚Äôt just learn the theory, I‚Äôm immediately applying it to code that can do things I‚Äôve never done before. This week focused on what is going on behind the fastai library abstractions (or ‚Äúunder the hood‚Äù). If I was being asked to do this for a commercial application or personal project, I wouldn‚Äôt rewrite tiny components of fastai‚Äôs library that already exist. I did so here to follow the course, understand how model optimization works, and to learn more about what the library is actually doing."
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p1/index.html#setting-up-a-performance-baseline",
    "href": "posts/2025_04_14_MNIST_p1/index.html#setting-up-a-performance-baseline",
    "title": "Classifying MNIST Digits (part 1)",
    "section": "",
    "text": "I learned that programming ML/DL systems and models is actually much harder than other types of software development. Typically, you know what your code is supposed to be doing and can write tests to prevent it from being obnoxiously buggy. If I have a simple function I can write a brief test that makes sure I‚Äôm getting the right output:\n\n\nCode\n# Simple code output test\ndef addition(x,y): return x+y\n\nassert(addition(3,7) == 10)\nprint(\"Test passes\")\n\nassert(addition(2,2) == 5)\nprint(\"This statement never displays because the above assertion will fail, 2+2 is not 5\")\n\n\nTest passes\n\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[1], line 7\n      4 assert(addition(3,7) == 10)\n      5 print(\"Test passes\")\n----&gt; 7 assert(addition(2,2) == 5)\n      8 print(\"This statement never displays because the above assertion will fail, 2+2 is not 5\")\n\nAssertionError: \n\n\n\nWe can‚Äôt do that so easily when writing ML/DL code. There are places where this type of test will work, but the main output doesn‚Äôt have a concrete measurable performance that can be tested in this way. Certainly we can use loss functions and metrics to determine how well our model is doing, but even these are highly subjective in the majority of applications.\nOne way we can get a sense of our model‚Äôs performance is to develop a baseline. So that‚Äôs what we‚Äôll be doing here.\nI found that baselining the multi-class (10-digit) model was really helpful in enabling me to understand how to apply the concepts differently. Working through it made it possible for me to even have a clue on how to tackle the actual model."
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p1/index.html#initializing-the-notebook-and-libraries",
    "href": "posts/2025_04_14_MNIST_p1/index.html#initializing-the-notebook-and-libraries",
    "title": "Classifying MNIST Digits (part 1)",
    "section": "",
    "text": "This is just the standard notebook initialization that loads the relevant fastai libraries\n\n\nCode\n#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n\nfrom fastai.vision.all import *\nfrom fastbook import *\n\nmatplotlib.rc('image', cmap='Greys')"
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p1/index.html#understanding-and-manipulating-the-dataset",
    "href": "posts/2025_04_14_MNIST_p1/index.html#understanding-and-manipulating-the-dataset",
    "title": "Classifying MNIST Digits (part 1)",
    "section": "",
    "text": "Ok so now we need to load the full dataset that has all the digits inside of it. Took me a bit to figure out how to get the full dataset, but its just URLS.MNIST.\n\n\nCode\npath = untar_data(URLs.MNIST)\n\n\nThe dataset has been downloaded and is in memory. I can use the path variable to save a reference to its top-level directory (the folder that has the downloaded data). This helps me access the data later on because I‚Äôve saved a reference to where it exists. Now I need to prove to myself that I can navigate the file structure of the dataset, which I do in the next two codeblocks:\n\n\nCode\npath.ls()\n\n\n(#2) [Path('/home/aharon/.fastai/data/mnist_png/training'),Path('/home/aharon/.fastai/data/mnist_png/testing')]\n\n\n\n\nCode\n(path/\"training\").ls()\n\n\n(#10) [Path('/home/aharon/.fastai/data/mnist_png/training/0'),Path('/home/aharon/.fastai/data/mnist_png/training/8'),Path('/home/aharon/.fastai/data/mnist_png/training/1'),Path('/home/aharon/.fastai/data/mnist_png/training/5'),Path('/home/aharon/.fastai/data/mnist_png/training/6'),Path('/home/aharon/.fastai/data/mnist_png/training/4'),Path('/home/aharon/.fastai/data/mnist_png/training/2'),Path('/home/aharon/.fastai/data/mnist_png/training/3'),Path('/home/aharon/.fastai/data/mnist_png/training/9'),Path('/home/aharon/.fastai/data/mnist_png/training/7')]\n\n\nThis just helps me see how the data is structured. There‚Äôs two main directories (again, just folders): training and testing. And then each of these directories have 10 sub-directories; one for each numerical digit. Within each of those directories there are a large number of images that contain the handwritten pictures of those digits. The training set has 60000 images, and the testing (or validation) set has 10000.\nTo prove to myself that I understand the file structure, I‚Äôm going to write some code that grabs and displays one of the ‚Äú8‚Äùs from the training set.\n\n\nCode\n# Show some images, see if I can display one of each image\n\nmnist_training_digit_paths = [p for p in (path/\"training\").ls().sorted()]\nsample_images = []\nfor p in mnist_training_digit_paths:\n    training_digit_im_paths = [im_path for im_path in p.ls().sorted()]\n    sample_images.append(Image.open(training_digit_im_paths[42]))\nsample_images[8]\n\n\n\n\n\n\n\n\n\nWhat‚Äôs helpful here is that I‚Äôve created this variable sample_images so that I can display any number I want to (at least, in index 42 of the training set). This is great because i can more easily access other numbers; I don‚Äôt have to rewrite the whole thing just to get a different result:\n\n\nCode\nsample_images[7]\n\n\n\n\n\n\n\n\n\nAwesome! I was just trying to see if I understood the file structure and I proved to myself that I did. Now I want a single variable that contains the entire dataset so I can just type something like x['training'][6] and get all of the ‚Äú6‚Äùs in the training set. There‚Äôs two more things I have to do. I turn each image into a pytorch tensor, and I normalize the pixel intensity matrix of each image‚Ä¶ yeah, that‚Äôs certainly some high-level jargon. Basically I‚Äôm converting the data that is contained in the .png (or w/e it is) into a format that the library can use more easily.\nHere‚Äôs how I made that data structure:\n\n\nCode\ndef create_dataset(top_dir):\n    # takes a top level directory and makes the aforementioned list of 3d tensors for each digit\n    \n    # grab each of the paths for each digit directory in this folder\n    mnist_datatype_digit_paths = [p for p in top_dir.ls().sorted()] \n\n    sample_images = []\n    for p in mnist_datatype_digit_paths:\n        # grab the path to each image included for the current digit\n        training_digit_im_paths = [im_path for im_path in p.ls().sorted()] \n        # open the image as tensors for each included image\n        training_image_tensors = [tensor(Image.open(im)) for im in training_digit_im_paths] \n        # turn the list of several pixel intensity matrices into a single 3D tensor\n        stacked_digit_tensors = torch.stack(training_image_tensors).float() / 255 \n        sample_images.append(stacked_digit_tensors)\n    return sample_images\n        \nmnist_dataset = {}\nmnist_dataset[\"training\"] = create_dataset((path/\"training\"))\nmnist_dataset[\"testing\"] = create_dataset((path/\"testing\"))\nshow_image(mnist_dataset[\"training\"][7][100])\n\n\n\n\n\n\n\n\n\nThat‚Äôs one of the longer running cells, but it makes sense because its loading 70000 images into memory as tensors. But we were successful! The mnist_dataset variable now contains all of our data in a really simple way for us to access using the python programming lanuguage."
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p1/index.html#defining-our-baseline",
    "href": "posts/2025_04_14_MNIST_p1/index.html#defining-our-baseline",
    "title": "Classifying MNIST Digits (part 1)",
    "section": "",
    "text": "Remember, we are trying to create a ML model that can determine which digit a given image is a drawing of. We need a baseline to determine if our model performs better than a non-ML method. How can we come up with a way of doing this without using an ML model?\nWell, here‚Äôs what we did in the lesson. We were only looking at a small subset of this dataset. We looked at a small percentage of the 3s and the 7s. Then, we created an ‚Äúaverage‚Äù 3. Each of these images is a 28x28 matrix of pixel intensities, ranging from 0 to 255. If we take the average 28x28 matrix of pixel intensities across the training set, we can ‚Äúsee‚Äù what the average digit looks like:\n\n\nCode\nshow_image(mnist_dataset['testing'][3].mean(0))\n\n\n\n\n\n\n\n\n\nPerfect! It‚Äôs blurry because it‚Äôs the combination of the pixels densities of a few thousand threes. And if I want the average of another number (like a 7) all I have to do is change the index.\n\n\nCode\nshow_image(mnist_dataset['testing'][7].mean(0))\n\n\n\n\n\n\n\n\n\nWith our average digit, we then realized we could compare each image to it. We could get the distance between the average three and the image we want to classify mathematically. If we were only considering 3s and 7s this is simple. If the distance between the image I‚Äôm looking at and the average 3 is less than its distance to the average 7, it‚Äôs probably a 3! This is exactly what we did for binary classification in the lesson.\nBut how do we extend this to multiclass classification? I don‚Äôt just want to know if an image is a 3 or not, I want to know exactly which digit it is. Honestly, it was pretty hard for me to imagine a way to do this at first. Here‚Äôs my initial reasoning:\n\n‚ÄúOk I‚Äôm not totally sure how to do this I think I need to make something that calculates the metrics as my goal. But I think I need to define both loss functions first. Also need something that turns the loss of a single image against each golden digit. Then, we compare our test number to each of the golden digits and find which it is closest to. What If I create a 2D tensor of vectors, and then each vector is the loss function of the input compared to each of the golden digits, shape of 1,10.‚Äù\n\nThis is basically what worked! I gave each image a 1x10 vector that had its distance to each of the 10 digits. Then classification was simply determining which of the 10 distances was smallest (more simply, which digit was it closest to).\n\n\nCode\n# Loss Functions\ndef loss_L1(x, y):\n    return (x-y).abs().mean((-1,-2))\n\ndef loss_L2(x, y):\n    # Alternatively, this is called RMSE\n    return torch.sqrt(((x - y) ** 2).mean((-1, -2)))\n\n\nWe use these calculations for distance because we can‚Äôt just subtract one matrix from the other. Some numbers in it would be positive and the others would be negative. If we took the average, it wouldn‚Äôt really indicate the distance very accurately. These two functions are the most common ones that data scientists use for comparison in this context. We‚Äôll look later at why you might use one over the other.\nSo then I continue my plan. We create the comparison matrix which is several 1x10 vectors (or a single severalx10 matrix) that has the distances to each ‚Äúaverage digit‚Äù for every image in the dataset. Then the classify_digit function turns each of those 1x10 vectors into a prediction by finding which digit corresponds to the smallest distance for the given image. Remember, we used the training data to create the average digits, and we used the testing data to see how well this method actually worked for prediction.\n\n\nCode\ndef create_comparison_matrix(d, loss):\n    mnist_distances = []\n    d_testing_images = mnist_dataset[\"testing\"][d]\n    for digit in range(len(mnist_dataset[\"testing\"])):\n        golden_digit = mnist_dataset[\"training\"][digit].mean(0)\n        mnist_distances.append(loss(d_testing_images, golden_digit))\n    # the dim parameter allows me to modify the final shape. I want output[0] to give me the first classification vector length 10\n    return torch.stack(mnist_distances, dim=1)\n\n\ndef classify_digit(d, loss):\n    comp_matrix = create_comparison_matrix(d, loss)\n    min_vals, min_indices = comp_matrix.min(dim=1)\n    ans = (min_indices == d).sum() / len(comp_matrix)\n    return ans\n\n\nWe‚Äôre basically done! Out of my own curiosity, I wanted to see which loss function would perform better:\n\n\nCode\nprint(\"Loss Function Correct Prediction Comparison: \\n\")\nprint(\"tensor([[digit , L1Acc%, L2Acc%], \\n\")\nmetrics = []\nfor d in range(10):\n    metrics.append([d, float(classify_digit(d, loss_L1)),float(classify_digit(d, loss_L2))])\nprint(tensor(metrics))\nprint(\"Average\")\nprint(tensor(metrics).mean(0))\n\n\nLoss Function Correct Prediction Comparison: \n\ntensor([[digit , L1Acc%, L2Acc%], \n\ntensor([[0.0000, 0.8153, 0.8959],\n        [1.0000, 0.9982, 0.9621],\n        [2.0000, 0.4234, 0.7568],\n        [3.0000, 0.6089, 0.8059],\n        [4.0000, 0.6680, 0.8259],\n        [5.0000, 0.3262, 0.6861],\n        [6.0000, 0.7871, 0.8633],\n        [7.0000, 0.7646, 0.8327],\n        [8.0000, 0.4425, 0.7372],\n        [9.0000, 0.7760, 0.8067]])\nAverage\ntensor([4.5000, 0.6610, 0.8173])\n\n\nOk I guess this is pretty cool. We see that the RMSE loss function (L2 Norm) performs better. Well what does better even mean? Its worst accuracy score is a 69% on the fives, whereas the L1 norm performs at a 33% for the same number. The average accuracy of L1 is 66%, whereas RMSE performs at 82%.\nWhy is RMSE so much better? The L1 norm simply takes the absolute value of the difference between the prediction and the target. The RMSE squares the difference. Very basically, this means that bigger mistakes are penalized more heavily than for the L1 norm.\nDoes this make sense for our use case? Absolutely. We are comparing the difference in intensity of each pixel in a 28x28 grid to determine image similarity. Let‚Äôs say we were comparing something to a zero. The image we were looking at had a very dark pixel in the exact center of the screen. A zero is basically a circle, so there shouldn‚Äôt be anything in the center of the image. If we could point out that a big mistake like that is much further away from a zero, then we‚Äôd be in a really good spot. Since we are comparing pixel densities, it makes a lot of sense that for images that have some pixels that are way off of the ideal image we would get more accurate scores with the RMSE vs the L2 norm.\nHere‚Äôs another way to visualize it.\n\n\n\nRMSE_L2_compare.png\n\n\nThe RMSE is way higher on the parts of the graph where the prediction is furthest away from the true value. Bigger mistakes are penalized much more intensely than other mistakes in the same environment. L1 says ‚ÄúOk, yeah that‚Äôs bad‚Äù, L2 says ‚ÄúThis is insane, how are you this wrong?!‚Äù"
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p1/index.html#conclusion",
    "href": "posts/2025_04_14_MNIST_p1/index.html#conclusion",
    "title": "Classifying MNIST Digits (part 1)",
    "section": "",
    "text": "This isn‚Äôt really a ML post, but it does pave the way to understand the next part in this series. We need a baseline to assess if our ML model performs better than the alternatives. In this case we saw that using the RMSE loss we could get a classification accuracy of our validation set of 81.73%. We will aim to beat that performance using Stochastic Gradient Descent in the next post.\n\n\n\nThe hardest part of this was making the conceptual shift from realizing each image needed 10 values associated with it. Once I made that leap this became way easier.\nA big part of this was just working through some of the code and fiddly bits. I had to remember and re-teach myself why sometimes the tensor.mean(x) changes based on what you want. I didn‚Äôt include it in the post but it was part of the process.\nComparing the two loss functions was a huge lightbulb moment for me, and the side-by-side graph was really helpful. Hopefully you learn something too!"
  },
  {
    "objectID": "posts/emotions_classifier/index.html",
    "href": "posts/emotions_classifier/index.html",
    "title": "Training an Emotional Face Classifier",
    "section": "",
    "text": "image\nOk I have been trying to get this thing posted for weeks and its finally time now. First step, what am I even doing. Well, broadly the goal is to get a decent understanding of deep learning concepts and apply them to subject matter I find interesting. I see this stage as learning to read sheet music and musc theory before I can create compositions.\nThe first step is going to be training the model. It‚Äôs taken me a while to decide what I want to train it on. Originally I wanted to do something music related, but now the desire to get something UP and POSTED has made me go with my first idea of emotion recognition in human faces."
  },
  {
    "objectID": "posts/emotions_classifier/index.html#local-environment-detour",
    "href": "posts/emotions_classifier/index.html#local-environment-detour",
    "title": "Training an Emotional Face Classifier",
    "section": "Local Environment Detour",
    "text": "Local Environment Detour\nOk, its around here that I had to shift gears to set up my own local fastai and jupyter environment on my ML desktop. Long story short I built an ML workstation for this purpose. I needed to do this because I‚Äôm having trouble understanding how the cleaner modifies files and I can understand a local filesystem more clearly than google drive‚Äôs (w/ colab).\nAlso, if you‚Äôve ever tried to make a local ML environment than you‚Äôll know how PERSNICKITY this process can be (read: highly specific and thorny). So anyway, this is what I‚Äôve been doing for the last day or so. Also, my latop broke and the first one I ordered to replace it DID NOT COME WITH A KEYBOARD üôÉ\nWe soldier on‚Ä¶\nNow I have edited the files in the original path. Some were moved and others were deleted. If I rebuild another dataloader from the original path it should be more accurate this time!"
  },
  {
    "objectID": "posts/emotions_classifier/index.html#epilogue",
    "href": "posts/emotions_classifier/index.html#epilogue",
    "title": "Training an Emotional Face Classifier",
    "section": "Epilogue",
    "text": "Epilogue\nLater I wanted to see if I could get better performance with the model if I got into all the images and modified them individually like in the cleaner. It took me roughly a half hour, and tI couldn‚Äôt get it better than the second round of training. So, my assumption is that this problem (automating facial emotion ndetection) is simply non-trivial and there may be other ways to approach it. Also, determining the emotions of a face in a photo is highly subjective which could be another reason I‚Äôm not seeing better results.\nI was really surprised by how much of an effective time save using the cleaner turned out to be! The first time I used it it took around 5 minutes and I got the same results as my 30 minutes image crawl of 600 pictures."
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p2/index.html",
    "href": "posts/2025_04_14_MNIST_p2/index.html",
    "title": "Classifying MNIST Digits (part 2)",
    "section": "",
    "text": "Learning_Robot.png\n\n\nIn part 1, we created a baseline for MNIST classification without using any ML/DL tools. The best we got was an accuracy of 81.73%; that‚Äôs our target to beat with some new models!\nNow we can move onto actually doing ML and DL. We‚Äôll get into the difference shortly. First we just want to a quick code reproduction of part 1 so that we can use the output from the code and load it into memory.\nHere‚Äôs a demo of my completed model.\n\n\nCode\n#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\nfrom fastai.vision.all import *\nfrom fastbook import *\n\npath = untar_data(URLs.MNIST)\n\n# Go crazy with list comprehensions and make all the datasets we need\ndef create_dataset(top_dir):\n    # takes a top level directory and makes the aforementioned list of 3d tensors for each digit\n\n    # grab each of the paths for each digit directory in this folder\n    mnist_datatype_digit_paths = [p for p in top_dir.ls().sorted()] \n    \n    sample_images = []\n\n    for p in mnist_datatype_digit_paths:\n        # grab the path to each image included for the current digit\n        training_digit_im_paths = [im_path for im_path in p.ls().sorted()] \n        # open the image as tensors for each included image\n        training_image_tensors = [tensor(Image.open(im)) for im in training_digit_im_paths] \n        # turn the list of several pixel intensity matrices into a single 3D tensor\n        stacked_digit_tensors = torch.stack(training_image_tensors).float() / 255 \n        sample_images.append(stacked_digit_tensors)\n    return sample_images\n        \nmnist_dataset = {}\nmnist_dataset[\"training\"] = create_dataset((path/\"training\"))\nmnist_dataset[\"testing\"] = create_dataset((path/\"testing\"))\n\n\n\n\nWe are now going to solve this problem using machine learning. ‚ÄúMachine Learning‚Äù is a broad field that is all about a computer ‚Äúlearning‚Äù patterns in data, and then using those insights to make predictions or decisions. Typically there is a mathematical model that enables us to learn and apply what we‚Äôve learned. ‚ÄúDeep Learning‚Äù is a subset of ML. In DL we use layered models to do the same thing on more complicated data.\nFirst we‚Äôre going to focus on machine learning. Then we‚Äôll change our model to incorporate deep learning techniques.\n\n\n\nHow can we actually say that a computer ‚Äúlearns‚Äù something? They obviously don‚Äôt have brains. This notebook created by Jeremy Howard gives a pretty great explanation. But I‚Äôll give my own attempted summary.\nOur goal is to find a way to predict data we don‚Äôt have using data we do have. Typically, the data is something like a spreadsheet. Often there is some sort of input and output. We want to determine a way to use math to relate the inputs and the outputs of the data that we do have. If we can get that right on the data we do have, we can be relatively confident we can use it for future predictions.\n\n\nIf we were relating salary vs monthly spending, we‚Äôd probably expect a fairly linear relationship. We might expect something like:\n\\(\\text{monthlySpending} = m\\times\\text{salary} + b\\)\nIf we had that data for several hundred households, we could probably estimate the values of \\(m\\) and \\(b\\). If we could do this, then we would say that our computer ‚Äúlearned‚Äù something. We could then apply those ‚Äúlearnings‚Äù. If we had the income of other people (but not their spending) we could use this model to predict their spending.\nSo that‚Äôs how we can say that a machine has ‚Äúlearned‚Äù something; it has estimated the parameters of a mathematical function that we think approximates the trend of the dataset accurately.\n\n\n\n\nMy goal here is to create something where I give it an image and it tells me the likelihood that the image is each of the 10 digits. I don‚Äôt just want it to work for this dataset, I want it to work for any image.\nActually applying these concepts in code was pretty hard for me. I didn‚Äôt just use the easy high-level fastai provided code (libraries), I had to get into the details and create most of the training loop myself. The first thing I had to do was format the data for this process. To get this to work properly, each image needs to be associated with a label. I also had to reformat the tensors so the computer could process them better.\n\n\nCode\n# Set up training and validation datasets\ndef test_train_labels_aligned(md, dtype, train_x, train_y):\n    training_digits = 0\n    for d in range(len(md[dtype]) - 1):\n        training_digits += len(md[dtype][d])\n        assert(train_y[training_digits - 1] == d)\n        assert(train_y[training_digits] == d + 1)\n        \ntrain_x = torch.cat(mnist_dataset[\"training\"]).view(-1,28*28)\ntrain_y = tensor([d for d, items in enumerate(mnist_dataset[\"training\"]) for _ in items]).unsqueeze(1)\n\n# test that the labels lign up correctly\ntest_train_labels_aligned(mnist_dataset, \"training\", train_x, train_y)\n\nvalid_x = torch.cat(mnist_dataset[\"testing\"]).view(-1,28*28)\nvalid_y = tensor([d for d, items in enumerate(mnist_dataset[\"testing\"]) for _ in items]).unsqueeze(1)\n\ntest_train_labels_aligned(mnist_dataset, \"testing\", valid_x, valid_y)\n\n# Zip x and y together for each datatype\ndset_training = list(zip(train_x, train_y))\ndset_validation = list(zip(valid_x, valid_y))\n\n\nThis was pretty straightforward, but then I really struggled with how to do ML for 10 digits:\n‚ÄúUgh this is hard. I pretty much just thought for like a few hours straight and overnight. Turning this problem into a spreadsheet was my first step and I should include it here‚Ä¶‚Äù\n\n\n\n‚ÄúSketch Spreadsheet‚Äù\n\n\nThought Process Continued:\n\n‚ÄúI think its like, you basically want a model with 10 sets of weights and biases that each do binary prediction that a photo is a digit. So you probably have to train 10 models? How to turn this into a training loop though. But what if the (hold on, eureka incoming) what if the parameters w and b are mega tensors? So 3D tensors that have an axis that contains all the digit data predictions together? Because the thing that‚Äôs hard is the output should NOT be a number between 0 and 9. That would imply that a 2.5 would be an image that is most similar to 2 and 5, but 2 and 5 are not similar looking digits. So we have to do multi-factor classification. I‚Äôm sure there is some official way to do this but I want to try doing it myself first.‚Äù\n\n\n\n\nIt was so hard I took a step back. I needed to refresh my memory on how to do all this. I ended up rewriting a binary classification training loop for the digit 0. This model should be able to determine if any image is a zero or not a zero.\n\n\nMore fancy words to make simple things sound complicated. We talked earlier about the fact that ‚Äúlearning‚Äù means estimating the parameters of a model (math equation, function) that fits our dataset. How do we use a computer to estimate the parameters of a function? We use Stochastic Gradient Descent (SGD). The linked notebook goes into really good depth on what exactly this is. Before we start applying this step, we really need to decide on a model. We use a linear model in this notebook for our machine learning example because its one of the simplest models there is.\n\n\n\nDecide on a model: We have chosen the linear model \\(y = mx + b\\)\nInitialize parameters: We come up with some random guesses for our parameters \\(m\\) and \\(b\\).\nPredict: We use our data and our initial parameters to make a prediction. The first time we do this we will always have a very poor prediction.\nCalculate the loss: We haven‚Äôt really talked about loss yet. We brushed on it in the last post. Basically, we need a numerical way of determining how wrong we are. We compared L1 norm and L2 norm (RMSE) in the last post, and it turned out that RMSE worked better for our dataset and current use case. Keep in mind that this is now another equation. Our model is about predicting the data, and our loss is about estimating the accuracy of our data. Two different equations.\nCalculate the Gradient. And here‚Äôs the hardest math part. Again here‚Äôs Jeremy‚Äôs SGD Post. Very basically, we use calculus to determine how much each parameter needs to change so that when we make our next prediction, our loss is smaller. We can use these numerical methods so we don‚Äôt have to make a random guess for our next estimate, it will be a very precise estimate based on the principles of multi-variate calculus.\nStep the parameters: We now adjust the parameters based on the gradient. We nudge them in the direction of a smaller loss that our previous gradients predicted.\nRepeat the process with Step 2. The two magic parts of this are the gradient, and the various iterations. If we can keep nudging out parameters in the right direction (with the gradient), and do it iteratively (step 6) eventually our loss will get closer and closer to zero. This is good. If loss is the difference between the prediction and the right answer, it will be very helpful if that distance is minimized\nStop. We can‚Äôt just iterate forever. We will likely have some imposed criteria that we will hit that will tell us we can stop. For example, we could iterate until we beat our baseline that we got in the last post. However we define it, we will have to stop at some point to be able to apply the learnings of our model\n\nThen I go into the code for making this all work. This first chunk for the binary 0 classifier is bad code. I‚Äôm including it because its ‚Äúfunctional‚Äù and because it documents my thought process. I wrote it mostly from memory using the steps of SGD.\n\n\nCode\ndef create_0_binary_class_dset():\n    # We modify the dset creation above\n    train_x_0 = torch.cat(mnist_dataset[\"training\"]).view(-1,28*28)\n    training_0s = len(mnist_dataset[\"training\"][0])\n    train_y_0 = tensor([1]*training_0s + [0]*(len(train_y)-training_0s)).unsqueeze(1)\n\n    valid_x_0 = torch.cat(mnist_dataset[\"testing\"]).view(-1,28*28)\n    testing_0s = len(mnist_dataset[\"testing\"][0])\n    valid_y_0 = tensor([1]*testing_0s + [0]*(len(valid_y)-testing_0s)).unsqueeze(1)\n\n    dset_train_0 = list(zip(train_x_0, train_y_0))\n    dset_valid_0 = list(zip(valid_x_0, valid_y_0))\n    \n    return dset_train_0, dset_valid_0\n\n\n\n\nCode\ndef init_params(shape, std = 1.0): return (torch.randn(shape)*std).requires_grad_()\n\ndef model_linear(xb, weights, bias): return xb@weights + bias\n\ndef loss_0_bin(preds, y_labels):\n    preds = preds.sigmoid()\n    return torch.where(y_labels == 1, 1 - preds, preds).mean()\n\ndef batch_acc(xb_preds, yb):\n    preds = xb_preds.sigmoid()\n    correct_preds = (preds &gt; 0.5) == yb\n    return correct_preds.float().mean()\n    \ndef main():\n    # Turn the dataset into randomized dataloaders\n    dset_train, dset_valid = create_0_binary_class_dset()\n    dl_train = DataLoader(dset_train, batch_size=256, shuffle=True)\n    dl_valid = DataLoader(dset_valid, batch_size=256, shuffle=True)\n\n    #1. Initialize parameters\n    weights = init_params((28*28,1))\n    bias = init_params(1)\n    lr = .75\n\n    epochs = 10\n    for _ in range(epochs):\n        for xb, yb in dl_train:\n            # 2. Make a prediction\n            # preds will be a vector with an output for each image in the batch. so 256x1\n            #print(xb.shape)\n            preds = model_linear(xb, weights, bias)\n            # 3. Calculate Loss\n            loss = loss_0_bin(preds, yb)\n            # 4. Get gradients\n            loss.backward()\n            weights.data -= weights.grad*lr\n            weights.grad.zero_()\n            bias.data -= bias.grad*lr\n            bias.grad.zero_()\n        # Now, because the params w/b have been modified slightly by each batch, we can determine how good they are now.\n        accs = [batch_acc(model_linear(xb, weights, bias), yb) for xb, yb in dl_valid]\n        print(\"accuracy\",torch.stack(accs).mean())\n\nmain()\n\n\n\naccuracy tensor(0.9007)\naccuracy tensor(0.8996)\naccuracy tensor(0.9012)\naccuracy tensor(0.9041)\naccuracy tensor(0.9012)\naccuracy tensor(0.8998)\naccuracy tensor(0.9013)\naccuracy tensor(0.9013)\naccuracy tensor(0.9028)\naccuracy tensor(0.9043)\n\n\nSomething I also had to add was determining the accuracy of each epoch (or training iteration). We want to be able to have some sense of if what we‚Äôre doing is working. I train the model on the training data, then I test of accuracy with the validation data which is what we did in the last post.\nInterestingly enough, after only one epoch we get accuracy of ~90%. Keep in mind this is still just binary classification, but it goes to show how quickly we can get acccurately tuned parameters.\n\n\n\n\n\nOk, I‚Äôm glad I took that detour. Now I can just slightly modify what I did to apply it to a multi class. I think it makes sense to go through the changes first before I show off what I built.\n\n\nThe first thing I did was make my code a bit more readable by breaking it into functions. Actually, I pretty much just used the ones from the fastai lesson.\nThere were really only 3 main pieces that had to change:\n\nInitializing the parameters: For binary classification, each image only needs one output which is essentially true/false to the question, ‚Äúis this image a zero?‚Äù. For multi-class, each image needs 10 different scores. We do this by changing our weights and biases initialization. They change from weights, bias = init_params((28*28,1)), init_params(1) to weights, bias = init_params((28*28,10)), init_params(10). A note: all I had to do was add TWO ZEROES but this took me forever to figure out‚Ä¶\nLoss Function: This was really tricky too. I‚Äôll get into it shortly.\nAccuracy Function: We just had to see if the prediction lined up with the actual correct answer. I did it by finding the index of the 1x10 vector that was highest. Since I set my data up this way, we know that that index corresponds exactly to the labels we had in our ‚Äúcorrect answers‚Äù data (y_label or targets). If I was classifying dog breeds, I‚Äôd have to come up with a different way of doing this, likely using a map that maps indices to the corresponding breed. It‚Äôs very convenient that index corresponds perfectly with the label.\n\n\n\n\nOk so I realized that we needed another one. My plan was to determine which digit that the parameters predicted the image was, compare it to the correct target labels, and then get the average number of misses per batch as my loss function. Two issues with this. First was implementation. If I use argmax on the prediction variable, I lose gradient tracking of my parameters w and b. So our whole stochastic gradient descent method falls apart. Second was conceptual. If this did work somehow, I‚Äôm not being optimal. My model would treat these two classifications of a 6 the same:\n\nHigh Confidence 6: [0.01, 0.01, 0.02, 0.02, 0.01, 0.01, 0.90, 0.01, 0.01, 0.01]\nLow Confidence 6: [0.09, 0.10, 0.08, 0.10, 0.10, 0.08, 0.13, 0.09, 0.11, 0.12]\n\nBoth of these prediction outputs predict a 6 because the seventh value is highest. However, the first output predicts it more optimally because it is significantly more confident that it is a 6 than any other digit.\nOur loss function is really important. It‚Äôs our way of telling the model EXACTLY what we want it to do using math. We don‚Äôt just want our model to get the right classification, we want it to get the right classification and be extremely confident about it.\nI just wanted to understand the math behind this better. Apparently the right way to do this problem is to use log_softmax(), where softmax is:\n\\(\\text{softmax}(r) = \\frac{e^{r_i}}{\\sum_{j} r_j}\\)\n\n\nCode\n# I just need to convince myself I understand the math of this new loss function...\nr = tensor([2.0, 1.0, 0.1])\n\nexp_r= torch.exp(r)\nnorm_exp_r = exp_r/exp_r.sum()\nassert(torch.allclose(norm_exp_r, F.softmax(r, dim=0), atol=1e-4))\nlog_norm_exp_r = torch.log(norm_exp_r)\nassert(torch.allclose(log_norm_exp_r, r.log_softmax(dim=0), atol=1e-4))\n\n\nSoftmax makes it so that the sum of the predictions vector is 1, which is essentially turning it into a vector of percentages that the current image is each of the 10 digits.\nHere‚Äôs how our loss function modifies the input to make it better and better:\n\nRaw Predictions: The index that contains the highest value is our predicted digit, no correlation to other values.\nSoftmax: Probabilities that image could be the digit at each index. Sum is 1, so these are relative to the other values in the vector. For example, if the first number is .75 and the second is .25, then the others have to all be 0, even if they were non zero at the last stage. Relativizing the prediction allows us to get more information from a single number.\nLog Softmax: This emphasizes penalty when the prediction is confidently incorrect, and still gives a small penalty when it gets the correct prediction not confidently. It‚Äôs the distribution of the -log function. It‚Äôs really similar to the way that we used RMSE to mega-penalize certain mistakes in part 1. It‚Äôs kind of like partial credit in math class. Oh, you were supposed to say this image was a 7? Were you:\n\nRight and confident? p=0.9: You don‚Äôt lose any points\nRight, but not sure? p=0.5: You lose a few point\nConfidently incorrect? p=0.1: You lose lots of points\n\n\n\n\nCode\n# Why do we take log of soft_max? Check index 6 (or the seventh digit) of the output\nhigh_conf = tensor([0.01, 0.01, 0.02, 0.02, 0.01, 0.01, 0.90, 0.01, 0.01, 0.01])\nlow_conf = tensor([0.09, 0.10, 0.08, 0.10, 0.10, 0.08, 0.13, 0.09, 0.11, 0.12])\n\nF.softmax(high_conf, dim=0), F.softmax(low_conf, dim=0), -high_conf.log_softmax(dim=0), -low_conf.log_softmax(dim=0),   \n\n\n(tensor([0.0873, 0.0873, 0.0882, 0.0882, 0.0873, 0.0873, 0.2126, 0.0873, 0.0873, 0.0873]),\n tensor([0.0990, 0.1000, 0.0980, 0.1000, 0.1000, 0.0980, 0.1030, 0.0990, 0.1010, 0.1020]),\n tensor([2.4384, 2.4384, 2.4284, 2.4284, 2.4384, 2.4384, 1.5484, 2.4384, 2.4384, 2.4384]),\n tensor([2.3127, 2.3027, 2.3227, 2.3027, 2.3027, 2.3227, 2.2727, 2.3127, 2.2927, 2.2827]))\n\n\nThe seventh prediction (corresponding to digit 6 since the first prediction is for 0) is what to focus on. With log_softmax() there is a much greater difference for the low confidence and high confidence predictions. This means that we have packed lots of information into a single digit, which is great for our model and highly optimal.\n\n\n\n\n\n\nCode\n# Basically we want the shape of logits that we had in the golden mean baseline, so our parameters have to mirror that shape\n# Declare global params variable. I know there's a better OOP way to do this, but it's chill...\n\n# Turn the dataset into randomized dataloaders\ndl_train = DataLoader(dset_training, batch_size=256, shuffle=True)\ndl_valid = DataLoader(dset_validation, batch_size=256, shuffle=True)\n\n#1. Initialize parameters\nlr = .25\nepochs = 20\nweights = init_params((28*28,10))\nbias = init_params(10)\nparams = weights,bias\n\ndef main_multi_class():\n    print(validate_epoch(linear_1))\n    for _ in range(epochs):\n        train_epoch(linear_1, lr, dl_train)\n        print(validate_epoch(linear_1))\n\ndef init_params(shape, std = 1.0): return (torch.randn(shape)*std).requires_grad_()\n\ndef linear_1(xb): \n    w, b = params\n    #print(\"model\", w, b)\n    return xb@w + b\n\ndef mnist_loss(preds, yb):\n    #print(preds.shape, yb.shape)\n    yb = yb.squeeze()\n    log_sm_probs = preds.log_softmax(dim=1)\n    return -log_sm_probs[range(len(yb)), yb].mean()\n    \ndef train_epoch(model, lr, dl):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\n\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    #print(loss.shape)\n    loss.backward()\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in dl_valid]\n    return round(torch.stack(accs).mean().item(), 4)\n\ndef batch_accuracy(preds, yb):\n    max_indices = preds.argmax(dim=1).unsqueeze(1)\n    #print(\"args\", max_indices)\n    correct = (max_indices == yb)\n    acc = correct.float().mean()\n    return acc\n\nmain_multi_class()\n\n\n0.0965\n0.7646\n0.8165\n0.8429\n0.8542\n0.8629\n0.8701\n0.8687\n0.8738\n0.8785\n0.8822\n0.8799\n0.8878\n0.8855\n0.8871\n0.8911\n0.8893\n0.8934\n0.8907\n0.8931\n0.8936\n\n\n\n\n\nSo, amazingly we have just ‚Äúsucceeded‚Äù! We have built a machine learning model that is able to classify digits in the MNIST dataset, and that outperforms our baseline classification of 81.73%! There‚Äôs a few more things to consider but this is definitely a moment to celebrate and enjoy. We‚Äôve used what we have been learning to outperform our benchmark. Good job.\nWe will now slightly optimize the code, and then we‚Äôll look at some deep learning models. Remember, the linear model is just an example of machine learning, and we‚Äôre interested in comparing the performance of both.\n\n\n\nOk, so what we did was the custom version of this. We now want to take steps to create the fastai/pyTorch extraction.\nEverything we have just completed actually works. However, we have just been looking under the hood. These details matter for my comprehension of the math and coding, but for actual training I would never create a custom training loop. This training loop is the main engine that drives most ML systems and we just built it from scratch. Think about a car engine. There‚Äôs no way I will ever need to build a car engine from scratch. If I‚Äôm building my own car, I might buy an engine, and knowledge of how it works will help me choose which to buy for the car I want to build. Most people will just end up buying the whole car and be blissfully unaware of the fact that it comes with a functioning engine.\nIn this analogy, fastai is the engine factory. So we will shape our custom engine into something closer to what gets produced in the factory. One thing to consider is that the pytorch library provides several models as a part of its packaging like nn.Linear. Instead of writing our own version of this, we want to use the one that is already provided. We also need an ‚Äúoptimizer‚Äù, so that we mirror the factory more accurately.\n\n\nCode\nlinear_model = nn.Linear(28*28,10)\n\nclass BasicOptim:\n    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n    def step(self, *args, **kwargs):\n        for p in self.params: p.data -= p.grad.data * self.lr\n\n    def zero_grad(self, *args, **kwargs):\n        for p in self.params: p.grad = None\n\nopt = BasicOptim(linear_model.parameters(), lr)\n\ndef train_epoch(model):\n    for xb,yb in dl_train:\n        #print(xb.dtype, yb.dtype)\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n\n\nTurning the parameters into class variables makes the code stronger. Earlier we had to rely on a global params variable. In our tiny use case this was no problem, but as the codebase grows global variables are a terrible idea.\nThen, we create a new optimizer and redefine our training function. This is the only change we have to make to get the same results as before!\n\n\nCode\ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch(model)\n        print(validate_epoch(model), end=' ')\n\n\nprint(validate_epoch(linear_model), end=' ')\ntrain_model(linear_model, 20)\n\n\n0.087 0.9045 0.9125 0.9155 0.9151 0.9197 0.9178 0.9211 0.9211 0.9195 0.9223 0.9227 0.9226 0.9207 0.924 0.9214 0.9234 0.9247 0.9256 0.9214 0.9247 \n\n\nPretty good. We can do the same thing the actual RIGHT way using fastai classes\n\n\nCode\nlr = .5\nlinear_model = nn.Linear(28*28,10)\nopt = SGD(linear_model.parameters(), lr)\ntrain_model(linear_model, 20)\n\ndls = DataLoaders(dl_train, dl_valid)\n\nlearn = Learner(dls, nn.Linear(28*28,10), opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\nlearn.fit(10, lr=lr)\n\n\n0.9095 0.9143 0.9174 0.9186 0.9194 0.9204 0.9245 0.9215 0.9226 0.9232 0.9242 0.9229 0.9244 0.9246 0.9243 0.9237 0.9234 0.9222 0.9218 0.9248 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.372398\n0.331982\n0.908200\n00:00\n\n\n1\n0.316947\n0.303885\n0.916700\n00:00\n\n\n2\n0.302206\n0.293496\n0.918900\n00:00\n\n\n3\n0.294819\n0.288783\n0.918600\n00:00\n\n\n4\n0.293716\n0.285339\n0.920900\n00:00\n\n\n5\n0.283038\n0.277278\n0.922900\n00:00\n\n\n6\n0.273962\n0.275699\n0.920400\n00:00\n\n\n7\n0.272748\n0.276465\n0.921400\n00:00\n\n\n8\n0.265907\n0.276644\n0.919700\n00:00\n\n\n9\n0.279539\n0.274251\n0.921500\n00:00\n\n\n\n\n\nSo what we just did was rewrite the custom engine using prebuilt parts. We proved that our custom engine was almost as functional as the store-bought version (or COTS if you prefer). I say ‚Äúalmost‚Äù because we seemed to get an accuracy cap of 90% on our custom engine, but the optimized version capped around 92%.\n\n\n\nRemember earlier we talked about the difference between ML and DL as using layers of models. The way we do this is to apply more than one model and separate those models with a non-linearity. So, more fancy words. If we had a function \\(y_1 = m_1 \\times x +b_1\\) and another function \\(y_2 = m_2 \\times x+b_2\\), we couldn‚Äôt just add these two together and expect a different type of function. If we did, we would just get \\(y_{1+2} = m_{1+2} \\times x+b_{1+2}\\). The problem is that the model doesn‚Äôt care about the difference between \\(m_1\\) and \\(m_2\\); it just see weights and a bias.\nWe can separate these models with something like a Recitified Linear Unit (or ReLu). All it does is \\(\\text{ReLu}(x)\\) turns the number into zero if its negative, otherwise it leaves it alone. This little ReLu turns two linear models into a very simple neural net.\nThe genius of this is that instead of fitting a line to data, we can fit several layers of Linear/ReLus to achieve a very precise function. Most data won‚Äôt be fit well by a linear model. However, almost all data can be fit by some arbitrarily large combination of linear models and ReLus.\n\n\n\nlinear\n\n\n\n\nCode\nsimple_net = nn.Sequential(\n    nn.Linear(28*28,30),\n    nn.ReLU(),\n    nn.Linear(30,10)\n)\n\nlearn = Learner(dls, simple_net, opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\nlr = .25\nlearn.fit(20, lr=lr)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.362304\n0.329384\n0.905600\n00:00\n\n\n1\n0.278429\n0.249775\n0.928900\n00:00\n\n\n2\n0.237960\n0.229455\n0.930400\n00:00\n\n\n3\n0.208032\n0.208346\n0.937100\n00:00\n\n\n4\n0.181758\n0.183263\n0.945700\n00:00\n\n\n5\n0.167817\n0.172169\n0.949300\n00:00\n\n\n6\n0.157353\n0.155710\n0.953900\n00:00\n\n\n7\n0.145984\n0.175108\n0.947600\n00:00\n\n\n8\n0.142243\n0.185708\n0.939700\n00:00\n\n\n9\n0.131697\n0.151334\n0.953200\n00:00\n\n\n10\n0.120169\n0.140592\n0.957600\n00:00\n\n\n11\n0.118723\n0.128918\n0.961100\n00:00\n\n\n12\n0.108958\n0.134065\n0.961300\n00:00\n\n\n13\n0.101567\n0.130075\n0.960400\n00:00\n\n\n14\n0.105227\n0.120318\n0.961100\n00:00\n\n\n15\n0.099600\n0.115329\n0.964500\n00:00\n\n\n16\n0.094892\n0.120461\n0.964300\n00:00\n\n\n17\n0.097695\n0.119797\n0.963400\n00:00\n\n\n18\n0.091835\n0.141129\n0.958500\n00:00\n\n\n19\n0.087854\n0.120385\n0.964300\n00:00\n\n\n\n\n\n\n\n\n2relu\n\n\nWe can keep going! We can use resnet18 which contains many more than 2 layers. These are the layers that we refer to when we say ‚Äúdeep learning‚Äù; its deep because it has so many layers of linear and nonlinear activations!\n\n\nCode\n\ndls = ImageDataLoaders.from_folder(\n    path,\n    train='training',\n    valid='testing',\n    valid_pct=None,        # use full test set\n    seed=42,\n    bs=64,                  # you can adjust batch size\n    item_tfms=Resize(224)   # resnet needs larger images\n)\nlearn = vision_learner(dls, resnet18, pretrained=False,\n                    loss_func=F.cross_entropy, metrics=accuracy)\nlearn.fit_one_cycle(1, 0.1)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.150041\n0.057427\n0.985800\n00:55\n\n\n\n\n\n\n\n\nDid we meet our goal? We wanted to classify digits in the MNIST dataset more accurately than our baseline of ~83%. We beat it immediately with our first ML model in around 2-3 epochs with 90%. We then optimized our code and got 92% accuracy. We used the simplest possible deep learning model, a 2-layer neural net with a non-linear activation, and got 96% accuracy. Then we used a more standard deep learning model, resnet 18, and got 98% accuracy in only one epoch. Granted, this last training took longer than the others.\nOk so we learned a lot.\n\nSaw the difference in performance between baseline, ML, and DL\nLearned how to format data and optimize code.\nGot a grounding in the mathematical principles of ML and DL\n\nThis was a hard project for me and I‚Äôm glad I finished. The hardest part was writing the code from scratch. Hopefully I never have to do that again, but at least I know I can if necessary. It‚Äôs pretty amazing to think about the potential of activating deep learning models on all sorts of intriguing datasets!"
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p2/index.html#machine-learning-vs-deep-learning",
    "href": "posts/2025_04_14_MNIST_p2/index.html#machine-learning-vs-deep-learning",
    "title": "Classifying MNIST Digits (part 2)",
    "section": "",
    "text": "We are now going to solve this problem using machine learning. ‚ÄúMachine Learning‚Äù is a broad field that is all about a computer ‚Äúlearning‚Äù patterns in data, and then using those insights to make predictions or decisions. Typically there is a mathematical model that enables us to learn and apply what we‚Äôve learned. ‚ÄúDeep Learning‚Äù is a subset of ML. In DL we use layered models to do the same thing on more complicated data.\nFirst we‚Äôre going to focus on machine learning. Then we‚Äôll change our model to incorporate deep learning techniques."
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p2/index.html#explaining-learning",
    "href": "posts/2025_04_14_MNIST_p2/index.html#explaining-learning",
    "title": "Classifying MNIST Digits (part 2)",
    "section": "",
    "text": "How can we actually say that a computer ‚Äúlearns‚Äù something? They obviously don‚Äôt have brains. This notebook created by Jeremy Howard gives a pretty great explanation. But I‚Äôll give my own attempted summary.\nOur goal is to find a way to predict data we don‚Äôt have using data we do have. Typically, the data is something like a spreadsheet. Often there is some sort of input and output. We want to determine a way to use math to relate the inputs and the outputs of the data that we do have. If we can get that right on the data we do have, we can be relatively confident we can use it for future predictions.\n\n\nIf we were relating salary vs monthly spending, we‚Äôd probably expect a fairly linear relationship. We might expect something like:\n\\(\\text{monthlySpending} = m\\times\\text{salary} + b\\)\nIf we had that data for several hundred households, we could probably estimate the values of \\(m\\) and \\(b\\). If we could do this, then we would say that our computer ‚Äúlearned‚Äù something. We could then apply those ‚Äúlearnings‚Äù. If we had the income of other people (but not their spending) we could use this model to predict their spending.\nSo that‚Äôs how we can say that a machine has ‚Äúlearned‚Äù something; it has estimated the parameters of a mathematical function that we think approximates the trend of the dataset accurately."
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p2/index.html#applying-ml-to-mnist-dataset",
    "href": "posts/2025_04_14_MNIST_p2/index.html#applying-ml-to-mnist-dataset",
    "title": "Classifying MNIST Digits (part 2)",
    "section": "",
    "text": "My goal here is to create something where I give it an image and it tells me the likelihood that the image is each of the 10 digits. I don‚Äôt just want it to work for this dataset, I want it to work for any image.\nActually applying these concepts in code was pretty hard for me. I didn‚Äôt just use the easy high-level fastai provided code (libraries), I had to get into the details and create most of the training loop myself. The first thing I had to do was format the data for this process. To get this to work properly, each image needs to be associated with a label. I also had to reformat the tensors so the computer could process them better.\n\n\nCode\n# Set up training and validation datasets\ndef test_train_labels_aligned(md, dtype, train_x, train_y):\n    training_digits = 0\n    for d in range(len(md[dtype]) - 1):\n        training_digits += len(md[dtype][d])\n        assert(train_y[training_digits - 1] == d)\n        assert(train_y[training_digits] == d + 1)\n        \ntrain_x = torch.cat(mnist_dataset[\"training\"]).view(-1,28*28)\ntrain_y = tensor([d for d, items in enumerate(mnist_dataset[\"training\"]) for _ in items]).unsqueeze(1)\n\n# test that the labels lign up correctly\ntest_train_labels_aligned(mnist_dataset, \"training\", train_x, train_y)\n\nvalid_x = torch.cat(mnist_dataset[\"testing\"]).view(-1,28*28)\nvalid_y = tensor([d for d, items in enumerate(mnist_dataset[\"testing\"]) for _ in items]).unsqueeze(1)\n\ntest_train_labels_aligned(mnist_dataset, \"testing\", valid_x, valid_y)\n\n# Zip x and y together for each datatype\ndset_training = list(zip(train_x, train_y))\ndset_validation = list(zip(valid_x, valid_y))\n\n\nThis was pretty straightforward, but then I really struggled with how to do ML for 10 digits:\n‚ÄúUgh this is hard. I pretty much just thought for like a few hours straight and overnight. Turning this problem into a spreadsheet was my first step and I should include it here‚Ä¶‚Äù\n\n\n\n‚ÄúSketch Spreadsheet‚Äù\n\n\nThought Process Continued:\n\n‚ÄúI think its like, you basically want a model with 10 sets of weights and biases that each do binary prediction that a photo is a digit. So you probably have to train 10 models? How to turn this into a training loop though. But what if the (hold on, eureka incoming) what if the parameters w and b are mega tensors? So 3D tensors that have an axis that contains all the digit data predictions together? Because the thing that‚Äôs hard is the output should NOT be a number between 0 and 9. That would imply that a 2.5 would be an image that is most similar to 2 and 5, but 2 and 5 are not similar looking digits. So we have to do multi-factor classification. I‚Äôm sure there is some official way to do this but I want to try doing it myself first.‚Äù"
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p2/index.html#a-detour-into-binary-classification",
    "href": "posts/2025_04_14_MNIST_p2/index.html#a-detour-into-binary-classification",
    "title": "Classifying MNIST Digits (part 2)",
    "section": "",
    "text": "It was so hard I took a step back. I needed to refresh my memory on how to do all this. I ended up rewriting a binary classification training loop for the digit 0. This model should be able to determine if any image is a zero or not a zero.\n\n\nMore fancy words to make simple things sound complicated. We talked earlier about the fact that ‚Äúlearning‚Äù means estimating the parameters of a model (math equation, function) that fits our dataset. How do we use a computer to estimate the parameters of a function? We use Stochastic Gradient Descent (SGD). The linked notebook goes into really good depth on what exactly this is. Before we start applying this step, we really need to decide on a model. We use a linear model in this notebook for our machine learning example because its one of the simplest models there is.\n\n\n\nDecide on a model: We have chosen the linear model \\(y = mx + b\\)\nInitialize parameters: We come up with some random guesses for our parameters \\(m\\) and \\(b\\).\nPredict: We use our data and our initial parameters to make a prediction. The first time we do this we will always have a very poor prediction.\nCalculate the loss: We haven‚Äôt really talked about loss yet. We brushed on it in the last post. Basically, we need a numerical way of determining how wrong we are. We compared L1 norm and L2 norm (RMSE) in the last post, and it turned out that RMSE worked better for our dataset and current use case. Keep in mind that this is now another equation. Our model is about predicting the data, and our loss is about estimating the accuracy of our data. Two different equations.\nCalculate the Gradient. And here‚Äôs the hardest math part. Again here‚Äôs Jeremy‚Äôs SGD Post. Very basically, we use calculus to determine how much each parameter needs to change so that when we make our next prediction, our loss is smaller. We can use these numerical methods so we don‚Äôt have to make a random guess for our next estimate, it will be a very precise estimate based on the principles of multi-variate calculus.\nStep the parameters: We now adjust the parameters based on the gradient. We nudge them in the direction of a smaller loss that our previous gradients predicted.\nRepeat the process with Step 2. The two magic parts of this are the gradient, and the various iterations. If we can keep nudging out parameters in the right direction (with the gradient), and do it iteratively (step 6) eventually our loss will get closer and closer to zero. This is good. If loss is the difference between the prediction and the right answer, it will be very helpful if that distance is minimized\nStop. We can‚Äôt just iterate forever. We will likely have some imposed criteria that we will hit that will tell us we can stop. For example, we could iterate until we beat our baseline that we got in the last post. However we define it, we will have to stop at some point to be able to apply the learnings of our model\n\nThen I go into the code for making this all work. This first chunk for the binary 0 classifier is bad code. I‚Äôm including it because its ‚Äúfunctional‚Äù and because it documents my thought process. I wrote it mostly from memory using the steps of SGD.\n\n\nCode\ndef create_0_binary_class_dset():\n    # We modify the dset creation above\n    train_x_0 = torch.cat(mnist_dataset[\"training\"]).view(-1,28*28)\n    training_0s = len(mnist_dataset[\"training\"][0])\n    train_y_0 = tensor([1]*training_0s + [0]*(len(train_y)-training_0s)).unsqueeze(1)\n\n    valid_x_0 = torch.cat(mnist_dataset[\"testing\"]).view(-1,28*28)\n    testing_0s = len(mnist_dataset[\"testing\"][0])\n    valid_y_0 = tensor([1]*testing_0s + [0]*(len(valid_y)-testing_0s)).unsqueeze(1)\n\n    dset_train_0 = list(zip(train_x_0, train_y_0))\n    dset_valid_0 = list(zip(valid_x_0, valid_y_0))\n    \n    return dset_train_0, dset_valid_0\n\n\n\n\nCode\ndef init_params(shape, std = 1.0): return (torch.randn(shape)*std).requires_grad_()\n\ndef model_linear(xb, weights, bias): return xb@weights + bias\n\ndef loss_0_bin(preds, y_labels):\n    preds = preds.sigmoid()\n    return torch.where(y_labels == 1, 1 - preds, preds).mean()\n\ndef batch_acc(xb_preds, yb):\n    preds = xb_preds.sigmoid()\n    correct_preds = (preds &gt; 0.5) == yb\n    return correct_preds.float().mean()\n    \ndef main():\n    # Turn the dataset into randomized dataloaders\n    dset_train, dset_valid = create_0_binary_class_dset()\n    dl_train = DataLoader(dset_train, batch_size=256, shuffle=True)\n    dl_valid = DataLoader(dset_valid, batch_size=256, shuffle=True)\n\n    #1. Initialize parameters\n    weights = init_params((28*28,1))\n    bias = init_params(1)\n    lr = .75\n\n    epochs = 10\n    for _ in range(epochs):\n        for xb, yb in dl_train:\n            # 2. Make a prediction\n            # preds will be a vector with an output for each image in the batch. so 256x1\n            #print(xb.shape)\n            preds = model_linear(xb, weights, bias)\n            # 3. Calculate Loss\n            loss = loss_0_bin(preds, yb)\n            # 4. Get gradients\n            loss.backward()\n            weights.data -= weights.grad*lr\n            weights.grad.zero_()\n            bias.data -= bias.grad*lr\n            bias.grad.zero_()\n        # Now, because the params w/b have been modified slightly by each batch, we can determine how good they are now.\n        accs = [batch_acc(model_linear(xb, weights, bias), yb) for xb, yb in dl_valid]\n        print(\"accuracy\",torch.stack(accs).mean())\n\nmain()\n\n\n\naccuracy tensor(0.9007)\naccuracy tensor(0.8996)\naccuracy tensor(0.9012)\naccuracy tensor(0.9041)\naccuracy tensor(0.9012)\naccuracy tensor(0.8998)\naccuracy tensor(0.9013)\naccuracy tensor(0.9013)\naccuracy tensor(0.9028)\naccuracy tensor(0.9043)\n\n\nSomething I also had to add was determining the accuracy of each epoch (or training iteration). We want to be able to have some sense of if what we‚Äôre doing is working. I train the model on the training data, then I test of accuracy with the validation data which is what we did in the last post.\nInterestingly enough, after only one epoch we get accuracy of ~90%. Keep in mind this is still just binary classification, but it goes to show how quickly we can get acccurately tuned parameters."
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p2/index.html#returning-to-full-multi-class-classification",
    "href": "posts/2025_04_14_MNIST_p2/index.html#returning-to-full-multi-class-classification",
    "title": "Classifying MNIST Digits (part 2)",
    "section": "",
    "text": "Ok, I‚Äôm glad I took that detour. Now I can just slightly modify what I did to apply it to a multi class. I think it makes sense to go through the changes first before I show off what I built.\n\n\nThe first thing I did was make my code a bit more readable by breaking it into functions. Actually, I pretty much just used the ones from the fastai lesson.\nThere were really only 3 main pieces that had to change:\n\nInitializing the parameters: For binary classification, each image only needs one output which is essentially true/false to the question, ‚Äúis this image a zero?‚Äù. For multi-class, each image needs 10 different scores. We do this by changing our weights and biases initialization. They change from weights, bias = init_params((28*28,1)), init_params(1) to weights, bias = init_params((28*28,10)), init_params(10). A note: all I had to do was add TWO ZEROES but this took me forever to figure out‚Ä¶\nLoss Function: This was really tricky too. I‚Äôll get into it shortly.\nAccuracy Function: We just had to see if the prediction lined up with the actual correct answer. I did it by finding the index of the 1x10 vector that was highest. Since I set my data up this way, we know that that index corresponds exactly to the labels we had in our ‚Äúcorrect answers‚Äù data (y_label or targets). If I was classifying dog breeds, I‚Äôd have to come up with a different way of doing this, likely using a map that maps indices to the corresponding breed. It‚Äôs very convenient that index corresponds perfectly with the label.\n\n\n\n\nOk so I realized that we needed another one. My plan was to determine which digit that the parameters predicted the image was, compare it to the correct target labels, and then get the average number of misses per batch as my loss function. Two issues with this. First was implementation. If I use argmax on the prediction variable, I lose gradient tracking of my parameters w and b. So our whole stochastic gradient descent method falls apart. Second was conceptual. If this did work somehow, I‚Äôm not being optimal. My model would treat these two classifications of a 6 the same:\n\nHigh Confidence 6: [0.01, 0.01, 0.02, 0.02, 0.01, 0.01, 0.90, 0.01, 0.01, 0.01]\nLow Confidence 6: [0.09, 0.10, 0.08, 0.10, 0.10, 0.08, 0.13, 0.09, 0.11, 0.12]\n\nBoth of these prediction outputs predict a 6 because the seventh value is highest. However, the first output predicts it more optimally because it is significantly more confident that it is a 6 than any other digit.\nOur loss function is really important. It‚Äôs our way of telling the model EXACTLY what we want it to do using math. We don‚Äôt just want our model to get the right classification, we want it to get the right classification and be extremely confident about it.\nI just wanted to understand the math behind this better. Apparently the right way to do this problem is to use log_softmax(), where softmax is:\n\\(\\text{softmax}(r) = \\frac{e^{r_i}}{\\sum_{j} r_j}\\)\n\n\nCode\n# I just need to convince myself I understand the math of this new loss function...\nr = tensor([2.0, 1.0, 0.1])\n\nexp_r= torch.exp(r)\nnorm_exp_r = exp_r/exp_r.sum()\nassert(torch.allclose(norm_exp_r, F.softmax(r, dim=0), atol=1e-4))\nlog_norm_exp_r = torch.log(norm_exp_r)\nassert(torch.allclose(log_norm_exp_r, r.log_softmax(dim=0), atol=1e-4))\n\n\nSoftmax makes it so that the sum of the predictions vector is 1, which is essentially turning it into a vector of percentages that the current image is each of the 10 digits.\nHere‚Äôs how our loss function modifies the input to make it better and better:\n\nRaw Predictions: The index that contains the highest value is our predicted digit, no correlation to other values.\nSoftmax: Probabilities that image could be the digit at each index. Sum is 1, so these are relative to the other values in the vector. For example, if the first number is .75 and the second is .25, then the others have to all be 0, even if they were non zero at the last stage. Relativizing the prediction allows us to get more information from a single number.\nLog Softmax: This emphasizes penalty when the prediction is confidently incorrect, and still gives a small penalty when it gets the correct prediction not confidently. It‚Äôs the distribution of the -log function. It‚Äôs really similar to the way that we used RMSE to mega-penalize certain mistakes in part 1. It‚Äôs kind of like partial credit in math class. Oh, you were supposed to say this image was a 7? Were you:\n\nRight and confident? p=0.9: You don‚Äôt lose any points\nRight, but not sure? p=0.5: You lose a few point\nConfidently incorrect? p=0.1: You lose lots of points\n\n\n\n\nCode\n# Why do we take log of soft_max? Check index 6 (or the seventh digit) of the output\nhigh_conf = tensor([0.01, 0.01, 0.02, 0.02, 0.01, 0.01, 0.90, 0.01, 0.01, 0.01])\nlow_conf = tensor([0.09, 0.10, 0.08, 0.10, 0.10, 0.08, 0.13, 0.09, 0.11, 0.12])\n\nF.softmax(high_conf, dim=0), F.softmax(low_conf, dim=0), -high_conf.log_softmax(dim=0), -low_conf.log_softmax(dim=0),   \n\n\n(tensor([0.0873, 0.0873, 0.0882, 0.0882, 0.0873, 0.0873, 0.2126, 0.0873, 0.0873, 0.0873]),\n tensor([0.0990, 0.1000, 0.0980, 0.1000, 0.1000, 0.0980, 0.1030, 0.0990, 0.1010, 0.1020]),\n tensor([2.4384, 2.4384, 2.4284, 2.4284, 2.4384, 2.4384, 1.5484, 2.4384, 2.4384, 2.4384]),\n tensor([2.3127, 2.3027, 2.3227, 2.3027, 2.3027, 2.3227, 2.2727, 2.3127, 2.2927, 2.2827]))\n\n\nThe seventh prediction (corresponding to digit 6 since the first prediction is for 0) is what to focus on. With log_softmax() there is a much greater difference for the low confidence and high confidence predictions. This means that we have packed lots of information into a single digit, which is great for our model and highly optimal."
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p2/index.html#final-code-for-multi-class-mnist-low-level-training-loop",
    "href": "posts/2025_04_14_MNIST_p2/index.html#final-code-for-multi-class-mnist-low-level-training-loop",
    "title": "Classifying MNIST Digits (part 2)",
    "section": "",
    "text": "Code\n# Basically we want the shape of logits that we had in the golden mean baseline, so our parameters have to mirror that shape\n# Declare global params variable. I know there's a better OOP way to do this, but it's chill...\n\n# Turn the dataset into randomized dataloaders\ndl_train = DataLoader(dset_training, batch_size=256, shuffle=True)\ndl_valid = DataLoader(dset_validation, batch_size=256, shuffle=True)\n\n#1. Initialize parameters\nlr = .25\nepochs = 20\nweights = init_params((28*28,10))\nbias = init_params(10)\nparams = weights,bias\n\ndef main_multi_class():\n    print(validate_epoch(linear_1))\n    for _ in range(epochs):\n        train_epoch(linear_1, lr, dl_train)\n        print(validate_epoch(linear_1))\n\ndef init_params(shape, std = 1.0): return (torch.randn(shape)*std).requires_grad_()\n\ndef linear_1(xb): \n    w, b = params\n    #print(\"model\", w, b)\n    return xb@w + b\n\ndef mnist_loss(preds, yb):\n    #print(preds.shape, yb.shape)\n    yb = yb.squeeze()\n    log_sm_probs = preds.log_softmax(dim=1)\n    return -log_sm_probs[range(len(yb)), yb].mean()\n    \ndef train_epoch(model, lr, dl):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\n\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    #print(loss.shape)\n    loss.backward()\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in dl_valid]\n    return round(torch.stack(accs).mean().item(), 4)\n\ndef batch_accuracy(preds, yb):\n    max_indices = preds.argmax(dim=1).unsqueeze(1)\n    #print(\"args\", max_indices)\n    correct = (max_indices == yb)\n    acc = correct.float().mean()\n    return acc\n\nmain_multi_class()\n\n\n0.0965\n0.7646\n0.8165\n0.8429\n0.8542\n0.8629\n0.8701\n0.8687\n0.8738\n0.8785\n0.8822\n0.8799\n0.8878\n0.8855\n0.8871\n0.8911\n0.8893\n0.8934\n0.8907\n0.8931\n0.8936"
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p2/index.html#a-pivot-point",
    "href": "posts/2025_04_14_MNIST_p2/index.html#a-pivot-point",
    "title": "Classifying MNIST Digits (part 2)",
    "section": "",
    "text": "So, amazingly we have just ‚Äúsucceeded‚Äù! We have built a machine learning model that is able to classify digits in the MNIST dataset, and that outperforms our baseline classification of 81.73%! There‚Äôs a few more things to consider but this is definitely a moment to celebrate and enjoy. We‚Äôve used what we have been learning to outperform our benchmark. Good job.\nWe will now slightly optimize the code, and then we‚Äôll look at some deep learning models. Remember, the linear model is just an example of machine learning, and we‚Äôre interested in comparing the performance of both."
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p2/index.html#optimizing-the-code",
    "href": "posts/2025_04_14_MNIST_p2/index.html#optimizing-the-code",
    "title": "Classifying MNIST Digits (part 2)",
    "section": "",
    "text": "Ok, so what we did was the custom version of this. We now want to take steps to create the fastai/pyTorch extraction.\nEverything we have just completed actually works. However, we have just been looking under the hood. These details matter for my comprehension of the math and coding, but for actual training I would never create a custom training loop. This training loop is the main engine that drives most ML systems and we just built it from scratch. Think about a car engine. There‚Äôs no way I will ever need to build a car engine from scratch. If I‚Äôm building my own car, I might buy an engine, and knowledge of how it works will help me choose which to buy for the car I want to build. Most people will just end up buying the whole car and be blissfully unaware of the fact that it comes with a functioning engine.\nIn this analogy, fastai is the engine factory. So we will shape our custom engine into something closer to what gets produced in the factory. One thing to consider is that the pytorch library provides several models as a part of its packaging like nn.Linear. Instead of writing our own version of this, we want to use the one that is already provided. We also need an ‚Äúoptimizer‚Äù, so that we mirror the factory more accurately.\n\n\nCode\nlinear_model = nn.Linear(28*28,10)\n\nclass BasicOptim:\n    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n    def step(self, *args, **kwargs):\n        for p in self.params: p.data -= p.grad.data * self.lr\n\n    def zero_grad(self, *args, **kwargs):\n        for p in self.params: p.grad = None\n\nopt = BasicOptim(linear_model.parameters(), lr)\n\ndef train_epoch(model):\n    for xb,yb in dl_train:\n        #print(xb.dtype, yb.dtype)\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n\n\nTurning the parameters into class variables makes the code stronger. Earlier we had to rely on a global params variable. In our tiny use case this was no problem, but as the codebase grows global variables are a terrible idea.\nThen, we create a new optimizer and redefine our training function. This is the only change we have to make to get the same results as before!\n\n\nCode\ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch(model)\n        print(validate_epoch(model), end=' ')\n\n\nprint(validate_epoch(linear_model), end=' ')\ntrain_model(linear_model, 20)\n\n\n0.087 0.9045 0.9125 0.9155 0.9151 0.9197 0.9178 0.9211 0.9211 0.9195 0.9223 0.9227 0.9226 0.9207 0.924 0.9214 0.9234 0.9247 0.9256 0.9214 0.9247 \n\n\nPretty good. We can do the same thing the actual RIGHT way using fastai classes\n\n\nCode\nlr = .5\nlinear_model = nn.Linear(28*28,10)\nopt = SGD(linear_model.parameters(), lr)\ntrain_model(linear_model, 20)\n\ndls = DataLoaders(dl_train, dl_valid)\n\nlearn = Learner(dls, nn.Linear(28*28,10), opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\nlearn.fit(10, lr=lr)\n\n\n0.9095 0.9143 0.9174 0.9186 0.9194 0.9204 0.9245 0.9215 0.9226 0.9232 0.9242 0.9229 0.9244 0.9246 0.9243 0.9237 0.9234 0.9222 0.9218 0.9248 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.372398\n0.331982\n0.908200\n00:00\n\n\n1\n0.316947\n0.303885\n0.916700\n00:00\n\n\n2\n0.302206\n0.293496\n0.918900\n00:00\n\n\n3\n0.294819\n0.288783\n0.918600\n00:00\n\n\n4\n0.293716\n0.285339\n0.920900\n00:00\n\n\n5\n0.283038\n0.277278\n0.922900\n00:00\n\n\n6\n0.273962\n0.275699\n0.920400\n00:00\n\n\n7\n0.272748\n0.276465\n0.921400\n00:00\n\n\n8\n0.265907\n0.276644\n0.919700\n00:00\n\n\n9\n0.279539\n0.274251\n0.921500\n00:00\n\n\n\n\n\nSo what we just did was rewrite the custom engine using prebuilt parts. We proved that our custom engine was almost as functional as the store-bought version (or COTS if you prefer). I say ‚Äúalmost‚Äù because we seemed to get an accuracy cap of 90% on our custom engine, but the optimized version capped around 92%."
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p2/index.html#deep-learning-models",
    "href": "posts/2025_04_14_MNIST_p2/index.html#deep-learning-models",
    "title": "Classifying MNIST Digits (part 2)",
    "section": "",
    "text": "Remember earlier we talked about the difference between ML and DL as using layers of models. The way we do this is to apply more than one model and separate those models with a non-linearity. So, more fancy words. If we had a function \\(y_1 = m_1 \\times x +b_1\\) and another function \\(y_2 = m_2 \\times x+b_2\\), we couldn‚Äôt just add these two together and expect a different type of function. If we did, we would just get \\(y_{1+2} = m_{1+2} \\times x+b_{1+2}\\). The problem is that the model doesn‚Äôt care about the difference between \\(m_1\\) and \\(m_2\\); it just see weights and a bias.\nWe can separate these models with something like a Recitified Linear Unit (or ReLu). All it does is \\(\\text{ReLu}(x)\\) turns the number into zero if its negative, otherwise it leaves it alone. This little ReLu turns two linear models into a very simple neural net.\nThe genius of this is that instead of fitting a line to data, we can fit several layers of Linear/ReLus to achieve a very precise function. Most data won‚Äôt be fit well by a linear model. However, almost all data can be fit by some arbitrarily large combination of linear models and ReLus.\n\n\n\nlinear\n\n\n\n\nCode\nsimple_net = nn.Sequential(\n    nn.Linear(28*28,30),\n    nn.ReLU(),\n    nn.Linear(30,10)\n)\n\nlearn = Learner(dls, simple_net, opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\nlr = .25\nlearn.fit(20, lr=lr)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.362304\n0.329384\n0.905600\n00:00\n\n\n1\n0.278429\n0.249775\n0.928900\n00:00\n\n\n2\n0.237960\n0.229455\n0.930400\n00:00\n\n\n3\n0.208032\n0.208346\n0.937100\n00:00\n\n\n4\n0.181758\n0.183263\n0.945700\n00:00\n\n\n5\n0.167817\n0.172169\n0.949300\n00:00\n\n\n6\n0.157353\n0.155710\n0.953900\n00:00\n\n\n7\n0.145984\n0.175108\n0.947600\n00:00\n\n\n8\n0.142243\n0.185708\n0.939700\n00:00\n\n\n9\n0.131697\n0.151334\n0.953200\n00:00\n\n\n10\n0.120169\n0.140592\n0.957600\n00:00\n\n\n11\n0.118723\n0.128918\n0.961100\n00:00\n\n\n12\n0.108958\n0.134065\n0.961300\n00:00\n\n\n13\n0.101567\n0.130075\n0.960400\n00:00\n\n\n14\n0.105227\n0.120318\n0.961100\n00:00\n\n\n15\n0.099600\n0.115329\n0.964500\n00:00\n\n\n16\n0.094892\n0.120461\n0.964300\n00:00\n\n\n17\n0.097695\n0.119797\n0.963400\n00:00\n\n\n18\n0.091835\n0.141129\n0.958500\n00:00\n\n\n19\n0.087854\n0.120385\n0.964300\n00:00\n\n\n\n\n\n\n\n\n2relu\n\n\nWe can keep going! We can use resnet18 which contains many more than 2 layers. These are the layers that we refer to when we say ‚Äúdeep learning‚Äù; its deep because it has so many layers of linear and nonlinear activations!\n\n\nCode\n\ndls = ImageDataLoaders.from_folder(\n    path,\n    train='training',\n    valid='testing',\n    valid_pct=None,        # use full test set\n    seed=42,\n    bs=64,                  # you can adjust batch size\n    item_tfms=Resize(224)   # resnet needs larger images\n)\nlearn = vision_learner(dls, resnet18, pretrained=False,\n                    loss_func=F.cross_entropy, metrics=accuracy)\nlearn.fit_one_cycle(1, 0.1)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.150041\n0.057427\n0.985800\n00:55"
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p2/index.html#conclusion",
    "href": "posts/2025_04_14_MNIST_p2/index.html#conclusion",
    "title": "Classifying MNIST Digits (part 2)",
    "section": "",
    "text": "Did we meet our goal? We wanted to classify digits in the MNIST dataset more accurately than our baseline of ~83%. We beat it immediately with our first ML model in around 2-3 epochs with 90%. We then optimized our code and got 92% accuracy. We used the simplest possible deep learning model, a 2-layer neural net with a non-linear activation, and got 96% accuracy. Then we used a more standard deep learning model, resnet 18, and got 98% accuracy in only one epoch. Granted, this last training took longer than the others.\nOk so we learned a lot.\n\nSaw the difference in performance between baseline, ML, and DL\nLearned how to format data and optimize code.\nGot a grounding in the mathematical principles of ML and DL\n\nThis was a hard project for me and I‚Äôm glad I finished. The hardest part was writing the code from scratch. Hopefully I never have to do that again, but at least I know I can if necessary. It‚Äôs pretty amazing to think about the potential of activating deep learning models on all sorts of intriguing datasets!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Aaron James",
    "section": "",
    "text": "Aaron James is a life long learner with a passion for reducing the risks of widespread AI adoption. When not focusing on this, Aaron enjoys performing music and connecting with like-minded humans."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Aaron James",
    "section": "Education",
    "text": "Education\nCarnegie-Mellon University | Pittsburgh, PA\nB.S. Mechanical Engineering | Dec 2020"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Aaron James",
    "section": "Experience",
    "text": "Experience\nOutlier.ai | SME Math/Physics/Python | Jan 2024 - Present\nBowler Pons Solutions Consultants LLC | Solutions Engineer | June 2021 - Mar 2023"
  }
]