[
  {
    "objectID": "Flowers.html",
    "href": "Flowers.html",
    "title": "How I Trained Resnet 50 on the Oxford Flowers Dataset",
    "section": "",
    "text": "oxford_flowers.jpg\nI’m applying to an AI fellowship, and this was their application project. I needed to use the resnet50 CNN model and fine tune it to be able to classify the flowers from the Oxford Flowers dataset. In this post, I detail how I was able to do this and beat the original accuracy in the paper.\nI’ve been working through fastai’s course Part 1, so I’ll be using their framework for this challenge. I also just finished the sixth lesson, which references Jeremy Howard’s “Road to the Top” series of kaggle notebooks. I’m mostly implementing the methodology used there on this dataset.\nfrom fastai.imports import *\nfrom fastai.vision.all import *\nfrom fastai.data.all import *"
  },
  {
    "objectID": "Flowers.html#effect-of-image-transforms",
    "href": "Flowers.html#effect-of-image-transforms",
    "title": "How I Trained Resnet 50 on the Oxford Flowers Dataset",
    "section": "Effect of Image Transforms",
    "text": "Effect of Image Transforms\nNow I want to see what happens if I change the image pre-processing. We first “squished” the images, now we’ll see how cropping them and padding them affects the results.\n\n# Crop results\nlearn = train('resnet26d', item=Resize(192),\n              batch=aug_transforms(size=128, min_scale=0.75), folder=\"/smaller\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n4.369109\n2.711665\n0.475490\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.494434\n1.471795\n0.369608\n00:02\n\n\n1\n1.330196\n1.422566\n0.329412\n00:02\n\n\n2\n1.054390\n1.319151\n0.317647\n00:02\n\n\n3\n0.833957\n1.227265\n0.292157\n00:02\n\n\n4\n0.668740\n1.183494\n0.290196\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Pad results\nlearn = train('resnet26d', item=Resize((256,192), method=ResizeMethod.Pad, pad_mode=PadMode.Zeros),\n      batch=aug_transforms(size=(171,128), min_scale=0.75), folder = \"/smaller\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n4.652976\n2.947920\n0.556863\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.831202\n1.742645\n0.403922\n00:02\n\n\n1\n1.585920\n1.497415\n0.354902\n00:02\n\n\n2\n1.272542\n1.304852\n0.307843\n00:02\n\n\n3\n0.995634\n1.165755\n0.278431\n00:02\n\n\n4\n0.790987\n1.140745\n0.282353\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\nSo our best performance comes from the padding method. We’ll keep that for our final model."
  },
  {
    "objectID": "Flowers.html#experimenting-with-test-time-augmentation",
    "href": "Flowers.html#experimenting-with-test-time-augmentation",
    "title": "How I Trained Resnet 50 on the Oxford Flowers Dataset",
    "section": "Experimenting with Test Time Augmentation",
    "text": "Experimenting with Test Time Augmentation\n\nvalid = learn.dls.valid\npreds,targs = learn.get_preds(dl=valid)\nerror_rate(preds, targs)\n\n\n\n\n\n\n\n\nTensorBase(0.2824)\n\n\nWe want to show that we can use the above code to get the error rate. We then can use test time augmentation to get the average of the predictions of each of the augmented images and the original image itself. This lets us leverage all of the augmented images into more accurate predictions\n\n#learn.dls.train.show_batch(max_n=6, unique=True)\n\n\ntta_preds,_ = learn.tta(dl=valid)\nerror_rate(tta_preds, targs)\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.2431)"
  },
  {
    "objectID": "Flowers.html#furthering-results",
    "href": "Flowers.html#furthering-results",
    "title": "How I Trained Resnet 50 on the Oxford Flowers Dataset",
    "section": "Furthering Results",
    "text": "Furthering Results\nI am a bit of a perfectionist, so I like to comment on ways that I think this could be imporved further, but are outside of the time limit and scope (in my opinion)\n\nI would play around with the image sizes. I pretty arbitralily decided to resize the images to 500x500 pixels because they were all at least that size. There might be a better image resizing method that I’m not aware of.\nOne of the constraints of this project was to use the resnet50 model. If I were to try and get better results I would probably try to ensemble the predictions of several models together.\nThe fastai version of this dataset did not include the original image segmentations from the research paper. It would be a bit intesive, but my results would probably increase if I incorporated those segmentations into training.\nSpeaking of training, why not just combine the validation set and training set into one training set? Technically we just wanted to get good results on the dataset, we don’t necessarily need the model to be robust. We could just do it this way, which would double our model’s training data and probably give us great results. Definitely a terrible idea if this model ever needed to enter something resembling production though."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Aaron James",
    "section": "",
    "text": "Aaron James is a life long learner with a passion for reducing the risks of widespread AI adoption. When not focusing on this, Aaron enjoys performing music and connecting with like-minded humans."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Aaron James",
    "section": "Education",
    "text": "Education\nCarnegie-Mellon University | Pittsburgh, PA\nB.S. Mechanical Engineering | Dec 2020"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Aaron James",
    "section": "Experience",
    "text": "Experience\nOutlier.ai | SME Math/Physics/Python | Jan 2024 - Present\nBowler Pons Solutions Consultants LLC | Solutions Engineer | June 2021 - Mar 2023"
  },
  {
    "objectID": "posts/oxford_flowers/index.html",
    "href": "posts/oxford_flowers/index.html",
    "title": "Training Resnet50 on Oxford Flowers",
    "section": "",
    "text": "oxford_flowers.jpg\n\n\nI’m applying to an AI fellowship, and this was their first test! I needed to use the resnet50 CNN model and fine tune it to be able to classify the flowers from the Oxford Flowers dataset. In this post, I detail how I was able to do this and beat the accuracy achieved by the authors of the original paper.\nI’ve been working through fastai’s course Part 1, so I’ll be using their framework for this challenge. I also just finished the sixth lesson, which references Jeremy Howard’s “Road to the Top” series of kaggle notebooks. I’m mostly implementing the methodologies used in those notebooks on this dataset.\n\n\nCode\nfrom fastai.imports import *\nfrom fastai.vision.all import *\nfrom fastai.data.all import *"
  },
  {
    "objectID": "posts/oxford_flowers/index.html#effect-of-image-transforms",
    "href": "posts/oxford_flowers/index.html#effect-of-image-transforms",
    "title": "Training Resnet50 on Oxford Flowers",
    "section": "Effect of Image Transforms",
    "text": "Effect of Image Transforms\nNow I want to see what happens if I change the image pre-processing. We first “squished” the images, now we’ll see how cropping them and padding them affects the results.\n\n\nCode\n# Crop results\nlearn = train('resnet26d', item=Resize(192),\n              batch=aug_transforms(size=128, min_scale=0.75), folder=\"/smaller\")\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n4.369109\n2.711665\n0.475490\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.494434\n1.471795\n0.369608\n00:02\n\n\n1\n1.330196\n1.422566\n0.329412\n00:02\n\n\n2\n1.054390\n1.319151\n0.317647\n00:02\n\n\n3\n0.833957\n1.227265\n0.292157\n00:02\n\n\n4\n0.668740\n1.183494\n0.290196\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Pad results\nlearn = train('resnet26d', item=Resize((256,192), method=ResizeMethod.Pad, pad_mode=PadMode.Zeros),\n      batch=aug_transforms(size=(171,128), min_scale=0.75), folder = \"/smaller\")\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n4.652976\n2.947920\n0.556863\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.831202\n1.742645\n0.403922\n00:02\n\n\n1\n1.585920\n1.497415\n0.354902\n00:02\n\n\n2\n1.272542\n1.304852\n0.307843\n00:02\n\n\n3\n0.995634\n1.165755\n0.278431\n00:02\n\n\n4\n0.790987\n1.140745\n0.282353\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\nSo our best performance comes from the padding method. We’ll keep that for our final model."
  },
  {
    "objectID": "posts/oxford_flowers/index.html#experimenting-with-test-time-augmentation",
    "href": "posts/oxford_flowers/index.html#experimenting-with-test-time-augmentation",
    "title": "Training Resnet50 on Oxford Flowers",
    "section": "Experimenting with Test Time Augmentation",
    "text": "Experimenting with Test Time Augmentation\n\n\nCode\nvalid = learn.dls.valid\npreds,targs = learn.get_preds(dl=valid)\nerror_rate(preds, targs)\n\n\n\n\n\n\n\n\n\nTensorBase(0.2824)\n\n\nWe want to show that we can use the above code to get the error rate. We then can use test time augmentation to get the average of the predictions of each of the augmented images and the original image itself. This lets us leverage all of the augmented images into more accurate predictions\n\n\nCode\ntta_preds,_ = learn.tta(dl=valid)\nerror_rate(tta_preds, targs)\n\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nTensorBase(0.2431)"
  },
  {
    "objectID": "posts/oxford_flowers/index.html#furthering-results",
    "href": "posts/oxford_flowers/index.html#furthering-results",
    "title": "Training Resnet50 on Oxford Flowers",
    "section": "Furthering Results",
    "text": "Furthering Results\nI am a bit of a perfectionist, so I like to comment on ways that I think this could be improved further, but are outside of the time limit and scope (in my opinion)\n\nI would play around with the image sizes. I pretty arbitrarily decided to resize the images to 500x500 pixels because they were all at least that size. There might be a better image resizing method that I’m not aware of.\nOne of the constraints of this project was to use the resnet50 model. If I were to try and get better results I would probably try to ensemble the predictions of several models together.\nThe fastai version of this dataset did not include the original image segmentations from the research paper. It would be a bit intensive, but my results would probably increase if I incorporated those segmentations into training.\nSpeaking of training, why not just combine the validation set and training set into one training set? Technically we just wanted to get good results on the dataset, we don’t necessarily need the model to be robust. We could just do it this way, which would double our model’s training data and probably give us great results. Definitely a terrible idea if this model ever needed to enter something resembling production though."
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p2/index.html",
    "href": "posts/2025_04_14_MNIST_p2/index.html",
    "title": "Classifying MNIST Digits (part 2)",
    "section": "",
    "text": "Learning_Robot.png\n\n\nIn part 1, we created a baseline for MNIST classification without using any ML/DL tools. The best we got was an accuracy of 81.73%; that’s our target to beat with some new models!\nNow we can move onto actually doing ML and DL. We’ll get into the difference shortly. First we just want to a quick code reproduction of part 1 so that we can use the output from the code and load it into memory.\nHere’s a demo of my completed model.\n\n\nCode\n#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\nfrom fastai.vision.all import *\nfrom fastbook import *\n\npath = untar_data(URLs.MNIST)\n\n# Go crazy with list comprehensions and make all the datasets we need\ndef create_dataset(top_dir):\n    # takes a top level directory and makes the aforementioned list of 3d tensors for each digit\n\n    # grab each of the paths for each digit directory in this folder\n    mnist_datatype_digit_paths = [p for p in top_dir.ls().sorted()] \n    \n    sample_images = []\n\n    for p in mnist_datatype_digit_paths:\n        # grab the path to each image included for the current digit\n        training_digit_im_paths = [im_path for im_path in p.ls().sorted()] \n        # open the image as tensors for each included image\n        training_image_tensors = [tensor(Image.open(im)) for im in training_digit_im_paths] \n        # turn the list of several pixel intensity matrices into a single 3D tensor\n        stacked_digit_tensors = torch.stack(training_image_tensors).float() / 255 \n        sample_images.append(stacked_digit_tensors)\n    return sample_images\n        \nmnist_dataset = {}\nmnist_dataset[\"training\"] = create_dataset((path/\"training\"))\nmnist_dataset[\"testing\"] = create_dataset((path/\"testing\"))\n\n\n\n\nWe are now going to solve this problem using machine learning. “Machine Learning” is a broad field that is all about a computer “learning” patterns in data, and then using those insights to make predictions or decisions. Typically there is a mathematical model that enables us to learn and apply what we’ve learned. “Deep Learning” is a subset of ML. In DL we use layered models to do the same thing on more complicated data.\nFirst we’re going to focus on machine learning. Then we’ll change our model to incorporate deep learning techniques.\n\n\n\nHow can we actually say that a computer “learns” something? They obviously don’t have brains. This notebook created by Jeremy Howard gives a pretty great explanation. But I’ll give my own attempted summary.\nOur goal is to find a way to predict data we don’t have using data we do have. Typically, the data is something like a spreadsheet. Often there is some sort of input and output. We want to determine a way to use math to relate the inputs and the outputs of the data that we do have. If we can get that right on the data we do have, we can be relatively confident we can use it for future predictions.\n\n\nIf we were relating salary vs monthly spending, we’d probably expect a fairly linear relationship. We might expect something like:\n\\(\\text{monthlySpending} = m\\times\\text{salary} + b\\)\nIf we had that data for several hundred households, we could probably estimate the values of \\(m\\) and \\(b\\). If we could do this, then we would say that our computer “learned” something. We could then apply those “learnings”. If we had the income of other people (but not their spending) we could use this model to predict their spending.\nSo that’s how we can say that a machine has “learned” something; it has estimated the parameters of a mathematical function that we think approximates the trend of the dataset accurately.\n\n\n\n\nMy goal here is to create something where I give it an image and it tells me the likelihood that the image is each of the 10 digits. I don’t just want it to work for this dataset, I want it to work for any image.\nActually applying these concepts in code was pretty hard for me. I didn’t just use the easy high-level fastai provided code (libraries), I had to get into the details and create most of the training loop myself. The first thing I had to do was format the data for this process. To get this to work properly, each image needs to be associated with a label. I also had to reformat the tensors so the computer could process them better.\n\n\nCode\n# Set up training and validation datasets\ndef test_train_labels_aligned(md, dtype, train_x, train_y):\n    training_digits = 0\n    for d in range(len(md[dtype]) - 1):\n        training_digits += len(md[dtype][d])\n        assert(train_y[training_digits - 1] == d)\n        assert(train_y[training_digits] == d + 1)\n        \ntrain_x = torch.cat(mnist_dataset[\"training\"]).view(-1,28*28)\ntrain_y = tensor([d for d, items in enumerate(mnist_dataset[\"training\"]) for _ in items]).unsqueeze(1)\n\n# test that the labels lign up correctly\ntest_train_labels_aligned(mnist_dataset, \"training\", train_x, train_y)\n\nvalid_x = torch.cat(mnist_dataset[\"testing\"]).view(-1,28*28)\nvalid_y = tensor([d for d, items in enumerate(mnist_dataset[\"testing\"]) for _ in items]).unsqueeze(1)\n\ntest_train_labels_aligned(mnist_dataset, \"testing\", valid_x, valid_y)\n\n# Zip x and y together for each datatype\ndset_training = list(zip(train_x, train_y))\ndset_validation = list(zip(valid_x, valid_y))\n\n\nThis was pretty straightforward, but then I really struggled with how to do ML for 10 digits:\n“Ugh this is hard. I pretty much just thought for like a few hours straight and overnight. Turning this problem into a spreadsheet was my first step and I should include it here…”\n\n\n\n“Sketch Spreadsheet”\n\n\nThought Process Continued:\n\n“I think its like, you basically want a model with 10 sets of weights and biases that each do binary prediction that a photo is a digit. So you probably have to train 10 models? How to turn this into a training loop though. But what if the (hold on, eureka incoming) what if the parameters w and b are mega tensors? So 3D tensors that have an axis that contains all the digit data predictions together? Because the thing that’s hard is the output should NOT be a number between 0 and 9. That would imply that a 2.5 would be an image that is most similar to 2 and 5, but 2 and 5 are not similar looking digits. So we have to do multi-factor classification. I’m sure there is some official way to do this but I want to try doing it myself first.”\n\n\n\n\nIt was so hard I took a step back. I needed to refresh my memory on how to do all this. I ended up rewriting a binary classification training loop for the digit 0. This model should be able to determine if any image is a zero or not a zero.\n\n\nMore fancy words to make simple things sound complicated. We talked earlier about the fact that “learning” means estimating the parameters of a model (math equation, function) that fits our dataset. How do we use a computer to estimate the parameters of a function? We use Stochastic Gradient Descent (SGD). The linked notebook goes into really good depth on what exactly this is. Before we start applying this step, we really need to decide on a model. We use a linear model in this notebook for our machine learning example because its one of the simplest models there is.\n\n\n\nDecide on a model: We have chosen the linear model \\(y = mx + b\\)\nInitialize parameters: We come up with some random guesses for our parameters \\(m\\) and \\(b\\).\nPredict: We use our data and our initial parameters to make a prediction. The first time we do this we will always have a very poor prediction.\nCalculate the loss: We haven’t really talked about loss yet. We brushed on it in the last post. Basically, we need a numerical way of determining how wrong we are. We compared L1 norm and L2 norm (RMSE) in the last post, and it turned out that RMSE worked better for our dataset and current use case. Keep in mind that this is now another equation. Our model is about predicting the data, and our loss is about estimating the accuracy of our data. Two different equations.\nCalculate the Gradient. And here’s the hardest math part. Again here’s Jeremy’s SGD Post. Very basically, we use calculus to determine how much each parameter needs to change so that when we make our next prediction, our loss is smaller. We can use these numerical methods so we don’t have to make a random guess for our next estimate, it will be a very precise estimate based on the principles of multi-variate calculus.\nStep the parameters: We now adjust the parameters based on the gradient. We nudge them in the direction of a smaller loss that our previous gradients predicted.\nRepeat the process with Step 2. The two magic parts of this are the gradient, and the various iterations. If we can keep nudging out parameters in the right direction (with the gradient), and do it iteratively (step 6) eventually our loss will get closer and closer to zero. This is good. If loss is the difference between the prediction and the right answer, it will be very helpful if that distance is minimized\nStop. We can’t just iterate forever. We will likely have some imposed criteria that we will hit that will tell us we can stop. For example, we could iterate until we beat our baseline that we got in the last post. However we define it, we will have to stop at some point to be able to apply the learnings of our model\n\nThen I go into the code for making this all work. This first chunk for the binary 0 classifier is bad code. I’m including it because its “functional” and because it documents my thought process. I wrote it mostly from memory using the steps of SGD.\n\n\nCode\ndef create_0_binary_class_dset():\n    # We modify the dset creation above\n    train_x_0 = torch.cat(mnist_dataset[\"training\"]).view(-1,28*28)\n    training_0s = len(mnist_dataset[\"training\"][0])\n    train_y_0 = tensor([1]*training_0s + [0]*(len(train_y)-training_0s)).unsqueeze(1)\n\n    valid_x_0 = torch.cat(mnist_dataset[\"testing\"]).view(-1,28*28)\n    testing_0s = len(mnist_dataset[\"testing\"][0])\n    valid_y_0 = tensor([1]*testing_0s + [0]*(len(valid_y)-testing_0s)).unsqueeze(1)\n\n    dset_train_0 = list(zip(train_x_0, train_y_0))\n    dset_valid_0 = list(zip(valid_x_0, valid_y_0))\n    \n    return dset_train_0, dset_valid_0\n\n\n\n\nCode\ndef init_params(shape, std = 1.0): return (torch.randn(shape)*std).requires_grad_()\n\ndef model_linear(xb, weights, bias): return xb@weights + bias\n\ndef loss_0_bin(preds, y_labels):\n    preds = preds.sigmoid()\n    return torch.where(y_labels == 1, 1 - preds, preds).mean()\n\ndef batch_acc(xb_preds, yb):\n    preds = xb_preds.sigmoid()\n    correct_preds = (preds &gt; 0.5) == yb\n    return correct_preds.float().mean()\n    \ndef main():\n    # Turn the dataset into randomized dataloaders\n    dset_train, dset_valid = create_0_binary_class_dset()\n    dl_train = DataLoader(dset_train, batch_size=256, shuffle=True)\n    dl_valid = DataLoader(dset_valid, batch_size=256, shuffle=True)\n\n    #1. Initialize parameters\n    weights = init_params((28*28,1))\n    bias = init_params(1)\n    lr = .75\n\n    epochs = 10\n    for _ in range(epochs):\n        for xb, yb in dl_train:\n            # 2. Make a prediction\n            # preds will be a vector with an output for each image in the batch. so 256x1\n            #print(xb.shape)\n            preds = model_linear(xb, weights, bias)\n            # 3. Calculate Loss\n            loss = loss_0_bin(preds, yb)\n            # 4. Get gradients\n            loss.backward()\n            weights.data -= weights.grad*lr\n            weights.grad.zero_()\n            bias.data -= bias.grad*lr\n            bias.grad.zero_()\n        # Now, because the params w/b have been modified slightly by each batch, we can determine how good they are now.\n        accs = [batch_acc(model_linear(xb, weights, bias), yb) for xb, yb in dl_valid]\n        print(\"accuracy\",torch.stack(accs).mean())\n\nmain()\n\n\n\naccuracy tensor(0.9007)\naccuracy tensor(0.8996)\naccuracy tensor(0.9012)\naccuracy tensor(0.9041)\naccuracy tensor(0.9012)\naccuracy tensor(0.8998)\naccuracy tensor(0.9013)\naccuracy tensor(0.9013)\naccuracy tensor(0.9028)\naccuracy tensor(0.9043)\n\n\nSomething I also had to add was determining the accuracy of each epoch (or training iteration). We want to be able to have some sense of if what we’re doing is working. I train the model on the training data, then I test of accuracy with the validation data which is what we did in the last post.\nInterestingly enough, after only one epoch we get accuracy of ~90%. Keep in mind this is still just binary classification, but it goes to show how quickly we can get acccurately tuned parameters.\n\n\n\n\n\nOk, I’m glad I took that detour. Now I can just slightly modify what I did to apply it to a multi class. I think it makes sense to go through the changes first before I show off what I built.\n\n\nThe first thing I did was make my code a bit more readable by breaking it into functions. Actually, I pretty much just used the ones from the fastai lesson.\nThere were really only 3 main pieces that had to change:\n\nInitializing the parameters: For binary classification, each image only needs one output which is essentially true/false to the question, “is this image a zero?”. For multi-class, each image needs 10 different scores. We do this by changing our weights and biases initialization. They change from weights, bias = init_params((28*28,1)), init_params(1) to weights, bias = init_params((28*28,10)), init_params(10). A note: all I had to do was add TWO ZEROES but this took me forever to figure out…\nLoss Function: This was really tricky too. I’ll get into it shortly.\nAccuracy Function: We just had to see if the prediction lined up with the actual correct answer. I did it by finding the index of the 1x10 vector that was highest. Since I set my data up this way, we know that that index corresponds exactly to the labels we had in our “correct answers” data (y_label or targets). If I was classifying dog breeds, I’d have to come up with a different way of doing this, likely using a map that maps indices to the corresponding breed. It’s very convenient that index corresponds perfectly with the label.\n\n\n\n\nOk so I realized that we needed another one. My plan was to determine which digit that the parameters predicted the image was, compare it to the correct target labels, and then get the average number of misses per batch as my loss function. Two issues with this. First was implementation. If I use argmax on the prediction variable, I lose gradient tracking of my parameters w and b. So our whole stochastic gradient descent method falls apart. Second was conceptual. If this did work somehow, I’m not being optimal. My model would treat these two classifications of a 6 the same:\n\nHigh Confidence 6: [0.01, 0.01, 0.02, 0.02, 0.01, 0.01, 0.90, 0.01, 0.01, 0.01]\nLow Confidence 6: [0.09, 0.10, 0.08, 0.10, 0.10, 0.08, 0.13, 0.09, 0.11, 0.12]\n\nBoth of these prediction outputs predict a 6 because the seventh value is highest. However, the first output predicts it more optimally because it is significantly more confident that it is a 6 than any other digit.\nOur loss function is really important. It’s our way of telling the model EXACTLY what we want it to do using math. We don’t just want our model to get the right classification, we want it to get the right classification and be extremely confident about it.\nI just wanted to understand the math behind this better. Apparently the right way to do this problem is to use log_softmax(), where softmax is:\n\\(\\text{softmax}(r) = \\frac{e^{r_i}}{\\sum_{j} r_j}\\)\n\n\nCode\n# I just need to convince myself I understand the math of this new loss function...\nr = tensor([2.0, 1.0, 0.1])\n\nexp_r= torch.exp(r)\nnorm_exp_r = exp_r/exp_r.sum()\nassert(torch.allclose(norm_exp_r, F.softmax(r, dim=0), atol=1e-4))\nlog_norm_exp_r = torch.log(norm_exp_r)\nassert(torch.allclose(log_norm_exp_r, r.log_softmax(dim=0), atol=1e-4))\n\n\nSoftmax makes it so that the sum of the predictions vector is 1, which is essentially turning it into a vector of percentages that the current image is each of the 10 digits.\nHere’s how our loss function modifies the input to make it better and better:\n\nRaw Predictions: The index that contains the highest value is our predicted digit, no correlation to other values.\nSoftmax: Probabilities that image could be the digit at each index. Sum is 1, so these are relative to the other values in the vector. For example, if the first number is .75 and the second is .25, then the others have to all be 0, even if they were non zero at the last stage. Relativizing the prediction allows us to get more information from a single number.\nLog Softmax: This emphasizes penalty when the prediction is confidently incorrect, and still gives a small penalty when it gets the correct prediction not confidently. It’s the distribution of the -log function. It’s really similar to the way that we used RMSE to mega-penalize certain mistakes in part 1. It’s kind of like partial credit in math class. Oh, you were supposed to say this image was a 7? Were you:\n\nRight and confident? p=0.9: You don’t lose any points\nRight, but not sure? p=0.5: You lose a few point\nConfidently incorrect? p=0.1: You lose lots of points\n\n\n\n\nCode\n# Why do we take log of soft_max? Check index 6 (or the seventh digit) of the output\nhigh_conf = tensor([0.01, 0.01, 0.02, 0.02, 0.01, 0.01, 0.90, 0.01, 0.01, 0.01])\nlow_conf = tensor([0.09, 0.10, 0.08, 0.10, 0.10, 0.08, 0.13, 0.09, 0.11, 0.12])\n\nF.softmax(high_conf, dim=0), F.softmax(low_conf, dim=0), -high_conf.log_softmax(dim=0), -low_conf.log_softmax(dim=0),   \n\n\n(tensor([0.0873, 0.0873, 0.0882, 0.0882, 0.0873, 0.0873, 0.2126, 0.0873, 0.0873, 0.0873]),\n tensor([0.0990, 0.1000, 0.0980, 0.1000, 0.1000, 0.0980, 0.1030, 0.0990, 0.1010, 0.1020]),\n tensor([2.4384, 2.4384, 2.4284, 2.4284, 2.4384, 2.4384, 1.5484, 2.4384, 2.4384, 2.4384]),\n tensor([2.3127, 2.3027, 2.3227, 2.3027, 2.3027, 2.3227, 2.2727, 2.3127, 2.2927, 2.2827]))\n\n\nThe seventh prediction (corresponding to digit 6 since the first prediction is for 0) is what to focus on. With log_softmax() there is a much greater difference for the low confidence and high confidence predictions. This means that we have packed lots of information into a single digit, which is great for our model and highly optimal.\n\n\n\n\n\n\nCode\n# Basically we want the shape of logits that we had in the golden mean baseline, so our parameters have to mirror that shape\n# Declare global params variable. I know there's a better OOP way to do this, but it's chill...\n\n# Turn the dataset into randomized dataloaders\ndl_train = DataLoader(dset_training, batch_size=256, shuffle=True)\ndl_valid = DataLoader(dset_validation, batch_size=256, shuffle=True)\n\n#1. Initialize parameters\nlr = .25\nepochs = 20\nweights = init_params((28*28,10))\nbias = init_params(10)\nparams = weights,bias\n\ndef main_multi_class():\n    print(validate_epoch(linear_1))\n    for _ in range(epochs):\n        train_epoch(linear_1, lr, dl_train)\n        print(validate_epoch(linear_1))\n\ndef init_params(shape, std = 1.0): return (torch.randn(shape)*std).requires_grad_()\n\ndef linear_1(xb): \n    w, b = params\n    #print(\"model\", w, b)\n    return xb@w + b\n\ndef mnist_loss(preds, yb):\n    #print(preds.shape, yb.shape)\n    yb = yb.squeeze()\n    log_sm_probs = preds.log_softmax(dim=1)\n    return -log_sm_probs[range(len(yb)), yb].mean()\n    \ndef train_epoch(model, lr, dl):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\n\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    #print(loss.shape)\n    loss.backward()\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in dl_valid]\n    return round(torch.stack(accs).mean().item(), 4)\n\ndef batch_accuracy(preds, yb):\n    max_indices = preds.argmax(dim=1).unsqueeze(1)\n    #print(\"args\", max_indices)\n    correct = (max_indices == yb)\n    acc = correct.float().mean()\n    return acc\n\nmain_multi_class()\n\n\n0.0965\n0.7646\n0.8165\n0.8429\n0.8542\n0.8629\n0.8701\n0.8687\n0.8738\n0.8785\n0.8822\n0.8799\n0.8878\n0.8855\n0.8871\n0.8911\n0.8893\n0.8934\n0.8907\n0.8931\n0.8936\n\n\n\n\n\nSo, amazingly we have just “succeeded”! We have built a machine learning model that is able to classify digits in the MNIST dataset, and that outperforms our baseline classification of 81.73%! There’s a few more things to consider but this is definitely a moment to celebrate and enjoy. We’ve used what we have been learning to outperform our benchmark. Good job.\nWe will now slightly optimize the code, and then we’ll look at some deep learning models. Remember, the linear model is just an example of machine learning, and we’re interested in comparing the performance of both.\n\n\n\nOk, so what we did was the custom version of this. We now want to take steps to create the fastai/pyTorch extraction.\nEverything we have just completed actually works. However, we have just been looking under the hood. These details matter for my comprehension of the math and coding, but for actual training I would never create a custom training loop. This training loop is the main engine that drives most ML systems and we just built it from scratch. Think about a car engine. There’s no way I will ever need to build a car engine from scratch. If I’m building my own car, I might buy an engine, and knowledge of how it works will help me choose which to buy for the car I want to build. Most people will just end up buying the whole car and be blissfully unaware of the fact that it comes with a functioning engine.\nIn this analogy, fastai is the engine factory. So we will shape our custom engine into something closer to what gets produced in the factory. One thing to consider is that the pytorch library provides several models as a part of its packaging like nn.Linear. Instead of writing our own version of this, we want to use the one that is already provided. We also need an “optimizer”, so that we mirror the factory more accurately.\n\n\nCode\nlinear_model = nn.Linear(28*28,10)\n\nclass BasicOptim:\n    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n    def step(self, *args, **kwargs):\n        for p in self.params: p.data -= p.grad.data * self.lr\n\n    def zero_grad(self, *args, **kwargs):\n        for p in self.params: p.grad = None\n\nopt = BasicOptim(linear_model.parameters(), lr)\n\ndef train_epoch(model):\n    for xb,yb in dl_train:\n        #print(xb.dtype, yb.dtype)\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n\n\nTurning the parameters into class variables makes the code stronger. Earlier we had to rely on a global params variable. In our tiny use case this was no problem, but as the codebase grows global variables are a terrible idea.\nThen, we create a new optimizer and redefine our training function. This is the only change we have to make to get the same results as before!\n\n\nCode\ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch(model)\n        print(validate_epoch(model), end=' ')\n\n\nprint(validate_epoch(linear_model), end=' ')\ntrain_model(linear_model, 20)\n\n\n0.087 0.9045 0.9125 0.9155 0.9151 0.9197 0.9178 0.9211 0.9211 0.9195 0.9223 0.9227 0.9226 0.9207 0.924 0.9214 0.9234 0.9247 0.9256 0.9214 0.9247 \n\n\nPretty good. We can do the same thing the actual RIGHT way using fastai classes\n\n\nCode\nlr = .5\nlinear_model = nn.Linear(28*28,10)\nopt = SGD(linear_model.parameters(), lr)\ntrain_model(linear_model, 20)\n\ndls = DataLoaders(dl_train, dl_valid)\n\nlearn = Learner(dls, nn.Linear(28*28,10), opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\nlearn.fit(10, lr=lr)\n\n\n0.9095 0.9143 0.9174 0.9186 0.9194 0.9204 0.9245 0.9215 0.9226 0.9232 0.9242 0.9229 0.9244 0.9246 0.9243 0.9237 0.9234 0.9222 0.9218 0.9248 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.372398\n0.331982\n0.908200\n00:00\n\n\n1\n0.316947\n0.303885\n0.916700\n00:00\n\n\n2\n0.302206\n0.293496\n0.918900\n00:00\n\n\n3\n0.294819\n0.288783\n0.918600\n00:00\n\n\n4\n0.293716\n0.285339\n0.920900\n00:00\n\n\n5\n0.283038\n0.277278\n0.922900\n00:00\n\n\n6\n0.273962\n0.275699\n0.920400\n00:00\n\n\n7\n0.272748\n0.276465\n0.921400\n00:00\n\n\n8\n0.265907\n0.276644\n0.919700\n00:00\n\n\n9\n0.279539\n0.274251\n0.921500\n00:00\n\n\n\n\n\nSo what we just did was rewrite the custom engine using prebuilt parts. We proved that our custom engine was almost as functional as the store-bought version (or COTS if you prefer). I say “almost” because we seemed to get an accuracy cap of 90% on our custom engine, but the optimized version capped around 92%.\n\n\n\nRemember earlier we talked about the difference between ML and DL as using layers of models. The way we do this is to apply more than one model and separate those models with a non-linearity. So, more fancy words. If we had a function \\(y_1 = m_1 \\times x +b_1\\) and another function \\(y_2 = m_2 \\times x+b_2\\), we couldn’t just add these two together and expect a different type of function. If we did, we would just get \\(y_{1+2} = m_{1+2} \\times x+b_{1+2}\\). The problem is that the model doesn’t care about the difference between \\(m_1\\) and \\(m_2\\); it just see weights and a bias.\nWe can separate these models with something like a Recitified Linear Unit (or ReLu). All it does is \\(\\text{ReLu}(x)\\) turns the number into zero if its negative, otherwise it leaves it alone. This little ReLu turns two linear models into a very simple neural net.\nThe genius of this is that instead of fitting a line to data, we can fit several layers of Linear/ReLus to achieve a very precise function. Most data won’t be fit well by a linear model. However, almost all data can be fit by some arbitrarily large combination of linear models and ReLus.\n\n\n\nlinear\n\n\n\n\nCode\nsimple_net = nn.Sequential(\n    nn.Linear(28*28,30),\n    nn.ReLU(),\n    nn.Linear(30,10)\n)\n\nlearn = Learner(dls, simple_net, opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\nlr = .25\nlearn.fit(20, lr=lr)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.362304\n0.329384\n0.905600\n00:00\n\n\n1\n0.278429\n0.249775\n0.928900\n00:00\n\n\n2\n0.237960\n0.229455\n0.930400\n00:00\n\n\n3\n0.208032\n0.208346\n0.937100\n00:00\n\n\n4\n0.181758\n0.183263\n0.945700\n00:00\n\n\n5\n0.167817\n0.172169\n0.949300\n00:00\n\n\n6\n0.157353\n0.155710\n0.953900\n00:00\n\n\n7\n0.145984\n0.175108\n0.947600\n00:00\n\n\n8\n0.142243\n0.185708\n0.939700\n00:00\n\n\n9\n0.131697\n0.151334\n0.953200\n00:00\n\n\n10\n0.120169\n0.140592\n0.957600\n00:00\n\n\n11\n0.118723\n0.128918\n0.961100\n00:00\n\n\n12\n0.108958\n0.134065\n0.961300\n00:00\n\n\n13\n0.101567\n0.130075\n0.960400\n00:00\n\n\n14\n0.105227\n0.120318\n0.961100\n00:00\n\n\n15\n0.099600\n0.115329\n0.964500\n00:00\n\n\n16\n0.094892\n0.120461\n0.964300\n00:00\n\n\n17\n0.097695\n0.119797\n0.963400\n00:00\n\n\n18\n0.091835\n0.141129\n0.958500\n00:00\n\n\n19\n0.087854\n0.120385\n0.964300\n00:00\n\n\n\n\n\n\n\n\n2relu\n\n\nWe can keep going! We can use resnet18 which contains many more than 2 layers. These are the layers that we refer to when we say “deep learning”; its deep because it has so many layers of linear and nonlinear activations!\n\n\nCode\n\ndls = ImageDataLoaders.from_folder(\n    path,\n    train='training',\n    valid='testing',\n    valid_pct=None,        # use full test set\n    seed=42,\n    bs=64,                  # you can adjust batch size\n    item_tfms=Resize(224)   # resnet needs larger images\n)\nlearn = vision_learner(dls, resnet18, pretrained=False,\n                    loss_func=F.cross_entropy, metrics=accuracy)\nlearn.fit_one_cycle(1, 0.1)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.150041\n0.057427\n0.985800\n00:55\n\n\n\n\n\n\n\n\nDid we meet our goal? We wanted to classify digits in the MNIST dataset more accurately than our baseline of ~83%. We beat it immediately with our first ML model in around 2-3 epochs with 90%. We then optimized our code and got 92% accuracy. We used the simplest possible deep learning model, a 2-layer neural net with a non-linear activation, and got 96% accuracy. Then we used a more standard deep learning model, resnet 18, and got 98% accuracy in only one epoch. Granted, this last training took longer than the others.\nOk so we learned a lot.\n\nSaw the difference in performance between baseline, ML, and DL\nLearned how to format data and optimize code.\nGot a grounding in the mathematical principles of ML and DL\n\nThis was a hard project for me and I’m glad I finished. The hardest part was writing the code from scratch. Hopefully I never have to do that again, but at least I know I can if necessary. It’s pretty amazing to think about the potential of activating deep learning models on all sorts of intriguing datasets!"
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p2/index.html#machine-learning-vs-deep-learning",
    "href": "posts/2025_04_14_MNIST_p2/index.html#machine-learning-vs-deep-learning",
    "title": "Classifying MNIST Digits (part 2)",
    "section": "",
    "text": "We are now going to solve this problem using machine learning. “Machine Learning” is a broad field that is all about a computer “learning” patterns in data, and then using those insights to make predictions or decisions. Typically there is a mathematical model that enables us to learn and apply what we’ve learned. “Deep Learning” is a subset of ML. In DL we use layered models to do the same thing on more complicated data.\nFirst we’re going to focus on machine learning. Then we’ll change our model to incorporate deep learning techniques."
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p2/index.html#explaining-learning",
    "href": "posts/2025_04_14_MNIST_p2/index.html#explaining-learning",
    "title": "Classifying MNIST Digits (part 2)",
    "section": "",
    "text": "How can we actually say that a computer “learns” something? They obviously don’t have brains. This notebook created by Jeremy Howard gives a pretty great explanation. But I’ll give my own attempted summary.\nOur goal is to find a way to predict data we don’t have using data we do have. Typically, the data is something like a spreadsheet. Often there is some sort of input and output. We want to determine a way to use math to relate the inputs and the outputs of the data that we do have. If we can get that right on the data we do have, we can be relatively confident we can use it for future predictions.\n\n\nIf we were relating salary vs monthly spending, we’d probably expect a fairly linear relationship. We might expect something like:\n\\(\\text{monthlySpending} = m\\times\\text{salary} + b\\)\nIf we had that data for several hundred households, we could probably estimate the values of \\(m\\) and \\(b\\). If we could do this, then we would say that our computer “learned” something. We could then apply those “learnings”. If we had the income of other people (but not their spending) we could use this model to predict their spending.\nSo that’s how we can say that a machine has “learned” something; it has estimated the parameters of a mathematical function that we think approximates the trend of the dataset accurately."
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p2/index.html#applying-ml-to-mnist-dataset",
    "href": "posts/2025_04_14_MNIST_p2/index.html#applying-ml-to-mnist-dataset",
    "title": "Classifying MNIST Digits (part 2)",
    "section": "",
    "text": "My goal here is to create something where I give it an image and it tells me the likelihood that the image is each of the 10 digits. I don’t just want it to work for this dataset, I want it to work for any image.\nActually applying these concepts in code was pretty hard for me. I didn’t just use the easy high-level fastai provided code (libraries), I had to get into the details and create most of the training loop myself. The first thing I had to do was format the data for this process. To get this to work properly, each image needs to be associated with a label. I also had to reformat the tensors so the computer could process them better.\n\n\nCode\n# Set up training and validation datasets\ndef test_train_labels_aligned(md, dtype, train_x, train_y):\n    training_digits = 0\n    for d in range(len(md[dtype]) - 1):\n        training_digits += len(md[dtype][d])\n        assert(train_y[training_digits - 1] == d)\n        assert(train_y[training_digits] == d + 1)\n        \ntrain_x = torch.cat(mnist_dataset[\"training\"]).view(-1,28*28)\ntrain_y = tensor([d for d, items in enumerate(mnist_dataset[\"training\"]) for _ in items]).unsqueeze(1)\n\n# test that the labels lign up correctly\ntest_train_labels_aligned(mnist_dataset, \"training\", train_x, train_y)\n\nvalid_x = torch.cat(mnist_dataset[\"testing\"]).view(-1,28*28)\nvalid_y = tensor([d for d, items in enumerate(mnist_dataset[\"testing\"]) for _ in items]).unsqueeze(1)\n\ntest_train_labels_aligned(mnist_dataset, \"testing\", valid_x, valid_y)\n\n# Zip x and y together for each datatype\ndset_training = list(zip(train_x, train_y))\ndset_validation = list(zip(valid_x, valid_y))\n\n\nThis was pretty straightforward, but then I really struggled with how to do ML for 10 digits:\n“Ugh this is hard. I pretty much just thought for like a few hours straight and overnight. Turning this problem into a spreadsheet was my first step and I should include it here…”\n\n\n\n“Sketch Spreadsheet”\n\n\nThought Process Continued:\n\n“I think its like, you basically want a model with 10 sets of weights and biases that each do binary prediction that a photo is a digit. So you probably have to train 10 models? How to turn this into a training loop though. But what if the (hold on, eureka incoming) what if the parameters w and b are mega tensors? So 3D tensors that have an axis that contains all the digit data predictions together? Because the thing that’s hard is the output should NOT be a number between 0 and 9. That would imply that a 2.5 would be an image that is most similar to 2 and 5, but 2 and 5 are not similar looking digits. So we have to do multi-factor classification. I’m sure there is some official way to do this but I want to try doing it myself first.”"
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p2/index.html#a-detour-into-binary-classification",
    "href": "posts/2025_04_14_MNIST_p2/index.html#a-detour-into-binary-classification",
    "title": "Classifying MNIST Digits (part 2)",
    "section": "",
    "text": "It was so hard I took a step back. I needed to refresh my memory on how to do all this. I ended up rewriting a binary classification training loop for the digit 0. This model should be able to determine if any image is a zero or not a zero.\n\n\nMore fancy words to make simple things sound complicated. We talked earlier about the fact that “learning” means estimating the parameters of a model (math equation, function) that fits our dataset. How do we use a computer to estimate the parameters of a function? We use Stochastic Gradient Descent (SGD). The linked notebook goes into really good depth on what exactly this is. Before we start applying this step, we really need to decide on a model. We use a linear model in this notebook for our machine learning example because its one of the simplest models there is.\n\n\n\nDecide on a model: We have chosen the linear model \\(y = mx + b\\)\nInitialize parameters: We come up with some random guesses for our parameters \\(m\\) and \\(b\\).\nPredict: We use our data and our initial parameters to make a prediction. The first time we do this we will always have a very poor prediction.\nCalculate the loss: We haven’t really talked about loss yet. We brushed on it in the last post. Basically, we need a numerical way of determining how wrong we are. We compared L1 norm and L2 norm (RMSE) in the last post, and it turned out that RMSE worked better for our dataset and current use case. Keep in mind that this is now another equation. Our model is about predicting the data, and our loss is about estimating the accuracy of our data. Two different equations.\nCalculate the Gradient. And here’s the hardest math part. Again here’s Jeremy’s SGD Post. Very basically, we use calculus to determine how much each parameter needs to change so that when we make our next prediction, our loss is smaller. We can use these numerical methods so we don’t have to make a random guess for our next estimate, it will be a very precise estimate based on the principles of multi-variate calculus.\nStep the parameters: We now adjust the parameters based on the gradient. We nudge them in the direction of a smaller loss that our previous gradients predicted.\nRepeat the process with Step 2. The two magic parts of this are the gradient, and the various iterations. If we can keep nudging out parameters in the right direction (with the gradient), and do it iteratively (step 6) eventually our loss will get closer and closer to zero. This is good. If loss is the difference between the prediction and the right answer, it will be very helpful if that distance is minimized\nStop. We can’t just iterate forever. We will likely have some imposed criteria that we will hit that will tell us we can stop. For example, we could iterate until we beat our baseline that we got in the last post. However we define it, we will have to stop at some point to be able to apply the learnings of our model\n\nThen I go into the code for making this all work. This first chunk for the binary 0 classifier is bad code. I’m including it because its “functional” and because it documents my thought process. I wrote it mostly from memory using the steps of SGD.\n\n\nCode\ndef create_0_binary_class_dset():\n    # We modify the dset creation above\n    train_x_0 = torch.cat(mnist_dataset[\"training\"]).view(-1,28*28)\n    training_0s = len(mnist_dataset[\"training\"][0])\n    train_y_0 = tensor([1]*training_0s + [0]*(len(train_y)-training_0s)).unsqueeze(1)\n\n    valid_x_0 = torch.cat(mnist_dataset[\"testing\"]).view(-1,28*28)\n    testing_0s = len(mnist_dataset[\"testing\"][0])\n    valid_y_0 = tensor([1]*testing_0s + [0]*(len(valid_y)-testing_0s)).unsqueeze(1)\n\n    dset_train_0 = list(zip(train_x_0, train_y_0))\n    dset_valid_0 = list(zip(valid_x_0, valid_y_0))\n    \n    return dset_train_0, dset_valid_0\n\n\n\n\nCode\ndef init_params(shape, std = 1.0): return (torch.randn(shape)*std).requires_grad_()\n\ndef model_linear(xb, weights, bias): return xb@weights + bias\n\ndef loss_0_bin(preds, y_labels):\n    preds = preds.sigmoid()\n    return torch.where(y_labels == 1, 1 - preds, preds).mean()\n\ndef batch_acc(xb_preds, yb):\n    preds = xb_preds.sigmoid()\n    correct_preds = (preds &gt; 0.5) == yb\n    return correct_preds.float().mean()\n    \ndef main():\n    # Turn the dataset into randomized dataloaders\n    dset_train, dset_valid = create_0_binary_class_dset()\n    dl_train = DataLoader(dset_train, batch_size=256, shuffle=True)\n    dl_valid = DataLoader(dset_valid, batch_size=256, shuffle=True)\n\n    #1. Initialize parameters\n    weights = init_params((28*28,1))\n    bias = init_params(1)\n    lr = .75\n\n    epochs = 10\n    for _ in range(epochs):\n        for xb, yb in dl_train:\n            # 2. Make a prediction\n            # preds will be a vector with an output for each image in the batch. so 256x1\n            #print(xb.shape)\n            preds = model_linear(xb, weights, bias)\n            # 3. Calculate Loss\n            loss = loss_0_bin(preds, yb)\n            # 4. Get gradients\n            loss.backward()\n            weights.data -= weights.grad*lr\n            weights.grad.zero_()\n            bias.data -= bias.grad*lr\n            bias.grad.zero_()\n        # Now, because the params w/b have been modified slightly by each batch, we can determine how good they are now.\n        accs = [batch_acc(model_linear(xb, weights, bias), yb) for xb, yb in dl_valid]\n        print(\"accuracy\",torch.stack(accs).mean())\n\nmain()\n\n\n\naccuracy tensor(0.9007)\naccuracy tensor(0.8996)\naccuracy tensor(0.9012)\naccuracy tensor(0.9041)\naccuracy tensor(0.9012)\naccuracy tensor(0.8998)\naccuracy tensor(0.9013)\naccuracy tensor(0.9013)\naccuracy tensor(0.9028)\naccuracy tensor(0.9043)\n\n\nSomething I also had to add was determining the accuracy of each epoch (or training iteration). We want to be able to have some sense of if what we’re doing is working. I train the model on the training data, then I test of accuracy with the validation data which is what we did in the last post.\nInterestingly enough, after only one epoch we get accuracy of ~90%. Keep in mind this is still just binary classification, but it goes to show how quickly we can get acccurately tuned parameters."
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p2/index.html#returning-to-full-multi-class-classification",
    "href": "posts/2025_04_14_MNIST_p2/index.html#returning-to-full-multi-class-classification",
    "title": "Classifying MNIST Digits (part 2)",
    "section": "",
    "text": "Ok, I’m glad I took that detour. Now I can just slightly modify what I did to apply it to a multi class. I think it makes sense to go through the changes first before I show off what I built.\n\n\nThe first thing I did was make my code a bit more readable by breaking it into functions. Actually, I pretty much just used the ones from the fastai lesson.\nThere were really only 3 main pieces that had to change:\n\nInitializing the parameters: For binary classification, each image only needs one output which is essentially true/false to the question, “is this image a zero?”. For multi-class, each image needs 10 different scores. We do this by changing our weights and biases initialization. They change from weights, bias = init_params((28*28,1)), init_params(1) to weights, bias = init_params((28*28,10)), init_params(10). A note: all I had to do was add TWO ZEROES but this took me forever to figure out…\nLoss Function: This was really tricky too. I’ll get into it shortly.\nAccuracy Function: We just had to see if the prediction lined up with the actual correct answer. I did it by finding the index of the 1x10 vector that was highest. Since I set my data up this way, we know that that index corresponds exactly to the labels we had in our “correct answers” data (y_label or targets). If I was classifying dog breeds, I’d have to come up with a different way of doing this, likely using a map that maps indices to the corresponding breed. It’s very convenient that index corresponds perfectly with the label.\n\n\n\n\nOk so I realized that we needed another one. My plan was to determine which digit that the parameters predicted the image was, compare it to the correct target labels, and then get the average number of misses per batch as my loss function. Two issues with this. First was implementation. If I use argmax on the prediction variable, I lose gradient tracking of my parameters w and b. So our whole stochastic gradient descent method falls apart. Second was conceptual. If this did work somehow, I’m not being optimal. My model would treat these two classifications of a 6 the same:\n\nHigh Confidence 6: [0.01, 0.01, 0.02, 0.02, 0.01, 0.01, 0.90, 0.01, 0.01, 0.01]\nLow Confidence 6: [0.09, 0.10, 0.08, 0.10, 0.10, 0.08, 0.13, 0.09, 0.11, 0.12]\n\nBoth of these prediction outputs predict a 6 because the seventh value is highest. However, the first output predicts it more optimally because it is significantly more confident that it is a 6 than any other digit.\nOur loss function is really important. It’s our way of telling the model EXACTLY what we want it to do using math. We don’t just want our model to get the right classification, we want it to get the right classification and be extremely confident about it.\nI just wanted to understand the math behind this better. Apparently the right way to do this problem is to use log_softmax(), where softmax is:\n\\(\\text{softmax}(r) = \\frac{e^{r_i}}{\\sum_{j} r_j}\\)\n\n\nCode\n# I just need to convince myself I understand the math of this new loss function...\nr = tensor([2.0, 1.0, 0.1])\n\nexp_r= torch.exp(r)\nnorm_exp_r = exp_r/exp_r.sum()\nassert(torch.allclose(norm_exp_r, F.softmax(r, dim=0), atol=1e-4))\nlog_norm_exp_r = torch.log(norm_exp_r)\nassert(torch.allclose(log_norm_exp_r, r.log_softmax(dim=0), atol=1e-4))\n\n\nSoftmax makes it so that the sum of the predictions vector is 1, which is essentially turning it into a vector of percentages that the current image is each of the 10 digits.\nHere’s how our loss function modifies the input to make it better and better:\n\nRaw Predictions: The index that contains the highest value is our predicted digit, no correlation to other values.\nSoftmax: Probabilities that image could be the digit at each index. Sum is 1, so these are relative to the other values in the vector. For example, if the first number is .75 and the second is .25, then the others have to all be 0, even if they were non zero at the last stage. Relativizing the prediction allows us to get more information from a single number.\nLog Softmax: This emphasizes penalty when the prediction is confidently incorrect, and still gives a small penalty when it gets the correct prediction not confidently. It’s the distribution of the -log function. It’s really similar to the way that we used RMSE to mega-penalize certain mistakes in part 1. It’s kind of like partial credit in math class. Oh, you were supposed to say this image was a 7? Were you:\n\nRight and confident? p=0.9: You don’t lose any points\nRight, but not sure? p=0.5: You lose a few point\nConfidently incorrect? p=0.1: You lose lots of points\n\n\n\n\nCode\n# Why do we take log of soft_max? Check index 6 (or the seventh digit) of the output\nhigh_conf = tensor([0.01, 0.01, 0.02, 0.02, 0.01, 0.01, 0.90, 0.01, 0.01, 0.01])\nlow_conf = tensor([0.09, 0.10, 0.08, 0.10, 0.10, 0.08, 0.13, 0.09, 0.11, 0.12])\n\nF.softmax(high_conf, dim=0), F.softmax(low_conf, dim=0), -high_conf.log_softmax(dim=0), -low_conf.log_softmax(dim=0),   \n\n\n(tensor([0.0873, 0.0873, 0.0882, 0.0882, 0.0873, 0.0873, 0.2126, 0.0873, 0.0873, 0.0873]),\n tensor([0.0990, 0.1000, 0.0980, 0.1000, 0.1000, 0.0980, 0.1030, 0.0990, 0.1010, 0.1020]),\n tensor([2.4384, 2.4384, 2.4284, 2.4284, 2.4384, 2.4384, 1.5484, 2.4384, 2.4384, 2.4384]),\n tensor([2.3127, 2.3027, 2.3227, 2.3027, 2.3027, 2.3227, 2.2727, 2.3127, 2.2927, 2.2827]))\n\n\nThe seventh prediction (corresponding to digit 6 since the first prediction is for 0) is what to focus on. With log_softmax() there is a much greater difference for the low confidence and high confidence predictions. This means that we have packed lots of information into a single digit, which is great for our model and highly optimal."
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p2/index.html#final-code-for-multi-class-mnist-low-level-training-loop",
    "href": "posts/2025_04_14_MNIST_p2/index.html#final-code-for-multi-class-mnist-low-level-training-loop",
    "title": "Classifying MNIST Digits (part 2)",
    "section": "",
    "text": "Code\n# Basically we want the shape of logits that we had in the golden mean baseline, so our parameters have to mirror that shape\n# Declare global params variable. I know there's a better OOP way to do this, but it's chill...\n\n# Turn the dataset into randomized dataloaders\ndl_train = DataLoader(dset_training, batch_size=256, shuffle=True)\ndl_valid = DataLoader(dset_validation, batch_size=256, shuffle=True)\n\n#1. Initialize parameters\nlr = .25\nepochs = 20\nweights = init_params((28*28,10))\nbias = init_params(10)\nparams = weights,bias\n\ndef main_multi_class():\n    print(validate_epoch(linear_1))\n    for _ in range(epochs):\n        train_epoch(linear_1, lr, dl_train)\n        print(validate_epoch(linear_1))\n\ndef init_params(shape, std = 1.0): return (torch.randn(shape)*std).requires_grad_()\n\ndef linear_1(xb): \n    w, b = params\n    #print(\"model\", w, b)\n    return xb@w + b\n\ndef mnist_loss(preds, yb):\n    #print(preds.shape, yb.shape)\n    yb = yb.squeeze()\n    log_sm_probs = preds.log_softmax(dim=1)\n    return -log_sm_probs[range(len(yb)), yb].mean()\n    \ndef train_epoch(model, lr, dl):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\n\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    #print(loss.shape)\n    loss.backward()\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in dl_valid]\n    return round(torch.stack(accs).mean().item(), 4)\n\ndef batch_accuracy(preds, yb):\n    max_indices = preds.argmax(dim=1).unsqueeze(1)\n    #print(\"args\", max_indices)\n    correct = (max_indices == yb)\n    acc = correct.float().mean()\n    return acc\n\nmain_multi_class()\n\n\n0.0965\n0.7646\n0.8165\n0.8429\n0.8542\n0.8629\n0.8701\n0.8687\n0.8738\n0.8785\n0.8822\n0.8799\n0.8878\n0.8855\n0.8871\n0.8911\n0.8893\n0.8934\n0.8907\n0.8931\n0.8936"
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p2/index.html#a-pivot-point",
    "href": "posts/2025_04_14_MNIST_p2/index.html#a-pivot-point",
    "title": "Classifying MNIST Digits (part 2)",
    "section": "",
    "text": "So, amazingly we have just “succeeded”! We have built a machine learning model that is able to classify digits in the MNIST dataset, and that outperforms our baseline classification of 81.73%! There’s a few more things to consider but this is definitely a moment to celebrate and enjoy. We’ve used what we have been learning to outperform our benchmark. Good job.\nWe will now slightly optimize the code, and then we’ll look at some deep learning models. Remember, the linear model is just an example of machine learning, and we’re interested in comparing the performance of both."
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p2/index.html#optimizing-the-code",
    "href": "posts/2025_04_14_MNIST_p2/index.html#optimizing-the-code",
    "title": "Classifying MNIST Digits (part 2)",
    "section": "",
    "text": "Ok, so what we did was the custom version of this. We now want to take steps to create the fastai/pyTorch extraction.\nEverything we have just completed actually works. However, we have just been looking under the hood. These details matter for my comprehension of the math and coding, but for actual training I would never create a custom training loop. This training loop is the main engine that drives most ML systems and we just built it from scratch. Think about a car engine. There’s no way I will ever need to build a car engine from scratch. If I’m building my own car, I might buy an engine, and knowledge of how it works will help me choose which to buy for the car I want to build. Most people will just end up buying the whole car and be blissfully unaware of the fact that it comes with a functioning engine.\nIn this analogy, fastai is the engine factory. So we will shape our custom engine into something closer to what gets produced in the factory. One thing to consider is that the pytorch library provides several models as a part of its packaging like nn.Linear. Instead of writing our own version of this, we want to use the one that is already provided. We also need an “optimizer”, so that we mirror the factory more accurately.\n\n\nCode\nlinear_model = nn.Linear(28*28,10)\n\nclass BasicOptim:\n    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n    def step(self, *args, **kwargs):\n        for p in self.params: p.data -= p.grad.data * self.lr\n\n    def zero_grad(self, *args, **kwargs):\n        for p in self.params: p.grad = None\n\nopt = BasicOptim(linear_model.parameters(), lr)\n\ndef train_epoch(model):\n    for xb,yb in dl_train:\n        #print(xb.dtype, yb.dtype)\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n\n\nTurning the parameters into class variables makes the code stronger. Earlier we had to rely on a global params variable. In our tiny use case this was no problem, but as the codebase grows global variables are a terrible idea.\nThen, we create a new optimizer and redefine our training function. This is the only change we have to make to get the same results as before!\n\n\nCode\ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch(model)\n        print(validate_epoch(model), end=' ')\n\n\nprint(validate_epoch(linear_model), end=' ')\ntrain_model(linear_model, 20)\n\n\n0.087 0.9045 0.9125 0.9155 0.9151 0.9197 0.9178 0.9211 0.9211 0.9195 0.9223 0.9227 0.9226 0.9207 0.924 0.9214 0.9234 0.9247 0.9256 0.9214 0.9247 \n\n\nPretty good. We can do the same thing the actual RIGHT way using fastai classes\n\n\nCode\nlr = .5\nlinear_model = nn.Linear(28*28,10)\nopt = SGD(linear_model.parameters(), lr)\ntrain_model(linear_model, 20)\n\ndls = DataLoaders(dl_train, dl_valid)\n\nlearn = Learner(dls, nn.Linear(28*28,10), opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\nlearn.fit(10, lr=lr)\n\n\n0.9095 0.9143 0.9174 0.9186 0.9194 0.9204 0.9245 0.9215 0.9226 0.9232 0.9242 0.9229 0.9244 0.9246 0.9243 0.9237 0.9234 0.9222 0.9218 0.9248 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.372398\n0.331982\n0.908200\n00:00\n\n\n1\n0.316947\n0.303885\n0.916700\n00:00\n\n\n2\n0.302206\n0.293496\n0.918900\n00:00\n\n\n3\n0.294819\n0.288783\n0.918600\n00:00\n\n\n4\n0.293716\n0.285339\n0.920900\n00:00\n\n\n5\n0.283038\n0.277278\n0.922900\n00:00\n\n\n6\n0.273962\n0.275699\n0.920400\n00:00\n\n\n7\n0.272748\n0.276465\n0.921400\n00:00\n\n\n8\n0.265907\n0.276644\n0.919700\n00:00\n\n\n9\n0.279539\n0.274251\n0.921500\n00:00\n\n\n\n\n\nSo what we just did was rewrite the custom engine using prebuilt parts. We proved that our custom engine was almost as functional as the store-bought version (or COTS if you prefer). I say “almost” because we seemed to get an accuracy cap of 90% on our custom engine, but the optimized version capped around 92%."
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p2/index.html#deep-learning-models",
    "href": "posts/2025_04_14_MNIST_p2/index.html#deep-learning-models",
    "title": "Classifying MNIST Digits (part 2)",
    "section": "",
    "text": "Remember earlier we talked about the difference between ML and DL as using layers of models. The way we do this is to apply more than one model and separate those models with a non-linearity. So, more fancy words. If we had a function \\(y_1 = m_1 \\times x +b_1\\) and another function \\(y_2 = m_2 \\times x+b_2\\), we couldn’t just add these two together and expect a different type of function. If we did, we would just get \\(y_{1+2} = m_{1+2} \\times x+b_{1+2}\\). The problem is that the model doesn’t care about the difference between \\(m_1\\) and \\(m_2\\); it just see weights and a bias.\nWe can separate these models with something like a Recitified Linear Unit (or ReLu). All it does is \\(\\text{ReLu}(x)\\) turns the number into zero if its negative, otherwise it leaves it alone. This little ReLu turns two linear models into a very simple neural net.\nThe genius of this is that instead of fitting a line to data, we can fit several layers of Linear/ReLus to achieve a very precise function. Most data won’t be fit well by a linear model. However, almost all data can be fit by some arbitrarily large combination of linear models and ReLus.\n\n\n\nlinear\n\n\n\n\nCode\nsimple_net = nn.Sequential(\n    nn.Linear(28*28,30),\n    nn.ReLU(),\n    nn.Linear(30,10)\n)\n\nlearn = Learner(dls, simple_net, opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\nlr = .25\nlearn.fit(20, lr=lr)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.362304\n0.329384\n0.905600\n00:00\n\n\n1\n0.278429\n0.249775\n0.928900\n00:00\n\n\n2\n0.237960\n0.229455\n0.930400\n00:00\n\n\n3\n0.208032\n0.208346\n0.937100\n00:00\n\n\n4\n0.181758\n0.183263\n0.945700\n00:00\n\n\n5\n0.167817\n0.172169\n0.949300\n00:00\n\n\n6\n0.157353\n0.155710\n0.953900\n00:00\n\n\n7\n0.145984\n0.175108\n0.947600\n00:00\n\n\n8\n0.142243\n0.185708\n0.939700\n00:00\n\n\n9\n0.131697\n0.151334\n0.953200\n00:00\n\n\n10\n0.120169\n0.140592\n0.957600\n00:00\n\n\n11\n0.118723\n0.128918\n0.961100\n00:00\n\n\n12\n0.108958\n0.134065\n0.961300\n00:00\n\n\n13\n0.101567\n0.130075\n0.960400\n00:00\n\n\n14\n0.105227\n0.120318\n0.961100\n00:00\n\n\n15\n0.099600\n0.115329\n0.964500\n00:00\n\n\n16\n0.094892\n0.120461\n0.964300\n00:00\n\n\n17\n0.097695\n0.119797\n0.963400\n00:00\n\n\n18\n0.091835\n0.141129\n0.958500\n00:00\n\n\n19\n0.087854\n0.120385\n0.964300\n00:00\n\n\n\n\n\n\n\n\n2relu\n\n\nWe can keep going! We can use resnet18 which contains many more than 2 layers. These are the layers that we refer to when we say “deep learning”; its deep because it has so many layers of linear and nonlinear activations!\n\n\nCode\n\ndls = ImageDataLoaders.from_folder(\n    path,\n    train='training',\n    valid='testing',\n    valid_pct=None,        # use full test set\n    seed=42,\n    bs=64,                  # you can adjust batch size\n    item_tfms=Resize(224)   # resnet needs larger images\n)\nlearn = vision_learner(dls, resnet18, pretrained=False,\n                    loss_func=F.cross_entropy, metrics=accuracy)\nlearn.fit_one_cycle(1, 0.1)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.150041\n0.057427\n0.985800\n00:55"
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p2/index.html#conclusion",
    "href": "posts/2025_04_14_MNIST_p2/index.html#conclusion",
    "title": "Classifying MNIST Digits (part 2)",
    "section": "",
    "text": "Did we meet our goal? We wanted to classify digits in the MNIST dataset more accurately than our baseline of ~83%. We beat it immediately with our first ML model in around 2-3 epochs with 90%. We then optimized our code and got 92% accuracy. We used the simplest possible deep learning model, a 2-layer neural net with a non-linear activation, and got 96% accuracy. Then we used a more standard deep learning model, resnet 18, and got 98% accuracy in only one epoch. Granted, this last training took longer than the others.\nOk so we learned a lot.\n\nSaw the difference in performance between baseline, ML, and DL\nLearned how to format data and optimize code.\nGot a grounding in the mathematical principles of ML and DL\n\nThis was a hard project for me and I’m glad I finished. The hardest part was writing the code from scratch. Hopefully I never have to do that again, but at least I know I can if necessary. It’s pretty amazing to think about the potential of activating deep learning models on all sorts of intriguing datasets!"
  },
  {
    "objectID": "posts/emotions_classifier/index.html",
    "href": "posts/emotions_classifier/index.html",
    "title": "Training an Emotional Face Classifier",
    "section": "",
    "text": "image\nOk I have been trying to get this thing posted for weeks and its finally time now. First step, what am I even doing. Well, broadly the goal is to get a decent understanding of deep learning concepts and apply them to subject matter I find interesting. I see this stage as learning to read sheet music and musc theory before I can create compositions.\nThe first step is going to be training the model. It’s taken me a while to decide what I want to train it on. Originally I wanted to do something music related, but now the desire to get something UP and POSTED has made me go with my first idea of emotion recognition in human faces."
  },
  {
    "objectID": "posts/emotions_classifier/index.html#local-environment-detour",
    "href": "posts/emotions_classifier/index.html#local-environment-detour",
    "title": "Training an Emotional Face Classifier",
    "section": "Local Environment Detour",
    "text": "Local Environment Detour\nOk, its around here that I had to shift gears to set up my own local fastai and jupyter environment on my ML desktop. Long story short I built an ML workstation for this purpose. I needed to do this because I’m having trouble understanding how the cleaner modifies files and I can understand a local filesystem more clearly than google drive’s (w/ colab).\nAlso, if you’ve ever tried to make a local ML environment than you’ll know how PERSNICKITY this process can be (read: highly specific and thorny). So anyway, this is what I’ve been doing for the last day or so. Also, my latop broke and the first one I ordered to replace it DID NOT COME WITH A KEYBOARD 🙃\nWe soldier on…\nNow I have edited the files in the original path. Some were moved and others were deleted. If I rebuild another dataloader from the original path it should be more accurate this time!"
  },
  {
    "objectID": "posts/emotions_classifier/index.html#epilogue",
    "href": "posts/emotions_classifier/index.html#epilogue",
    "title": "Training an Emotional Face Classifier",
    "section": "Epilogue",
    "text": "Epilogue\nLater I wanted to see if I could get better performance with the model if I got into all the images and modified them individually like in the cleaner. It took me roughly a half hour, and tI couldn’t get it better than the second round of training. So, my assumption is that this problem (automating facial emotion ndetection) is simply non-trivial and there may be other ways to approach it. Also, determining the emotions of a face in a photo is highly subjective which could be another reason I’m not seeing better results.\nI was really surprised by how much of an effective time save using the cleaner turned out to be! The first time I used it it took around 5 minutes and I got the same results as my 30 minutes image crawl of 600 pictures."
  },
  {
    "objectID": "posts/nlp_tweets/index.html",
    "href": "posts/nlp_tweets/index.html",
    "title": "Predicting Disaster Tweets with NLP",
    "section": "",
    "text": "nlp_tweets.png\n\n\nThis lesson focused on applying NLP using hugging face’s library. In the book we used the fastai library. I decided to apply what I learned to the kaggle NLP disaster tweets competition. I’ll be referencing the two course notebooks for this lesson Chapter 10 and getting-started-with-NLP\nThe goal of this competition is to take a set of tweets and determine based on their metadata whether or not they refer to real disasters. We use Natural Language Processing (NLP) to fit a model to the tweets to make our predictions.\nThe first thing I did was make sure I could download the dataset."
  },
  {
    "objectID": "posts/nlp_tweets/index.html#analyzing-data-distributions",
    "href": "posts/nlp_tweets/index.html#analyzing-data-distributions",
    "title": "Predicting Disaster Tweets with NLP",
    "section": "Analyzing Data Distributions",
    "text": "Analyzing Data Distributions\nSince this is my first time looking at the dataset, I want to see what category breakdowns seem to be significant. Basically, I want to get a sense of what variables it might be helpful to look at more closely.\n\n\nCode\ndf.keyword.value_counts()\n\n\nkeyword\nfatalities               45\ndeluge                   42\narmageddon               42\nsinking                  41\ndamage                   41\n                         ..\nforest%20fire            19\nepicentre                12\nthreat                   11\ninundation               10\nradiation%20emergency     9\nName: count, Length: 221, dtype: int64\n\n\n\n\nCode\ndf.location.value_counts()\n\n\nlocation\nUSA                    104\nNew York                71\nUnited States           50\nLondon                  45\nCanada                  29\n                      ... \nMontrÌ©al, QuÌ©bec       1\nMontreal                 1\nÌÏT: 6.4682,3.18287      1\nLive4Heed??              1\nLincoln                  1\nName: count, Length: 3341, dtype: int64"
  },
  {
    "objectID": "posts/nlp_tweets/index.html#create-another-baseline",
    "href": "posts/nlp_tweets/index.html#create-another-baseline",
    "title": "Predicting Disaster Tweets with NLP",
    "section": "Create another baseline",
    "text": "Create another baseline\nI spent a good amount of time looking into novel ways to break the data down to give the model an optimal input. I considered:\n\nremoving the %20 and replacing with a space for the keywords\nremoving the location to see what impact that has\ncollapsing duplicate locations into one (theoretically, USA == United States)\n\nThen I realized, before I can determine how those edits would affect my results… I need results. I was reminded that for our last lesson we started by creating a baseline. I decided to do this in the simplest way I could think of and iterate from there. To me, that was squishing all the default features of the data into a single string, and training the model on that input.\n\n\nCode\ninput_col = 'inputs'\nna_fill = ''\ndf[input_col] = 'LOC: ' + df.location.fillna(na_fill) + '; KW: ' + df.keyword.fillna(na_fill) + '; TEXT: ' + df.text.fillna(na_fill)\ndf.head()\n\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\ninputs\n\n\n\n\n0\n1\nNaN\nNaN\nOur Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n1\nLOC: ; KW: ; TEXT: Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n\n\n1\n4\nNaN\nNaN\nForest fire near La Ronge Sask. Canada\n1\nLOC: ; KW: ; TEXT: Forest fire near La Ronge Sask. Canada\n\n\n2\n5\nNaN\nNaN\nAll residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n1\nLOC: ; KW: ; TEXT: All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n\n\n3\n6\nNaN\nNaN\n13,000 people receive #wildfires evacuation orders in California\n1\nLOC: ; KW: ; TEXT: 13,000 people receive #wildfires evacuation orders in California\n\n\n4\n7\nNaN\nNaN\nJust got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school\n1\nLOC: ; KW: ; TEXT: Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school\n\n\n\n\n\n\n\n\n\nCode\n# Suppress warnings to make the output look cleaner\nfrom transformers.utils import logging\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\n\n\nCode\nfrom datasets import Dataset,DatasetDict\nfrom transformers import AutoModelForSequenceClassification,AutoTokenizer\n\n# convert dataframe into a huggingface dataset\nds = Dataset.from_pandas(df)\n\nmodel_nm = 'microsoft/deberta-v3-small'\ntokz = AutoTokenizer.from_pretrained(model_nm)\ndef tok_func(x): return tokz(x[input_col])\n\n# tokenize the input\ntok_ds = ds.map(tok_func, batched=True)\ntok_ds = tok_ds.rename_columns({'target':'labels'})\n\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n\n\n\nI briefly considered trying to engineer a perfect validation dataset before remembering that the goal was to create a simple baseline first\n\n\nCode\n# Splitting up the validation set now\ndds = tok_ds.train_test_split(0.25, seed=1337)\ndds\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'keyword', 'location', 'text', 'labels', 'inputs', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 5709\n    })\n    test: Dataset({\n        features: ['id', 'keyword', 'location', 'text', 'labels', 'inputs', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 1904\n    })\n})"
  },
  {
    "objectID": "posts/nlp_tweets/index.html#defining-the-f1-metric",
    "href": "posts/nlp_tweets/index.html#defining-the-f1-metric",
    "title": "Predicting Disaster Tweets with NLP",
    "section": "Defining the F1 Metric",
    "text": "Defining the F1 Metric\nThe kaggle competition uses the F1 metric between the predicted values and expected values. So we have to define it in a way that huggingface understands to use it in our training.\n\n\nCode\nfrom sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=1)\n    return {\n        \"f1\": f1_score(labels, preds),\n        \"accuracy\": accuracy_score(labels, preds),\n        \"precision\": precision_score(labels, preds),\n        \"recall\": recall_score(labels, preds)\n    }\ndf_eval = dds['test']\ndf_eval\n\n\nDataset({\n    features: ['id', 'keyword', 'location', 'text', 'labels', 'inputs', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 1904\n})\n\n\nNow we can run our training loop. I’m not deeply concerned with every hyper parameter here, because I’m not there yet. I just want to focus on epochs, batch size bs and learning rate lr.\n\n\nCode\nfrom transformers import TrainingArguments,Trainer\n\n\nepochs = 4\nbs = 128\nlr = 8e-5\n\nargs = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n    num_train_epochs=epochs, weight_decay=0.01, report_to='none')\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=2)\ntrainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n                  tokenizer=tokz, compute_metrics=compute_metrics)\n\ntrainer.train();\n\n\nSome weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nYou're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\n\n    \n      \n      \n      [180/180 00:27, Epoch 4/4]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1\nAccuracy\nPrecision\nRecall\n\n\n\n\n1\nNo log\n0.524064\n0.628931\n0.752101\n0.950119\n0.470035\n\n\n2\nNo log\n0.456287\n0.751911\n0.812500\n0.920068\n0.635723\n\n\n3\nNo log\n0.462908\n0.783172\n0.824055\n0.871758\n0.710928\n\n\n4\nNo log\n0.476750\n0.787008\n0.820903\n0.840000\n0.740306"
  },
  {
    "objectID": "posts/nlp_tweets/index.html#analyzing-the-models-trouble-tweets-or-error-analysis",
    "href": "posts/nlp_tweets/index.html#analyzing-the-models-trouble-tweets-or-error-analysis",
    "title": "Predicting Disaster Tweets with NLP",
    "section": "Analyzing the Model’s “Trouble” Tweets (or Error Analysis)",
    "text": "Analyzing the Model’s “Trouble” Tweets (or Error Analysis)\nFirst I’ll store the main outputs of the training for further analysis\n\n\nCode\nimport torch\nimport torch.nn.functional as F\nfrom fastai.vision.all import *\n\n# save our validation set under a new name\ndf_eval = dds['test']\n\n# Get predictions and probabilities\noutputs = trainer.predict(df_eval)\nlogits = outputs.predictions\nlabels = outputs.label_ids\nprobs = F.softmax(tensor(logits), dim=1).numpy()\npreds = probs.argmax(axis=1)\n\n\n\n\n\nNow I want to create a dataframe that has additional relevant features. We already have the training output, so these new features will be based on the training. Once I have this dataframe I can start visualizing it in different ways\n\n\nCode\n# Compute per-example loss\nlosses = [F.cross_entropy(tensor(logit), tensor(preds)) for logit, preds in zip(logits, preds)]\n\ndef count_cap(text):\n    words = text.split()\n    count = 0\n    for word in words:\n        if word and word[0].isupper():\n            count += 1\n    return count\n\n# Set up up new dataframe with the results of the first pass\ndf_valid_results = pd.DataFrame({\n    \"text\": [df_eval[i][\"text\"] for i in range(len(df_eval))],\n    \"keyword\": [df_eval[i].get(\"keyword\") or \"n/a\" for i in range(len(labels))],\n    \"location\": [df_eval[i].get(\"location\") or \"n/a\" for i in range(len(labels))],\n    \"label\": labels,\n    \"cap_ratio\": [count_cap(twt)/float(len(twt)) for twt in df_eval['text']],\n    \"pred\": preds,\n    \"prob_1\": probs[:, 1],\n    \"contains_link\": [twt.count(\"http\") &gt; 0 for twt in df_eval['text']],\n    \"tweet_len\": [len(twt) for twt in df_eval['text']],\n    \"hashtags\": [twt.count(\"#\") for twt in df_eval['text']],\n    \"is_location\": [bool(loc) for loc in df_eval['location']],\n    \"loss\": losses\n})\n\n# Tag confusion type\ndef label_type(row):\n    if row.label == 1 and row.pred == 1: return \"TP\"\n    elif row.label == 0 and row.pred == 0: return \"TN\"\n    elif row.label == 1 and row.pred == 0: return \"FN\"\n    else: return \"FP\"\n\ndf_valid_results[\"type\"] = df_valid_results.apply(label_type, axis=1)"
  },
  {
    "objectID": "posts/nlp_tweets/index.html#eda-on-baseline-results",
    "href": "posts/nlp_tweets/index.html#eda-on-baseline-results",
    "title": "Predicting Disaster Tweets with NLP",
    "section": "EDA on Baseline Results",
    "text": "EDA on Baseline Results\nYou can see from the creation of this dataset some of my ideas on relevant features. Frankly I spent too much time going down the feature engineering rabbit hole. The whole point of our baseline is that we can see what are the tweets that the model find hardest to classify. Theoretically this will give us some insight on how to structure a more optimal input string.\nThe first method I tried was to sort each tweet by the cross-entropy loss function.\n\n\nCode\ndf_valid_results.sort_values(by=\"loss\", ascending=False).head()\n\n\n\n\n\n\n\n\n\ntext\nkeyword\nlocation\nlabel\ncap_ratio\npred\nprob_1\ncontains_link\ntweet_len\nhashtags\nis_location\nloss\ntype\n\n\n\n\n264\n'Since1970the 2 biggest depreciations in CAD:USD in yr b4federal election coincide w/landslide win for opposition' http://t.co/wgqKXmby3B\nlandslide\nn/a\n0\n0.007299\n1\n0.500412\nTrue\n137\n0\nFalse\ntensor(0.6923)\nFP\n\n\n1831\nFirepower in the lab [electronic resource] : automation in the fight against infectious diseases and bioterrorism /‰Û_ http://t.co/KvpbybglSR\nbioterrorism\nn/a\n0\n0.007092\n1\n0.501099\nTrue\n141\n0\nFalse\ntensor(0.6910)\nFP\n\n\n654\n@JakeGint the mass murder got her hot and bothered but at heart she was always a traditionalist.\nmass%20murder\nn/a\n1\n0.000000\n0\n0.498703\nFalse\n96\n0\nFalse\ntensor(0.6906)\nFN\n\n\n399\nOf what use exactly is the national Assembly? Honestly they are worthless. We are derailed.\nderailed\nKwara, Nigeria\n0\n0.043956\n0\n0.496643\nFalse\n91\n0\nTrue\ntensor(0.6865)\nTN\n\n\n960\nHat #russian soviet army kgb military #cossack #ushanka LINK:\\nhttp://t.co/bla42Rdt1O http://t.co/EInSQS8tFq\nmilitary\nn/a\n0\n0.018349\n1\n0.503387\nTrue\n109\n3\nFalse\ntensor(0.6864)\nFP\n\n\n\n\n\n\n\nI can immediately see a problem with this. We don’t only get incorrect predictions! Indeed the highest loss value across our entire dataset was a correct prediction. This means, we have to sort our data by different values to determine which tweets caused the most trouble. My strategy was to sort by a new “confidence” feature, and only look at the data that was predicted incorrectly.\n\n\nCode\ndf_valid_results[\"prob_0\"] = 1 - df_valid_results[\"prob_1\"]\ndf_valid_results[\"is_wrong\"] = (df_valid_results[\"label\"] != df_valid_results[\"pred\"])\ndf_valid_results[\"confidence\"] = df_valid_results[[\"prob_1\", \"prob_0\"]].max(axis=1)\n\nconf_sorted = df_valid_results.sort_values(by=\"confidence\", ascending=False)\nfiltered = conf_sorted[conf_sorted[\"is_wrong\"] == True]\n\n\nfiltered.head()\n\n\n\n\n\n\n\n\n\ntext\nkeyword\nlocation\nlabel\ncap_ratio\npred\nprob_1\ncontains_link\ntweet_len\nhashtags\nis_location\nloss\ntype\nprob_0\nis_wrong\nconfidence\n\n\n\n\n770\nOver half of poll respondents worry nuclear disaster fading from public consciousness http://t.co/YtnnnD631z ##fukushima\nnuclear%20disaster\nFukushima city Fukushima.pref\n0\n0.008333\n1\n0.998309\nTrue\n120\n2\nTrue\ntensor(0.0017)\nFP\n0.001691\nTrue\n0.998309\n\n\n1877\nAngry Woman Openly Accuses NEMA Of Stealing Relief Materials Meant For IDPs: An angry Internally Displaced wom... http://t.co/6ySbCSSzYS\ndisplaced\nNigeria\n0\n0.110294\n1\n0.998276\nTrue\n136\n0\nTrue\ntensor(0.0017)\nFP\n0.001724\nTrue\n0.998276\n\n\n305\n#hot C-130 specially modified to land in a stadium and rescue hostages in Iran in 1980 http://t.co/zY3hpdJNwg #prebreak #best\nhostages\nchina\n0\n0.015873\n1\n0.998269\nTrue\n126\n3\nTrue\ntensor(0.0017)\nFP\n0.001731\nTrue\n0.998269\n\n\n89\nSatellite Spies Super Typhoon Soudelor from Space (Photo) http://t.co/VBhu2t8wgB\ntyphoon\nEvergreen Colorado\n0\n0.075000\n1\n0.998245\nTrue\n80\n0\nTrue\ntensor(0.0018)\nFP\n0.001755\nTrue\n0.998245\n\n\n378\nAngry Woman Openly Accuses NEMA Of Stealing Relief Materials Meant For IDPs: An angry Internally Displaced wom... http://t.co/Khd99oZ7u3\ndisplaced\nOjodu,Lagos\n0\n0.110294\n1\n0.998168\nTrue\n136\n0\nTrue\ntensor(0.0018)\nFP\n0.001832\nTrue\n0.998168\n\n\n\n\n\n\n\nThese are the tweets that the model was most confident about that it got wrong. Unfortunately, I never really was able to extract more meaningful data from the dataset. When I compare features like keyword, location, has_link, or hashtags, there just isn’t much difference between the wrong predictions and the full dataset.\n\n\nCode\ndef compare_preds(feature):\n    full_set_data, wrongs_only_data = conf_sorted, filtered\n    max_total_perc = full_set_data[feature].value_counts().max()/len(conf_sorted)\n    max_wrong_perc = wrongs_only_data[feature].value_counts().max()/len(filtered)\n    print(feature, max_total_perc, max_wrong_perc)\n\n\ncompare_preds('keyword')\ncompare_preds('location')\ncompare_preds('contains_link')\ncompare_preds('hashtags')\nfiltered['keyword'].value_counts()\n\n\nkeyword 0.007878151260504201 0.017595307917888565\nlocation 0.3382352941176471 0.31671554252199413\ncontains_link 0.5341386554621849 0.5043988269794721\nhashtags 0.7657563025210085 0.7771260997067448\n\n\nkeyword\nbioterror              6\nhellfire               5\npandemonium            5\nburning%20buildings    4\nfire                   4\n                      ..\nsirens                 1\nwild%20fires           1\nbombed                 1\ninjured                1\nlandslide              1\nName: count, Length: 168, dtype: int64\n\n\nWe see that the keywords values is significantly different in the incorrect portion of the dataset. But again, the only meaningful way to split this would be to break up each of these problem keywords into a train/valid proportion. We would hope that the model would generalize for the first few keywords and be able to predict the others two but that doesn’t seem likely. Unfortunately, it seems like the bins for each keyword are too small for the model to make meaningful generalizations."
  },
  {
    "objectID": "posts/nlp_tweets/index.html#special-tokens",
    "href": "posts/nlp_tweets/index.html#special-tokens",
    "title": "Predicting Disaster Tweets with NLP",
    "section": "Special Tokens?",
    "text": "Special Tokens?\nI noticed that the high-confidence incorrect predictions above all had links. I wonder what ahppens if I make some subset of links, tags, and hashtags into special tokens. I think it makes more sense to do it with tags and links because the text of each of those doesn’t have much to do with the meaning of the data. The hashtag, however, does contain useful information. But I’ll run some experiments and see what happens.\n\n\nCode\n# create new special tokens\nnew_toks = [\"[L]\", \"[A]\", \"[X]\"]\ntokz.add_special_tokens({'additional_special_tokens': new_toks})\n\n# remove links and replace with [L]\ndf.loc[:, 'mod_text'] = (\n    df['text']\n    .astype(str) \n    .str.replace(r'https?://\\S+', '[L]', regex=True))   # Match http or https links\n\n\nna_fill = ''\ndf[input_col] = 'LOC: ' + df.location.fillna(na_fill) + '; KW: ' + df.keyword.fillna(na_fill) + '; TEXT: ' + df.mod_text.fillna(na_fill)\n\ndds = get_dds(df)\nget_trainer(dds, model).train()\n\n\n\n\n\n\n    \n      \n      \n      [180/180 00:24, Epoch 4/4]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1\nAccuracy\nPrecision\nRecall\n\n\n\n\n1\nNo log\n0.357041\n0.801756\n0.857668\n0.948097\n0.694550\n\n\n2\nNo log\n0.344914\n0.837116\n0.871849\n0.884344\n0.794677\n\n\n3\nNo log\n0.431433\n0.828608\n0.860294\n0.842726\n0.814956\n\n\n4\nNo log\n0.467705\n0.826142\n0.856092\n0.827192\n0.825095\n\n\n\n\n\n\nTrainOutput(global_step=180, training_loss=0.22589988708496095, metrics={'train_runtime': 24.4876, 'train_samples_per_second': 932.552, 'train_steps_per_second': 7.351, 'total_flos': 374670733971756.0, 'train_loss': 0.22589988708496095, 'epoch': 4.0})\n\n\nAwesome, huge jump! This makes sense because there’s no great way to tokenize links and get real meaning from them; the text of a shortened link doesn’t really tell much about whats in it.\n\n\nCode\n# keep previous formatting and remove mentions replace with [A]\ndf.loc[:, 'mod_text'] = (\n    df['text']\n    .astype(str)\n    .str.replace(r'https?://\\S+', '[L]', regex=True)\n    .str.replace(r'@\\w+', '[A]', regex=True))\n\nna_fill = ''\ndf[input_col] = 'LOC: ' + df.location.fillna(na_fill) + '; KW: ' + df.keyword.fillna(na_fill) + '; TEXT: ' + df.mod_text.fillna(na_fill)\n\ndds = get_dds(df)\nget_trainer(dds, model).train()\n\n\n\n\n\n\n    \n      \n      \n      [180/180 00:23, Epoch 4/4]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1\nAccuracy\nPrecision\nRecall\n\n\n\n\n1\nNo log\n0.532192\n0.827950\n0.862920\n0.862637\n0.795944\n\n\n2\nNo log\n0.574797\n0.800705\n0.821954\n0.746711\n0.863118\n\n\n3\nNo log\n0.538084\n0.822023\n0.853992\n0.830530\n0.813688\n\n\n4\nNo log\n0.640214\n0.824639\n0.853466\n0.817955\n0.831432\n\n\n\n\n\n\nTrainOutput(global_step=180, training_loss=0.12367356618245443, metrics={'train_runtime': 23.7139, 'train_samples_per_second': 962.979, 'train_steps_per_second': 7.59, 'total_flos': 362423531491416.0, 'train_loss': 0.12367356618245443, 'epoch': 4.0})\n\n\n\n\nCode\n# keep previous formatting and remove hashtags replace with [X]\ndf.loc[:, 'mod_text'] = (\n    df['text']\n    .astype(str)\n    .str.replace(r'https?://\\S+', '[L]', regex=True)\n    .str.replace(r'@\\w+', '[A]', regex=True)\n    .str.replace(r'#\\w+', '[X]', regex=True)\n)\n\nna_fill = ''\ndf[input_col] = 'LOC: ' + df.location.fillna(na_fill) + '; KW: ' + df.keyword.fillna(na_fill) + '; TEXT: ' + df.mod_text.fillna(na_fill)\n\ndds = get_dds(df)\nget_trainer(dds, model).train()\n\n\n\n\n\n\n    \n      \n      \n      [180/180 00:23, Epoch 4/4]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1\nAccuracy\nPrecision\nRecall\n\n\n\n\n1\nNo log\n0.682632\n0.799759\n0.825630\n0.762946\n0.840304\n\n\n2\nNo log\n0.551201\n0.802696\n0.830882\n0.776987\n0.830165\n\n\n3\nNo log\n0.612302\n0.800490\n0.828782\n0.773964\n0.828897\n\n\n4\nNo log\n0.698242\n0.804938\n0.834034\n0.784597\n0.826362\n\n\n\n\n\n\nTrainOutput(global_step=180, training_loss=0.08939082887437609, metrics={'train_runtime': 23.3512, 'train_samples_per_second': 977.936, 'train_steps_per_second': 7.708, 'total_flos': 352560827121540.0, 'train_loss': 0.08939082887437609, 'epoch': 4.0})\n\n\nOk, so as I predicted, the best combination is masking the links and mentions with a single new special token each. This makes sense because hashtags are often real words, whereas mentions are only rarely, and links never are. It’s all about tokenization. If there are pieces of data aren’t easy to tokenize, then the model spends resources attempting to do so. Or worse, it sees correlations where there aren’t any."
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p1/index.html",
    "href": "posts/2025_04_14_MNIST_p1/index.html",
    "title": "Classifying MNIST Digits (part 1)",
    "section": "",
    "text": "image\n\n\nAs I work through the fastai course, I’m committed to posting something concerning each lesson I complete. This lesson was regarding Stochastic Gradient Descent and the mathematics behind neural nets. The course instructor worked through binary classification of the mnist dataset; basically, writing software to determine if a handwritten digit was a 3 or not a 3.\nThe “challenge” problem this week was to use the same principles to do multi-class classification; given an image of a handwritten number, which digit is it most likely to be? This problem took me a while to do and as such I’ve split the work into two posts. You can find Part 2 right here\nHere is a demo of my final model.\nI appreciate the layout of the fastai course for many reasons. One of the best parts of it is its emphasis on application. So I don’t just learn the theory, I’m immediately applying it to code that can do things I’ve never done before. This week focused on what is going on behind the fastai library abstractions (or “under the hood”). If I was being asked to do this for a commercial application or personal project, I wouldn’t rewrite tiny components of fastai’s library that already exist. I did so here to follow the course, understand how model optimization works, and to learn more about what the library is actually doing.\n\n\n\nI learned that programming ML/DL systems and models is actually much harder than other types of software development. Typically, you know what your code is supposed to be doing and can write tests to prevent it from being obnoxiously buggy. If I have a simple function I can write a brief test that makes sure I’m getting the right output:\n\n\nCode\n# Simple code output test\ndef addition(x,y): return x+y\n\nassert(addition(3,7) == 10)\nprint(\"Test passes\")\n\nassert(addition(2,2) == 5)\nprint(\"This statement never displays because the above assertion will fail, 2+2 is not 5\")\n\n\nTest passes\n\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[1], line 7\n      4 assert(addition(3,7) == 10)\n      5 print(\"Test passes\")\n----&gt; 7 assert(addition(2,2) == 5)\n      8 print(\"This statement never displays because the above assertion will fail, 2+2 is not 5\")\n\nAssertionError: \n\n\n\nWe can’t do that so easily when writing ML/DL code. There are places where this type of test will work, but the main output doesn’t have a concrete measurable performance that can be tested in this way. Certainly we can use loss functions and metrics to determine how well our model is doing, but even these are highly subjective in the majority of applications.\nOne way we can get a sense of our model’s performance is to develop a baseline. So that’s what we’ll be doing here.\nI found that baselining the multi-class (10-digit) model was really helpful in enabling me to understand how to apply the concepts differently. Working through it made it possible for me to even have a clue on how to tackle the actual model.\n\n\n\nThis is just the standard notebook initialization that loads the relevant fastai libraries\n\n\nCode\n#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n\nfrom fastai.vision.all import *\nfrom fastbook import *\n\nmatplotlib.rc('image', cmap='Greys')\n\n\n\n\n\nOk so now we need to load the full dataset that has all the digits inside of it. Took me a bit to figure out how to get the full dataset, but its just URLS.MNIST.\n\n\nCode\npath = untar_data(URLs.MNIST)\n\n\nThe dataset has been downloaded and is in memory. I can use the path variable to save a reference to its top-level directory (the folder that has the downloaded data). This helps me access the data later on because I’ve saved a reference to where it exists. Now I need to prove to myself that I can navigate the file structure of the dataset, which I do in the next two codeblocks:\n\n\nCode\npath.ls()\n\n\n(#2) [Path('/home/aharon/.fastai/data/mnist_png/training'),Path('/home/aharon/.fastai/data/mnist_png/testing')]\n\n\n\n\nCode\n(path/\"training\").ls()\n\n\n(#10) [Path('/home/aharon/.fastai/data/mnist_png/training/0'),Path('/home/aharon/.fastai/data/mnist_png/training/8'),Path('/home/aharon/.fastai/data/mnist_png/training/1'),Path('/home/aharon/.fastai/data/mnist_png/training/5'),Path('/home/aharon/.fastai/data/mnist_png/training/6'),Path('/home/aharon/.fastai/data/mnist_png/training/4'),Path('/home/aharon/.fastai/data/mnist_png/training/2'),Path('/home/aharon/.fastai/data/mnist_png/training/3'),Path('/home/aharon/.fastai/data/mnist_png/training/9'),Path('/home/aharon/.fastai/data/mnist_png/training/7')]\n\n\nThis just helps me see how the data is structured. There’s two main directories (again, just folders): training and testing. And then each of these directories have 10 sub-directories; one for each numerical digit. Within each of those directories there are a large number of images that contain the handwritten pictures of those digits. The training set has 60000 images, and the testing (or validation) set has 10000.\nTo prove to myself that I understand the file structure, I’m going to write some code that grabs and displays one of the “8”s from the training set.\n\n\nCode\n# Show some images, see if I can display one of each image\n\nmnist_training_digit_paths = [p for p in (path/\"training\").ls().sorted()]\nsample_images = []\nfor p in mnist_training_digit_paths:\n    training_digit_im_paths = [im_path for im_path in p.ls().sorted()]\n    sample_images.append(Image.open(training_digit_im_paths[42]))\nsample_images[8]\n\n\n\n\n\n\n\n\n\nWhat’s helpful here is that I’ve created this variable sample_images so that I can display any number I want to (at least, in index 42 of the training set). This is great because i can more easily access other numbers; I don’t have to rewrite the whole thing just to get a different result:\n\n\nCode\nsample_images[7]\n\n\n\n\n\n\n\n\n\nAwesome! I was just trying to see if I understood the file structure and I proved to myself that I did. Now I want a single variable that contains the entire dataset so I can just type something like x['training'][6] and get all of the “6”s in the training set. There’s two more things I have to do. I turn each image into a pytorch tensor, and I normalize the pixel intensity matrix of each image… yeah, that’s certainly some high-level jargon. Basically I’m converting the data that is contained in the .png (or w/e it is) into a format that the library can use more easily.\nHere’s how I made that data structure:\n\n\nCode\ndef create_dataset(top_dir):\n    # takes a top level directory and makes the aforementioned list of 3d tensors for each digit\n    \n    # grab each of the paths for each digit directory in this folder\n    mnist_datatype_digit_paths = [p for p in top_dir.ls().sorted()] \n\n    sample_images = []\n    for p in mnist_datatype_digit_paths:\n        # grab the path to each image included for the current digit\n        training_digit_im_paths = [im_path for im_path in p.ls().sorted()] \n        # open the image as tensors for each included image\n        training_image_tensors = [tensor(Image.open(im)) for im in training_digit_im_paths] \n        # turn the list of several pixel intensity matrices into a single 3D tensor\n        stacked_digit_tensors = torch.stack(training_image_tensors).float() / 255 \n        sample_images.append(stacked_digit_tensors)\n    return sample_images\n        \nmnist_dataset = {}\nmnist_dataset[\"training\"] = create_dataset((path/\"training\"))\nmnist_dataset[\"testing\"] = create_dataset((path/\"testing\"))\nshow_image(mnist_dataset[\"training\"][7][100])\n\n\n\n\n\n\n\n\n\nThat’s one of the longer running cells, but it makes sense because its loading 70000 images into memory as tensors. But we were successful! The mnist_dataset variable now contains all of our data in a really simple way for us to access using the python programming lanuguage.\n\n\n\nRemember, we are trying to create a ML model that can determine which digit a given image is a drawing of. We need a baseline to determine if our model performs better than a non-ML method. How can we come up with a way of doing this without using an ML model?\nWell, here’s what we did in the lesson. We were only looking at a small subset of this dataset. We looked at a small percentage of the 3s and the 7s. Then, we created an “average” 3. Each of these images is a 28x28 matrix of pixel intensities, ranging from 0 to 255. If we take the average 28x28 matrix of pixel intensities across the training set, we can “see” what the average digit looks like:\n\n\nCode\nshow_image(mnist_dataset['testing'][3].mean(0))\n\n\n\n\n\n\n\n\n\nPerfect! It’s blurry because it’s the combination of the pixels densities of a few thousand threes. And if I want the average of another number (like a 7) all I have to do is change the index.\n\n\nCode\nshow_image(mnist_dataset['testing'][7].mean(0))\n\n\n\n\n\n\n\n\n\nWith our average digit, we then realized we could compare each image to it. We could get the distance between the average three and the image we want to classify mathematically. If we were only considering 3s and 7s this is simple. If the distance between the image I’m looking at and the average 3 is less than its distance to the average 7, it’s probably a 3! This is exactly what we did for binary classification in the lesson.\nBut how do we extend this to multiclass classification? I don’t just want to know if an image is a 3 or not, I want to know exactly which digit it is. Honestly, it was pretty hard for me to imagine a way to do this at first. Here’s my initial reasoning:\n\n“Ok I’m not totally sure how to do this I think I need to make something that calculates the metrics as my goal. But I think I need to define both loss functions first. Also need something that turns the loss of a single image against each golden digit. Then, we compare our test number to each of the golden digits and find which it is closest to. What If I create a 2D tensor of vectors, and then each vector is the loss function of the input compared to each of the golden digits, shape of 1,10.”\n\nThis is basically what worked! I gave each image a 1x10 vector that had its distance to each of the 10 digits. Then classification was simply determining which of the 10 distances was smallest (more simply, which digit was it closest to).\n\n\nCode\n# Loss Functions\ndef loss_L1(x, y):\n    return (x-y).abs().mean((-1,-2))\n\ndef loss_L2(x, y):\n    # Alternatively, this is called RMSE\n    return torch.sqrt(((x - y) ** 2).mean((-1, -2)))\n\n\nWe use these calculations for distance because we can’t just subtract one matrix from the other. Some numbers in it would be positive and the others would be negative. If we took the average, it wouldn’t really indicate the distance very accurately. These two functions are the most common ones that data scientists use for comparison in this context. We’ll look later at why you might use one over the other.\nSo then I continue my plan. We create the comparison matrix which is several 1x10 vectors (or a single severalx10 matrix) that has the distances to each “average digit” for every image in the dataset. Then the classify_digit function turns each of those 1x10 vectors into a prediction by finding which digit corresponds to the smallest distance for the given image. Remember, we used the training data to create the average digits, and we used the testing data to see how well this method actually worked for prediction.\n\n\nCode\ndef create_comparison_matrix(d, loss):\n    mnist_distances = []\n    d_testing_images = mnist_dataset[\"testing\"][d]\n    for digit in range(len(mnist_dataset[\"testing\"])):\n        golden_digit = mnist_dataset[\"training\"][digit].mean(0)\n        mnist_distances.append(loss(d_testing_images, golden_digit))\n    # the dim parameter allows me to modify the final shape. I want output[0] to give me the first classification vector length 10\n    return torch.stack(mnist_distances, dim=1)\n\n\ndef classify_digit(d, loss):\n    comp_matrix = create_comparison_matrix(d, loss)\n    min_vals, min_indices = comp_matrix.min(dim=1)\n    ans = (min_indices == d).sum() / len(comp_matrix)\n    return ans\n\n\nWe’re basically done! Out of my own curiosity, I wanted to see which loss function would perform better:\n\n\nCode\nprint(\"Loss Function Correct Prediction Comparison: \\n\")\nprint(\"tensor([[digit , L1Acc%, L2Acc%], \\n\")\nmetrics = []\nfor d in range(10):\n    metrics.append([d, float(classify_digit(d, loss_L1)),float(classify_digit(d, loss_L2))])\nprint(tensor(metrics))\nprint(\"Average\")\nprint(tensor(metrics).mean(0))\n\n\nLoss Function Correct Prediction Comparison: \n\ntensor([[digit , L1Acc%, L2Acc%], \n\ntensor([[0.0000, 0.8153, 0.8959],\n        [1.0000, 0.9982, 0.9621],\n        [2.0000, 0.4234, 0.7568],\n        [3.0000, 0.6089, 0.8059],\n        [4.0000, 0.6680, 0.8259],\n        [5.0000, 0.3262, 0.6861],\n        [6.0000, 0.7871, 0.8633],\n        [7.0000, 0.7646, 0.8327],\n        [8.0000, 0.4425, 0.7372],\n        [9.0000, 0.7760, 0.8067]])\nAverage\ntensor([4.5000, 0.6610, 0.8173])\n\n\nOk I guess this is pretty cool. We see that the RMSE loss function (L2 Norm) performs better. Well what does better even mean? Its worst accuracy score is a 69% on the fives, whereas the L1 norm performs at a 33% for the same number. The average accuracy of L1 is 66%, whereas RMSE performs at 82%.\nWhy is RMSE so much better? The L1 norm simply takes the absolute value of the difference between the prediction and the target. The RMSE squares the difference. Very basically, this means that bigger mistakes are penalized more heavily than for the L1 norm.\nDoes this make sense for our use case? Absolutely. We are comparing the difference in intensity of each pixel in a 28x28 grid to determine image similarity. Let’s say we were comparing something to a zero. The image we were looking at had a very dark pixel in the exact center of the screen. A zero is basically a circle, so there shouldn’t be anything in the center of the image. If we could point out that a big mistake like that is much further away from a zero, then we’d be in a really good spot. Since we are comparing pixel densities, it makes a lot of sense that for images that have some pixels that are way off of the ideal image we would get more accurate scores with the RMSE vs the L2 norm.\nHere’s another way to visualize it.\n\n\n\nRMSE_L2_compare.png\n\n\nThe RMSE is way higher on the parts of the graph where the prediction is furthest away from the true value. Bigger mistakes are penalized much more intensely than other mistakes in the same environment. L1 says “Ok, yeah that’s bad”, L2 says “This is insane, how are you this wrong?!”\n\n\n\nThis isn’t really a ML post, but it does pave the way to understand the next part in this series. We need a baseline to assess if our ML model performs better than the alternatives. In this case we saw that using the RMSE loss we could get a classification accuracy of our validation set of 81.73%. We will aim to beat that performance using Stochastic Gradient Descent in the next post.\n\n\n\nThe hardest part of this was making the conceptual shift from realizing each image needed 10 values associated with it. Once I made that leap this became way easier.\nA big part of this was just working through some of the code and fiddly bits. I had to remember and re-teach myself why sometimes the tensor.mean(x) changes based on what you want. I didn’t include it in the post but it was part of the process.\nComparing the two loss functions was a huge lightbulb moment for me, and the side-by-side graph was really helpful. Hopefully you learn something too!"
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p1/index.html#or-classifying-mnist-digits-part-1",
    "href": "posts/2025_04_14_MNIST_p1/index.html#or-classifying-mnist-digits-part-1",
    "title": "Classifying MNIST Digits (part 1)",
    "section": "",
    "text": "image\n\n\nAs I work through the fastai course, I’m committed to posting something concerning each lesson I complete. This lesson was regarding Stochastic Gradient Descent and the mathematics behind neural nets. The course instructor worked through binary classification of the mnist dataset; basically, writing software to determine if a handwritten digit was a 3 or not a 3.\nThe “challenge” problem this week was to use the same principles to do multi-class classification; given an image of a handwritten number, which digit is it most likely to be? This problem took me a while to do and as such I’ve split the work into two posts. You can find Part 2 right here\nHere is a demo of my final model.\nI appreciate the layout of the fastai course for many reasons. One of the best parts of it is its emphasis on application. So I don’t just learn the theory, I’m immediately applying it to code that can do things I’ve never done before. This week focused on what is going on behind the fastai library abstractions (or “under the hood”). If I was being asked to do this for a commercial application or personal project, I wouldn’t rewrite tiny components of fastai’s library that already exist. I did so here to follow the course, understand how model optimization works, and to learn more about what the library is actually doing."
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p1/index.html#setting-up-a-performance-baseline",
    "href": "posts/2025_04_14_MNIST_p1/index.html#setting-up-a-performance-baseline",
    "title": "Classifying MNIST Digits (part 1)",
    "section": "",
    "text": "I learned that programming ML/DL systems and models is actually much harder than other types of software development. Typically, you know what your code is supposed to be doing and can write tests to prevent it from being obnoxiously buggy. If I have a simple function I can write a brief test that makes sure I’m getting the right output:\n\n\nCode\n# Simple code output test\ndef addition(x,y): return x+y\n\nassert(addition(3,7) == 10)\nprint(\"Test passes\")\n\nassert(addition(2,2) == 5)\nprint(\"This statement never displays because the above assertion will fail, 2+2 is not 5\")\n\n\nTest passes\n\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[1], line 7\n      4 assert(addition(3,7) == 10)\n      5 print(\"Test passes\")\n----&gt; 7 assert(addition(2,2) == 5)\n      8 print(\"This statement never displays because the above assertion will fail, 2+2 is not 5\")\n\nAssertionError: \n\n\n\nWe can’t do that so easily when writing ML/DL code. There are places where this type of test will work, but the main output doesn’t have a concrete measurable performance that can be tested in this way. Certainly we can use loss functions and metrics to determine how well our model is doing, but even these are highly subjective in the majority of applications.\nOne way we can get a sense of our model’s performance is to develop a baseline. So that’s what we’ll be doing here.\nI found that baselining the multi-class (10-digit) model was really helpful in enabling me to understand how to apply the concepts differently. Working through it made it possible for me to even have a clue on how to tackle the actual model."
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p1/index.html#initializing-the-notebook-and-libraries",
    "href": "posts/2025_04_14_MNIST_p1/index.html#initializing-the-notebook-and-libraries",
    "title": "Classifying MNIST Digits (part 1)",
    "section": "",
    "text": "This is just the standard notebook initialization that loads the relevant fastai libraries\n\n\nCode\n#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n\nfrom fastai.vision.all import *\nfrom fastbook import *\n\nmatplotlib.rc('image', cmap='Greys')"
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p1/index.html#understanding-and-manipulating-the-dataset",
    "href": "posts/2025_04_14_MNIST_p1/index.html#understanding-and-manipulating-the-dataset",
    "title": "Classifying MNIST Digits (part 1)",
    "section": "",
    "text": "Ok so now we need to load the full dataset that has all the digits inside of it. Took me a bit to figure out how to get the full dataset, but its just URLS.MNIST.\n\n\nCode\npath = untar_data(URLs.MNIST)\n\n\nThe dataset has been downloaded and is in memory. I can use the path variable to save a reference to its top-level directory (the folder that has the downloaded data). This helps me access the data later on because I’ve saved a reference to where it exists. Now I need to prove to myself that I can navigate the file structure of the dataset, which I do in the next two codeblocks:\n\n\nCode\npath.ls()\n\n\n(#2) [Path('/home/aharon/.fastai/data/mnist_png/training'),Path('/home/aharon/.fastai/data/mnist_png/testing')]\n\n\n\n\nCode\n(path/\"training\").ls()\n\n\n(#10) [Path('/home/aharon/.fastai/data/mnist_png/training/0'),Path('/home/aharon/.fastai/data/mnist_png/training/8'),Path('/home/aharon/.fastai/data/mnist_png/training/1'),Path('/home/aharon/.fastai/data/mnist_png/training/5'),Path('/home/aharon/.fastai/data/mnist_png/training/6'),Path('/home/aharon/.fastai/data/mnist_png/training/4'),Path('/home/aharon/.fastai/data/mnist_png/training/2'),Path('/home/aharon/.fastai/data/mnist_png/training/3'),Path('/home/aharon/.fastai/data/mnist_png/training/9'),Path('/home/aharon/.fastai/data/mnist_png/training/7')]\n\n\nThis just helps me see how the data is structured. There’s two main directories (again, just folders): training and testing. And then each of these directories have 10 sub-directories; one for each numerical digit. Within each of those directories there are a large number of images that contain the handwritten pictures of those digits. The training set has 60000 images, and the testing (or validation) set has 10000.\nTo prove to myself that I understand the file structure, I’m going to write some code that grabs and displays one of the “8”s from the training set.\n\n\nCode\n# Show some images, see if I can display one of each image\n\nmnist_training_digit_paths = [p for p in (path/\"training\").ls().sorted()]\nsample_images = []\nfor p in mnist_training_digit_paths:\n    training_digit_im_paths = [im_path for im_path in p.ls().sorted()]\n    sample_images.append(Image.open(training_digit_im_paths[42]))\nsample_images[8]\n\n\n\n\n\n\n\n\n\nWhat’s helpful here is that I’ve created this variable sample_images so that I can display any number I want to (at least, in index 42 of the training set). This is great because i can more easily access other numbers; I don’t have to rewrite the whole thing just to get a different result:\n\n\nCode\nsample_images[7]\n\n\n\n\n\n\n\n\n\nAwesome! I was just trying to see if I understood the file structure and I proved to myself that I did. Now I want a single variable that contains the entire dataset so I can just type something like x['training'][6] and get all of the “6”s in the training set. There’s two more things I have to do. I turn each image into a pytorch tensor, and I normalize the pixel intensity matrix of each image… yeah, that’s certainly some high-level jargon. Basically I’m converting the data that is contained in the .png (or w/e it is) into a format that the library can use more easily.\nHere’s how I made that data structure:\n\n\nCode\ndef create_dataset(top_dir):\n    # takes a top level directory and makes the aforementioned list of 3d tensors for each digit\n    \n    # grab each of the paths for each digit directory in this folder\n    mnist_datatype_digit_paths = [p for p in top_dir.ls().sorted()] \n\n    sample_images = []\n    for p in mnist_datatype_digit_paths:\n        # grab the path to each image included for the current digit\n        training_digit_im_paths = [im_path for im_path in p.ls().sorted()] \n        # open the image as tensors for each included image\n        training_image_tensors = [tensor(Image.open(im)) for im in training_digit_im_paths] \n        # turn the list of several pixel intensity matrices into a single 3D tensor\n        stacked_digit_tensors = torch.stack(training_image_tensors).float() / 255 \n        sample_images.append(stacked_digit_tensors)\n    return sample_images\n        \nmnist_dataset = {}\nmnist_dataset[\"training\"] = create_dataset((path/\"training\"))\nmnist_dataset[\"testing\"] = create_dataset((path/\"testing\"))\nshow_image(mnist_dataset[\"training\"][7][100])\n\n\n\n\n\n\n\n\n\nThat’s one of the longer running cells, but it makes sense because its loading 70000 images into memory as tensors. But we were successful! The mnist_dataset variable now contains all of our data in a really simple way for us to access using the python programming lanuguage."
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p1/index.html#defining-our-baseline",
    "href": "posts/2025_04_14_MNIST_p1/index.html#defining-our-baseline",
    "title": "Classifying MNIST Digits (part 1)",
    "section": "",
    "text": "Remember, we are trying to create a ML model that can determine which digit a given image is a drawing of. We need a baseline to determine if our model performs better than a non-ML method. How can we come up with a way of doing this without using an ML model?\nWell, here’s what we did in the lesson. We were only looking at a small subset of this dataset. We looked at a small percentage of the 3s and the 7s. Then, we created an “average” 3. Each of these images is a 28x28 matrix of pixel intensities, ranging from 0 to 255. If we take the average 28x28 matrix of pixel intensities across the training set, we can “see” what the average digit looks like:\n\n\nCode\nshow_image(mnist_dataset['testing'][3].mean(0))\n\n\n\n\n\n\n\n\n\nPerfect! It’s blurry because it’s the combination of the pixels densities of a few thousand threes. And if I want the average of another number (like a 7) all I have to do is change the index.\n\n\nCode\nshow_image(mnist_dataset['testing'][7].mean(0))\n\n\n\n\n\n\n\n\n\nWith our average digit, we then realized we could compare each image to it. We could get the distance between the average three and the image we want to classify mathematically. If we were only considering 3s and 7s this is simple. If the distance between the image I’m looking at and the average 3 is less than its distance to the average 7, it’s probably a 3! This is exactly what we did for binary classification in the lesson.\nBut how do we extend this to multiclass classification? I don’t just want to know if an image is a 3 or not, I want to know exactly which digit it is. Honestly, it was pretty hard for me to imagine a way to do this at first. Here’s my initial reasoning:\n\n“Ok I’m not totally sure how to do this I think I need to make something that calculates the metrics as my goal. But I think I need to define both loss functions first. Also need something that turns the loss of a single image against each golden digit. Then, we compare our test number to each of the golden digits and find which it is closest to. What If I create a 2D tensor of vectors, and then each vector is the loss function of the input compared to each of the golden digits, shape of 1,10.”\n\nThis is basically what worked! I gave each image a 1x10 vector that had its distance to each of the 10 digits. Then classification was simply determining which of the 10 distances was smallest (more simply, which digit was it closest to).\n\n\nCode\n# Loss Functions\ndef loss_L1(x, y):\n    return (x-y).abs().mean((-1,-2))\n\ndef loss_L2(x, y):\n    # Alternatively, this is called RMSE\n    return torch.sqrt(((x - y) ** 2).mean((-1, -2)))\n\n\nWe use these calculations for distance because we can’t just subtract one matrix from the other. Some numbers in it would be positive and the others would be negative. If we took the average, it wouldn’t really indicate the distance very accurately. These two functions are the most common ones that data scientists use for comparison in this context. We’ll look later at why you might use one over the other.\nSo then I continue my plan. We create the comparison matrix which is several 1x10 vectors (or a single severalx10 matrix) that has the distances to each “average digit” for every image in the dataset. Then the classify_digit function turns each of those 1x10 vectors into a prediction by finding which digit corresponds to the smallest distance for the given image. Remember, we used the training data to create the average digits, and we used the testing data to see how well this method actually worked for prediction.\n\n\nCode\ndef create_comparison_matrix(d, loss):\n    mnist_distances = []\n    d_testing_images = mnist_dataset[\"testing\"][d]\n    for digit in range(len(mnist_dataset[\"testing\"])):\n        golden_digit = mnist_dataset[\"training\"][digit].mean(0)\n        mnist_distances.append(loss(d_testing_images, golden_digit))\n    # the dim parameter allows me to modify the final shape. I want output[0] to give me the first classification vector length 10\n    return torch.stack(mnist_distances, dim=1)\n\n\ndef classify_digit(d, loss):\n    comp_matrix = create_comparison_matrix(d, loss)\n    min_vals, min_indices = comp_matrix.min(dim=1)\n    ans = (min_indices == d).sum() / len(comp_matrix)\n    return ans\n\n\nWe’re basically done! Out of my own curiosity, I wanted to see which loss function would perform better:\n\n\nCode\nprint(\"Loss Function Correct Prediction Comparison: \\n\")\nprint(\"tensor([[digit , L1Acc%, L2Acc%], \\n\")\nmetrics = []\nfor d in range(10):\n    metrics.append([d, float(classify_digit(d, loss_L1)),float(classify_digit(d, loss_L2))])\nprint(tensor(metrics))\nprint(\"Average\")\nprint(tensor(metrics).mean(0))\n\n\nLoss Function Correct Prediction Comparison: \n\ntensor([[digit , L1Acc%, L2Acc%], \n\ntensor([[0.0000, 0.8153, 0.8959],\n        [1.0000, 0.9982, 0.9621],\n        [2.0000, 0.4234, 0.7568],\n        [3.0000, 0.6089, 0.8059],\n        [4.0000, 0.6680, 0.8259],\n        [5.0000, 0.3262, 0.6861],\n        [6.0000, 0.7871, 0.8633],\n        [7.0000, 0.7646, 0.8327],\n        [8.0000, 0.4425, 0.7372],\n        [9.0000, 0.7760, 0.8067]])\nAverage\ntensor([4.5000, 0.6610, 0.8173])\n\n\nOk I guess this is pretty cool. We see that the RMSE loss function (L2 Norm) performs better. Well what does better even mean? Its worst accuracy score is a 69% on the fives, whereas the L1 norm performs at a 33% for the same number. The average accuracy of L1 is 66%, whereas RMSE performs at 82%.\nWhy is RMSE so much better? The L1 norm simply takes the absolute value of the difference between the prediction and the target. The RMSE squares the difference. Very basically, this means that bigger mistakes are penalized more heavily than for the L1 norm.\nDoes this make sense for our use case? Absolutely. We are comparing the difference in intensity of each pixel in a 28x28 grid to determine image similarity. Let’s say we were comparing something to a zero. The image we were looking at had a very dark pixel in the exact center of the screen. A zero is basically a circle, so there shouldn’t be anything in the center of the image. If we could point out that a big mistake like that is much further away from a zero, then we’d be in a really good spot. Since we are comparing pixel densities, it makes a lot of sense that for images that have some pixels that are way off of the ideal image we would get more accurate scores with the RMSE vs the L2 norm.\nHere’s another way to visualize it.\n\n\n\nRMSE_L2_compare.png\n\n\nThe RMSE is way higher on the parts of the graph where the prediction is furthest away from the true value. Bigger mistakes are penalized much more intensely than other mistakes in the same environment. L1 says “Ok, yeah that’s bad”, L2 says “This is insane, how are you this wrong?!”"
  },
  {
    "objectID": "posts/2025_04_14_MNIST_p1/index.html#conclusion",
    "href": "posts/2025_04_14_MNIST_p1/index.html#conclusion",
    "title": "Classifying MNIST Digits (part 1)",
    "section": "",
    "text": "This isn’t really a ML post, but it does pave the way to understand the next part in this series. We need a baseline to assess if our ML model performs better than the alternatives. In this case we saw that using the RMSE loss we could get a classification accuracy of our validation set of 81.73%. We will aim to beat that performance using Stochastic Gradient Descent in the next post.\n\n\n\nThe hardest part of this was making the conceptual shift from realizing each image needed 10 values associated with it. Once I made that leap this became way easier.\nA big part of this was just working through some of the code and fiddly bits. I had to remember and re-teach myself why sometimes the tensor.mean(x) changes based on what you want. I didn’t include it in the post but it was part of the process.\nComparing the two loss functions was a huge lightbulb moment for me, and the side-by-side graph was really helpful. Hopefully you learn something too!"
  },
  {
    "objectID": "posts/Final Project PLan/index.html",
    "href": "posts/Final Project PLan/index.html",
    "title": "The Signal and the Noise: Building a Predictive Wellness Model from Personal Data",
    "section": "",
    "text": "image"
  },
  {
    "objectID": "posts/Final Project PLan/index.html#second-brain-pc",
    "href": "posts/Final Project PLan/index.html#second-brain-pc",
    "title": "The Signal and the Noise: Building a Predictive Wellness Model from Personal Data",
    "section": "Second Brain PC",
    "text": "Second Brain PC\nNo, I don’t have the tech to literally build another brain. The project name is based off of a concept from author Tiago Forte called Building a Second Brain. In the book, he talks about creating an organized system of creativity, summarizing concepts from thought leaders like Octavia Butler, David Allen, and Richard Feynman. He mentions that at times of great transition, great thinkers like Locke and DaVinci kept “commonplace books” that allowed them to make sense of what was happening around them. I’ve been using this system for 2.5 years and here’s a snapshot of some of today’s fruit:\n Figure 1: An example of my notes app designed with principles from Tiago Forte\nHe wrote another book PARA where he talks about digital organization without the creativity spin. He uses: Inbox, Projects, Areas, Resources, and Archives. I used that for a while but then did a creative pivot; Sunlight, Fruits, Roots, Nutrients, and Arboretum.\nA few months after implementing the PARA (FRAN) system, I also started archiving all of my digital files across time. Google Takeout, old computers, and I even downloaded a low-data backup of wikipedia. I built a webscraper that allowed me to download data from one of my favorite podcasts. They provide free resources that are already paid for by their community, so it didn’t feel like an ethics breach to download these. I started realizing the need to have some type of local server to help me with data collection.\n\nHardware Design\nThis January I started spec-ing out an AI PC. I wanted to get an NVIDIA 5090 and was tenth in line at microcenter for release; apparently they only had five 😭. Fortunately, one of the guys in line sold me a cheap 3090. I really wanted to be able to use some open-source models with my data, but not have to upload my data to anyone else’s platform. I realized I was making a much bigger system than I originally thought.\nI gutted my old PC and redesigned it with Deep Learning in mind (Ryzen 7 5700x, 64 GB RAM, NVIDIA 3090). I knew it didn’t have to be INCREDIBLY powerful, just enough to do some local image generation and run some local models; I’ve even had some rudimentary video generation. The problem is, whenever I ran those it maxed the VRAM used on the box. I’d need another machine to handle things like API calls and run “always-on” code, so I purchased a cheap NUC to handle this for me.\nI’ve always struggled to organize my data across platforms! I wanted to harmonize my data and so I realized that a single point of truth would be most effective. I researched local ways of implementing this and discovered that Seafile and NextCloud were the best options. I went with NextCloud because based on the file storage method, it was way easier to interact with local files than the other option.\nAs of right now, I have almost everything set up. I have a cloud backup service, and a local backup or two. I’m planning to work with a friend who does security red-teaming for “Big Tech” to help me make it more secure. The docker setup and nextcloud have been a bit finnicky, but everything is generally organized. My vision was something like JARVIS from Iron Man; at least, one that never turns into a synthezoid.\n\n\nPotential System Applications\n\nRemotely accessible personal library (NAS) of as much data as I can imagine\nPersonal AI API platform\n\nI could setup a simple request sender via google gemini voice that can be a personal secretary and help me send/monitor texts and dms across social media platform\nI have several more, but I’m keeping them under wraps for now.\n\nEnables the following projects. The Spotify API only allows me to grab the last 50 songs of my history. I’d need something that repeatedly scrapes my data and logs it for the following ideas to work."
  },
  {
    "objectID": "posts/Final Project PLan/index.html#the-spotify-examen-finding-a-signal-in-the-noise",
    "href": "posts/Final Project PLan/index.html#the-spotify-examen-finding-a-signal-in-the-noise",
    "title": "The Signal and the Noise: Building a Predictive Wellness Model from Personal Data",
    "section": "The Spotify Examen: Finding a Signal in the Noise",
    "text": "The Spotify Examen: Finding a Signal in the Noise\nI got my wrapped playlist from 2023 in December, and I was noticed how my top songs were suspiciously correlated to the things that had happened that year. I wondered if my spotify data could somehow be used for personal insight. There’s something incredibly powerful about leveraging data in this way! If I just take a survey, I might see the questions and engineer a response that I think someone wants to hear. I am incredibly honest with spotify about what I want to never listen; I never choose to listen to something I don’t want to hear. I downloaded my data and did a brief proof of concept:\n\n\n\nimage\n\n\nFigure 2: An example of daily listening habits from a stable period, the second half of 2021\n\n\n\nimage\n\n\nFigure 3: An example of daily listening habits from a turbulent period, summer 2023\nThis is exactly what I was looking for! See that flatline in July 2023? That was a total personal crash period for me and it was reflected in my data without me even knowing it.\nI did the prototype above in fall of 2024. I recently finished a period of deep guided reflection called the Spiritual Exercises. Through this, I got more familiar with the concept of finding Spirit in all things. Theoretically that should include my data; ALL of it. I’d like to keep exploring this concept. I started analyzing the time period for this meditation/prayer retreat; 9/24/24-7/15/25. I also broke it down just by a single “Week” of the exercises (which in this case was about a month long).\n\nTruncated Full Spiritual Exercises\n\n\n\nimage\n\n\nFigure 3: Listening data for all the Exercises, except for the final month\n\n\n\nimage\n\n\nFigure 4: A simple distribution of the most commonly listened to songs during the exercises\n\n\n‘First Week’ of Spiritual Exercises\n\n\n\nimage\n\n\nFigure 5: Listening data for all the “First Week” of the Exercises (38 days)\n\n\n\nimage\n\n\nFigure 6: A simple distribution of the most commonly listened to songs during the ‘First Week’\nThe most fascinating thing for me here is how the lyrics of the song I listened to MOST during Week 1 has lyrics that correspond directly to the themes of Week 1! To me that’s wild beyond imagination.\n\n\nImplications and Next Steps\n\nI’d continue to do this for the rest of the 9 month retreat and report my findings. Well, I’d at least report them to myself. Not sure what level of depth I’ll go into with this publicly, I’m still deciding.\nI’d break the data down by each of the four “weeks”, the preparation beforehand, and the “election” period afterwards.\nI’d add more features to this data and try and add more visuals. There’s no end to creative music theory, lyrical sentiment analysis, trend analysis features that could be engineered.\nI’m still waiting on spotify to give me my data! I put in a request recently but the data was incomplete on their end; it stopped on June 11th, but it should’ve gone until Aug 7. We’ll see what else I think of my the time I finally get my full dataset."
  },
  {
    "objectID": "posts/Final Project PLan/index.html#forging-a-soul-mirror",
    "href": "posts/Final Project PLan/index.html#forging-a-soul-mirror",
    "title": "The Signal and the Noise: Building a Predictive Wellness Model from Personal Data",
    "section": "Forging a Soul Mirror",
    "text": "Forging a Soul Mirror\nI credit the majority of this post to my Second Brain system. It’s been amazing to watch an idea develop from a tiny seed/bullet point into something fully fledged like what I’m about to describe. I’ve known I’ve needed to make a final project idea to finish the FastAI part 1 course for a few months now. Originally I wanted to see if I could use my headspace data and my fitbit heart rate to see if I could predict when I’m meditating based on my heart rate. If so, I wouldn’t need to rely on my headspace data, I’d just be able to extrapolate it from my heart rate data.\nI’ve wanted an automated way to track “how I’m doing”. People sometimes say, “Hey, how are you?” and I don’t always know how to respond. Sometimes I think I’m doing great but I’m really not. If I could create some type of data-driven Personal Wellness Index based on my actions, it might be really helpful. So the question I’d be using ML/AI to answer would be, “What digitally recorded habits predict my highest positive experiences”. I already know my daily listening marginally correlates; what else could help me see myself more clearly?\nOf course, if you want to use ML to do this, you need to somehow label the data. Happy is a pretty arbitrary adjective to measure with data. Fortunately, I’ve been keeping a daily Examen every day since mid-December. If you haven’t heard of it, it’s essentially a prayerful daily review focusing on the most significant emotional/spiritual occurrences of the day. So theoretically I’d be able to use this to create a metric to optimize for. At the very least, I could use an LLM to analyze all of these and predict a “consolation/desolation metric” for each day; a number between -2 and 2. From there, I’d be able to leverage ALL my data and optimize some type og PWI. When I say ALL, this includes (but is not limited to): * spotify * youtube * browsing history * journaling * sent emails * sent texts * fitbit HR data"
  },
  {
    "objectID": "posts/Final Project PLan/index.html#conclusion-the-soul-mirror-awaits",
    "href": "posts/Final Project PLan/index.html#conclusion-the-soul-mirror-awaits",
    "title": "The Signal and the Noise: Building a Predictive Wellness Model from Personal Data",
    "section": "Conclusion: The Soul Mirror Awaits",
    "text": "Conclusion: The Soul Mirror Awaits\nSo, this is the plan. It’s a journey in three stages, moving from the physical to the digital to the predictive.\n\nStage 1: Forging the Vessel. Building a custom, secure, two-part computer system to serve as a private “Second Brain.”\nStage 2: Polishing a Corner. Proving the concept with the Spotify Examen, finding a clear signal of my inner life in the noise of my listening data.\nStage 3: Assembling the Mirror. Unifying all of my data streams—from my heart rate to my journal entries—to train a model that can answer a simple, vital question: “How am I really doing?”\n\nI started this project to find a data-driven reflection of my own well-being—a personal check-engine light for the soul. Each stage is a deliberate step toward building that mirror. The hardware is the frame, the data pipelines are the silvering, and the machine learning model is the final polish that will, hopefully, reveal a clear image.\nThe experiment is personal, but the question is universal. What will I see when I’m finally able to look? I’m excited, and a little nervous, to find out.\nBut creating such a powerful tool for self-reflection inevitably raises even bigger questions about privacy, ownership, and what it means to be human in a digital world. I’ll explore those implications in my next post."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "The Signal and the Noise: Building a Predictive Wellness Model from Personal Data\n\n\n\n\n\n\nmachine learning\n\n\nfastai\n\n\nproject\n\n\nspotify\n\n\nmusic\n\n\nmental-health\n\n\nwellness\n\n\n\n\n\n\n\n\n\nAug 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Resnet50 on Oxford Flowers\n\n\n\n\n\n\nmachine learning\n\n\nfastai\n\n\ncomputer-vision\n\n\n\n\n\n\n\n\n\nApr 28, 2025\n\n\nAaron James\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Disaster Tweets with NLP\n\n\n\n\n\n\nmachine learning\n\n\ntutorial\n\n\nfastai\n\n\nNLP\n\n\nkaggle\n\n\n\n\n\n\n\n\n\nApr 21, 2025\n\n\nAaron James\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying MNIST Digits (part 1)\n\n\n\n\n\n\nmachine learning\n\n\ntutorial\n\n\nfastai\n\n\nMNIST\n\n\n\n\n\n\n\n\n\nApr 14, 2025\n\n\nAaron James\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying MNIST Digits (part 2)\n\n\n\n\n\n\nmachine learning\n\n\ntutorial\n\n\nfastai\n\n\nMNIST\n\n\n\n\n\n\n\n\n\nApr 14, 2025\n\n\nAaron James\n\n\n\n\n\n\n\n\n\n\n\n\nTraining an Emotional Face Classifier\n\n\n\n\n\n\nmachine learning\n\n\ntutorial\n\n\nfastai\n\n\ncomputer-vision\n\n\n\n\n\n\n\n\n\nMar 22, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Posts",
    "section": "",
    "text": "Welcome to my blog posts!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying MNIST Digits (part 1)\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2025\n\n\nAaron James\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying MNIST Digits (part 2)\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2025\n\n\nAaron James\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Disaster Tweets with NLP\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2025\n\n\nAaron James\n\n\n\n\n\n\n\n\n\n\n\n\nThe Signal and the Noise: Building a Predictive Wellness Model from Personal Data\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Resnet50 on Oxford Flowers\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2025\n\n\nAaron James\n\n\n\n\n\n\n\n\n\n\n\n\nTraining an Emotional Face Classifier\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2025\n\n\n\n\n\n\nNo matching items"
  }
]