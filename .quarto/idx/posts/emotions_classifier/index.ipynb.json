{"title":"Training an Emotional Face Classifier","markdown":{"yaml":{"title":"Training an Emotional Face Classifier","date":"2025-03-22","categories":["machine learning","tutorial","fastai","computer-vision"],"slug":"emotional-classifier","format":{"html":{"toc":true,"code-fold":true}}},"headingText":"Begin Creating the Dataset","containsRefs":false,"markdown":"\n\n\n![image](emotions.jpeg)\n\nOk I have been trying to get this thing posted for weeks and its finally time now. First step, what am I even doing. Well, broadly the goal is to get a decent understanding of deep learning concepts and apply them to subject matter I find interesting. I see this stage as learning to read sheet music and musc theory before I can create compositions. \n\nThe first step is going to be training the model. It's taken me a while to decide what I want to train it on. Originally I wanted to do something music related, but now the desire to get something UP and POSTED has made me go with my first idea of emotion recognition in human faces.\n\n\nBefore I get into any DL/ML specific things, I first have to import the fastai package into the notebook. So, that's what we see below:\n\nTheoretially the pakage is set up. Now I need some data. Since this is my first post, I'm just going to use duckduckgo to grab some images from the internet. In practice, datasets must be curated more carefully than this. I'll use one trick I learned, but there are issues with going directly to image search. One of the biggest is that the data usually ends up being pretty biased. My guess is that if I do it this way I'll get an attractive white people emotion analyzer, rather than a people analyzer. We'll see...\n\nOk, this is a great example of why going straight to image search is bad. The first image I got was [this](https://c8.alamy.com/comp/FTFFGG/cartoon-happy-female-face-with-speech-bubble-FTFFGG.jpg)\n\nThe image is a cartoon face with speech bubble. So maybe this blog post is \"why you shouldn't use a naive image search for building models, but how to fight through the pain anyway.\"\n\nLet's see if tweaking the search term helps...\n\nOkay great, we definitely got a human [this time](https://thumbs.dreamstime.com/z/close-up-portrait-smiling-laughing-attractive-man-happy-face-human-emotion-expression-having-fun-joy-camera-152138665.jpg). Still though we can't see if ALL of our images are of humans, nor can we tell if we have a decent demographic distribution. Just highlighting the necessities of data curation and bias analysis. Technically it could be possible that this bias wouldn't impact the deployment. In my case, my goal is not to make a robust emotions classifier but to make a simple deployed proof of concept, so I'm pressing on üòÅ.\n\nSpeeding things up, we can add the classifications 'sad' and 'neutral' to our options.\n\nAnother thing I noticed as I searched for face manually is that SO MANY of them are AI generated. Train a model of human emotion recognition on AI generated images of human faces does not seem like a good idea for any production level systems.\n\n\nWe'll go ahead and review our downloaded photos for any images that had errors during file trasnfer\n\n\n# Training the Model (Round 1)\n\nWe have a dataset of emotions now! The dataset has not been curated though which can result in some issues as we discussed previously. Interestingly, I do not have the time to sort through 600 photos at this point. To aid me in my curation I will train the model first, and use the loss of each image to determine where some of the hiccups in data may be.\n\nThis method is detailed by Jeremy Howard in his book/course [Practical Deep Learning](https://course.fast.ai/).\n\nBut before we do any of that we'll need to create a training and a validation set from our data. We will use fastai's `DataLoader's` class.\n\nA Quick Note. We use random resized crop to give us a little \"extra\" data here. It takes small sections of each photo and performs some standard image manipulation (data augmentation). This makes it seem like there's extra data to our model which needs all the help it can get. At this stage, we haven't curated our datset.\n\nNow we have enough data to train the model. We'll see how the model performs.\n\nOk the first training batch has it set so that the error rate is around 24.4%. Seems to be decent. It's much better than the starting rate of 42%.\n\nWe want to know if we can improve this error rate by curating the dataset.\n\nThis confusion matrix helps determine where the errors are. Another way we can see where the errors are is using fastai's loss plotting tool:\n\nSo using this loss plotting, we can already see where some of our issues are coming from:\n\n* Some images classified as neutral are actually joyful\n* Some images are cartoons or emojis\n* Some images are multiple faces\n\nPart of what I want to do here is explain some of the issues with data curation. I could go back and change my search terms to get rid of some of these issues, but this helps me show the issues with a naive search better.\n\nFortunately, fastai provides a tool for removing the high-loss images from the datasets.\n\nReminder to self/anyone:\n\nThis code needs to be run for each combination of category/set-type. For example, if you go through the top losses in sad/valid, then you need to run this code, and move to sad/training.\n\n## Local Environment Detour\n\nOk, its around here that I had to shift gears to set up my own local fastai and jupyter environment on my ML desktop. Long story short I built an ML workstation for this purpose. I needed to do this because I'm having trouble understanding how the cleaner modifies files and I can understand a local filesystem more clearly than google drive's (w/ colab).\n\nAlso, if you've ever tried to make a local ML environment than you'll know how PERSNICKITY this process can be (read: highly specific and thorny). So anyway, this is what I've been doing for the last day or so. Also, my latop broke and the first one I ordered to replace it DID NOT COME WITH A KEYBOARD üôÉ\n\nWe soldier on...\n\n\nNow I have edited the files in the original path. Some were moved and others were deleted. If I rebuild another dataloader from the original path it should be more accurate this time!\n\n# Training the Model (Round 2)\n\nWhy are there weird new images? These were always present in the dataset, and for some reason they did not propagate to the top of the cleaner. The cleaner only shows the hardest images for the model to classify (by their \"loss\" values). It's possible these images didn't have the highest loss values in their respective categories.\n\nOk, so we can see that cleaning the data did actually improve the performance of the model. We can see the error rate decreased from 24.4% to 18.2%. Aditionally we can see that the confusion matrix is much more precise. There is only one square outside of the main diagonal that is slightly blue. So yes, this tool is effective and it helps us improve the model.\n\n\n# Summary\n\nSo we trained a model on a dataset full of duckduckgo images that can generally determine the emotions of someone whose face it has a picture of. Kinda. \n\nWe learned that creating a dataset is actually rather difficult, especially if its done primarily from internet image searches. \n\nHowever, we saw large success with the idea of training a model first, finding the circumstances that cause high loss values, and editign the dataset based on the images the model finds hardest to classify. In other words, training a model before the dataset is perfectly curated helps curate the dataset. This is incredibly useful.\n\nThere are also all sorts of issues when it comes to creating a dataset this way.\n\n* Images that are animated or drawn\n* AI generated images are starting to crop up and find their way into datasets\n* There's all sorts of racial bias in the images that were returned\n\nThis is an effective proof of concept that suggests a facial emotion recognizer could work. You can check it out on [Hugging Face Spaces](https://huggingface.co/spaces/AaronJames95/emotion_classifier) here!\n\n## Epilogue\nLater I wanted to see if I could get better performance with the model if I got into all the images and modified them individually like in the cleaner. It took me roughly a half hour, and tI couldn't get it better than the second round of training. So, my assumption is that this problem (automating facial emotion ndetection) is simply non-trivial and there may be other ways to approach it. Also, determining the emotions of a face in a photo is highly subjective which could be another reason I'm not seeing better results.\n\nI was really surprised by how much of an effective time save using the cleaner turned out to be! The first time I used it it took around 5 minutes and I got the same results as my 30 minutes image crawl of 600 pictures.\n\n","srcMarkdownNoYaml":"\n\n\n![image](emotions.jpeg)\n\nOk I have been trying to get this thing posted for weeks and its finally time now. First step, what am I even doing. Well, broadly the goal is to get a decent understanding of deep learning concepts and apply them to subject matter I find interesting. I see this stage as learning to read sheet music and musc theory before I can create compositions. \n\nThe first step is going to be training the model. It's taken me a while to decide what I want to train it on. Originally I wanted to do something music related, but now the desire to get something UP and POSTED has made me go with my first idea of emotion recognition in human faces.\n\n# Begin Creating the Dataset\n\nBefore I get into any DL/ML specific things, I first have to import the fastai package into the notebook. So, that's what we see below:\n\nTheoretially the pakage is set up. Now I need some data. Since this is my first post, I'm just going to use duckduckgo to grab some images from the internet. In practice, datasets must be curated more carefully than this. I'll use one trick I learned, but there are issues with going directly to image search. One of the biggest is that the data usually ends up being pretty biased. My guess is that if I do it this way I'll get an attractive white people emotion analyzer, rather than a people analyzer. We'll see...\n\nOk, this is a great example of why going straight to image search is bad. The first image I got was [this](https://c8.alamy.com/comp/FTFFGG/cartoon-happy-female-face-with-speech-bubble-FTFFGG.jpg)\n\nThe image is a cartoon face with speech bubble. So maybe this blog post is \"why you shouldn't use a naive image search for building models, but how to fight through the pain anyway.\"\n\nLet's see if tweaking the search term helps...\n\nOkay great, we definitely got a human [this time](https://thumbs.dreamstime.com/z/close-up-portrait-smiling-laughing-attractive-man-happy-face-human-emotion-expression-having-fun-joy-camera-152138665.jpg). Still though we can't see if ALL of our images are of humans, nor can we tell if we have a decent demographic distribution. Just highlighting the necessities of data curation and bias analysis. Technically it could be possible that this bias wouldn't impact the deployment. In my case, my goal is not to make a robust emotions classifier but to make a simple deployed proof of concept, so I'm pressing on üòÅ.\n\nSpeeding things up, we can add the classifications 'sad' and 'neutral' to our options.\n\nAnother thing I noticed as I searched for face manually is that SO MANY of them are AI generated. Train a model of human emotion recognition on AI generated images of human faces does not seem like a good idea for any production level systems.\n\n\nWe'll go ahead and review our downloaded photos for any images that had errors during file trasnfer\n\n\n# Training the Model (Round 1)\n\nWe have a dataset of emotions now! The dataset has not been curated though which can result in some issues as we discussed previously. Interestingly, I do not have the time to sort through 600 photos at this point. To aid me in my curation I will train the model first, and use the loss of each image to determine where some of the hiccups in data may be.\n\nThis method is detailed by Jeremy Howard in his book/course [Practical Deep Learning](https://course.fast.ai/).\n\nBut before we do any of that we'll need to create a training and a validation set from our data. We will use fastai's `DataLoader's` class.\n\nA Quick Note. We use random resized crop to give us a little \"extra\" data here. It takes small sections of each photo and performs some standard image manipulation (data augmentation). This makes it seem like there's extra data to our model which needs all the help it can get. At this stage, we haven't curated our datset.\n\nNow we have enough data to train the model. We'll see how the model performs.\n\nOk the first training batch has it set so that the error rate is around 24.4%. Seems to be decent. It's much better than the starting rate of 42%.\n\nWe want to know if we can improve this error rate by curating the dataset.\n\nThis confusion matrix helps determine where the errors are. Another way we can see where the errors are is using fastai's loss plotting tool:\n\nSo using this loss plotting, we can already see where some of our issues are coming from:\n\n* Some images classified as neutral are actually joyful\n* Some images are cartoons or emojis\n* Some images are multiple faces\n\nPart of what I want to do here is explain some of the issues with data curation. I could go back and change my search terms to get rid of some of these issues, but this helps me show the issues with a naive search better.\n\nFortunately, fastai provides a tool for removing the high-loss images from the datasets.\n\nReminder to self/anyone:\n\nThis code needs to be run for each combination of category/set-type. For example, if you go through the top losses in sad/valid, then you need to run this code, and move to sad/training.\n\n## Local Environment Detour\n\nOk, its around here that I had to shift gears to set up my own local fastai and jupyter environment on my ML desktop. Long story short I built an ML workstation for this purpose. I needed to do this because I'm having trouble understanding how the cleaner modifies files and I can understand a local filesystem more clearly than google drive's (w/ colab).\n\nAlso, if you've ever tried to make a local ML environment than you'll know how PERSNICKITY this process can be (read: highly specific and thorny). So anyway, this is what I've been doing for the last day or so. Also, my latop broke and the first one I ordered to replace it DID NOT COME WITH A KEYBOARD üôÉ\n\nWe soldier on...\n\n\nNow I have edited the files in the original path. Some were moved and others were deleted. If I rebuild another dataloader from the original path it should be more accurate this time!\n\n# Training the Model (Round 2)\n\nWhy are there weird new images? These were always present in the dataset, and for some reason they did not propagate to the top of the cleaner. The cleaner only shows the hardest images for the model to classify (by their \"loss\" values). It's possible these images didn't have the highest loss values in their respective categories.\n\nOk, so we can see that cleaning the data did actually improve the performance of the model. We can see the error rate decreased from 24.4% to 18.2%. Aditionally we can see that the confusion matrix is much more precise. There is only one square outside of the main diagonal that is slightly blue. So yes, this tool is effective and it helps us improve the model.\n\n\n# Summary\n\nSo we trained a model on a dataset full of duckduckgo images that can generally determine the emotions of someone whose face it has a picture of. Kinda. \n\nWe learned that creating a dataset is actually rather difficult, especially if its done primarily from internet image searches. \n\nHowever, we saw large success with the idea of training a model first, finding the circumstances that cause high loss values, and editign the dataset based on the images the model finds hardest to classify. In other words, training a model before the dataset is perfectly curated helps curate the dataset. This is incredibly useful.\n\nThere are also all sorts of issues when it comes to creating a dataset this way.\n\n* Images that are animated or drawn\n* AI generated images are starting to crop up and find their way into datasets\n* There's all sorts of racial bias in the images that were returned\n\nThis is an effective proof of concept that suggests a facial emotion recognizer could work. You can check it out on [Hugging Face Spaces](https://huggingface.co/spaces/AaronJames95/emotion_classifier) here!\n\n## Epilogue\nLater I wanted to see if I could get better performance with the model if I got into all the images and modified them individually like in the cleaner. It took me roughly a half hour, and tI couldn't get it better than the second round of training. So, my assumption is that this problem (automating facial emotion ndetection) is simply non-trivial and there may be other ways to approach it. Also, determining the emotions of a face in a photo is highly subjective which could be another reason I'm not seeing better results.\n\nI was really surprised by how much of an effective time save using the cleaner turned out to be! The first time I used it it took around 5 minutes and I got the same results as my 30 minutes image crawl of 600 pictures.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","theme":["superhero","brand"],"title":"Training an Emotional Face Classifier","date":"2025-03-22","categories":["machine learning","tutorial","fastai","computer-vision"],"slug":"emotional-classifier"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}